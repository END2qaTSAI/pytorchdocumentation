[
    {
        "X": "The default gain forSELUsacrifices what effect for more stable gradient flow in rectangular layers?",
        "Y": "the normalisation effect",
        "Z": "Return the recommended gain value for the given nonlinearity function.\nThe values are as follows: nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the definition of Identity 111 Conv1,2,3D 111 Sigmoid 111 Tanh 53frac5",
        "Y": "Linear",
        "Z": "Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is needed to induce a stable fixed point in the forward pass?",
        "Y": "a variance of1/N",
        "Z": "Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What do the initial weights have to have in order to induce a stable fixed point in the forward pass?",
        "Y": "variance of1/N",
        "Z": "Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the name of the Sigmoid 111 Tanh?",
        "Y": "Sigmoid 111 Tanh",
        "Z": "Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is Tanh's number?",
        "Y": "111",
        "Z": "111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does usingnonlinearity='selu' give the initial weights?",
        "Y": "variance of1/N",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2).",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is param?",
        "Y": "optional parameter",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does the input Tensor have to fill the tensor with?",
        "Y": "scalar value",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does an n-dimensionaltorch fill the input Tensor with?",
        "Y": "scalar value0",
        "Z": "Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Fills the 2-dimensional inputTensor with what?",
        "Y": "identity matrix",
        "Z": "Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Preserves the identity of the inputs inLinearlayers where?",
        "Y": "as many inputs are preserved as possible",
        "Z": "Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does Fills the 2-dimensional inputTensorwith the identity matrix?",
        "Y": "Preserves the identity of the inputs",
        "Z": "Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the a-b bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal distribution?",
        "Y": "the lower bound of the uniform distribution",
        "Z": "a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "InLinearlayers, where as many inputs are preserved as possible, what does Fills the 2-dimensional inputTensorwith the identity",
        "Y": "Preserves the identity of the inputs",
        "Z": "Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Fills the 3, 4, 5-dimensional inputTensorwith what?",
        "Y": "Dirac delta function",
        "Z": "Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Where are as many input channels preserved as possible?",
        "Y": "Convolutionallayers",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the name of Glorot's paper Understanding the difficulty of training deep feedforward neural networks?",
        "Y": "X",
        "Z": "Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Who wrote Understanding the difficulty of training deep feedforward neural networks?",
        "Y": "Bengio",
        "Z": "val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is used to fill the inputTensor with values according to the method described inUnderstanding the difficulty of training deep feedforward neural networks",
        "Y": "uniform distribution",
        "Z": "Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the resulting tensor known as?",
        "Y": "Glorot initialization",
        "Z": "std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the name of the author of Understanding the difficulty of training deep feedforward neural networks?",
        "Y": "X",
        "Z": "Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does Moon-Penrose inverse stand for?",
        "Y": "the pseudoinverse",
        "Z": "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "Ifhermitian= True,Ais assumed to be what if complex or symmetric if real?",
        "Y": "Hermitian",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What function synchronizes that device with the CPU?",
        "Y": "usestorch.linalg.svd()ifhermitian= False",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is a possible way to multiply a matrix on the left by the pseudoinverse?",
        "Y": "usingtorch.linalg.lstsq()",
        "Z": "The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "The pseudoinverse can be defined algebraically but it is more computationally convenient to understand it through what?",
        "Y": "SVD",
        "Z": "The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does the pseudoinverse support?",
        "Y": "batches of matrices",
        "Z": "The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is faster and more numerically stable than computing the pseudoinverse explicitly?",
        "Y": "usingtorch.linalg.lstsq()",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is a warning about usinglstsq() for multiplying a matrix on the left by the pseudoinverse?",
        "Y": "Warning",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does usestorch.linalg.svd() do for CUDA inputs?",
        "Y": "synchronizes that device with the CPU",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the name of the warning that is issued when a matrix is multiplied by the pseudoinverse?",
        "Y": "Warning",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "How often does a value appear in a given row of the inputtensor?",
        "Y": "most often",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "If keepdimisTrue, the output tensors are of the same size asinput except in the dimensiondimwhere they",
        "Y": "1",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "What is the name of the function that returns whether the output tensor hasdimretained or not?",
        "Y": "keepdim",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "What does this class wrap around an arbitraryoptim.Optimizer?",
        "Y": "shards",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "The optimizer instance in each rank is only responsible for what?",
        "Y": "updating1/world_sizeparameters",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "To whom will each rank broadcast its parameters after parameters are updated locally?",
        "Y": "all other peers",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does ZeroRedundancyOptimizer use to pack a number of parameters at each rank?",
        "Y": "greedy algorithm",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How does each parameter belong to a single rank?",
        "Y": "not divided among ranks",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the partition of a zero redundancyoptimizer?",
        "Y": "arbitrary",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What class does optimizer_class(torch.nn.Optimizer) belong to?",
        "Y": "local optimizer",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does state_dict(dict) refer to?",
        "Y": "optimizer state",
        "Z": "to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update).",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is an object returned from a call tostate_dict() Gets this rank'sstate_dict?",
        "Y": "state_dict(dict)",
        "Z": "Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What should be an object returned from a call?",
        "Y": "tostate_dict()",
        "Z": "Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Inverse short time Fourier Transform is expected to be what?",
        "Y": "inverse ofstft()",
        "Z": "Inverse short time Fourier Transform. This is expected to be the inverse ofstft().\nIt has the same parameters (+ additional optional parameter oflength) and it should return the\nleast squares estimation of the original signal. The algorithm will check using the NOLA condition (\nnonzero overlap). Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the envelop created by the summation of all the windows at certain point in time?",
        "Y": "never zero",
        "Z": "Inverse short time Fourier Transform. This is expected to be the inverse ofstft().\nIt has the same parameters (+ additional optional parameter oflength) and it should return the\nleast squares estimation of the original signal. The algorithm will check using the NOLA condition (\nnonzero overlap). Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the envelop created by the summation of all the windows never zero at certain point in time?",
        "Y": "0",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What can be trimmed off precisely because they can be calculated?",
        "Y": "Left padding",
        "Z": "Inverse short time Fourier Transform. This is expected to be the inverse ofstft().\nIt has the same parameters (+ additional optional parameter oflength) and it should return the\nleast squares estimation of the original signal. The algorithm will check using the NOLA condition (\nnonzero overlap). Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What are some examples of complex inputs?",
        "Y": "channel,fft_size,n_frame",
        "Z": "The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Since what version is input(Tensor) deprecated?",
        "Y": "1.8.0",
        "Z": "input(Tensor) \u2013 The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the",
        "Y": "normalized",
        "Z": "Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What does Trueifn_fft! mean in the input size?",
        "Y": "fft_size",
        "Z": "The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the default signal to trim?",
        "Y": "whole",
        "Z": "Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Whether the STFT was normalized?",
        "Y": "normalized",
        "Z": "win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Whether the STFT was onesided or false?",
        "Y": "onesided",
        "Z": "win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Whether the output should be complex or if the input should be assumed to derive from a real signal and window?",
        "Y": "return_complex",
        "Z": "win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is returned by sizeendstartstepleftlceil fractextend - textstart",
        "Y": "1-D tensor",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart. Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0. end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "If the data type is not given, infer the data type from the other input arguments.",
        "Y": "Ifdtypeis not given",
        "Z": "Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0. end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "What is the default dtype inferred to be?",
        "Y": "betorch.int64",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart. Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0. end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "Non-integerstep is subject to what when comparing againstend?",
        "Y": "floating point rounding errors",
        "Z": "Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0. end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "What Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked",
        "Y": "Python Functions and Modules",
        "Z": "Creating TorchScript Code Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a script that will be optimized using?",
        "Y": "just-in-time compilation",
        "Z": "Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When is it first called during tracing?",
        "Y": "Compilesfn",
        "Z": "Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a scriptModule that will be optimized using?",
        "Y": "just-in-time compilation",
        "Z": "Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a wrapper around for atorch.jit.Future[T]asynchronous tasks?",
        "Y": "C++torch::jit::Module",
        "Z": "Creating TorchScript Code Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tra",
        "Y": "Python",
        "Z": "TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Any TorchScript program can be saved from a Python process and loaded in a process where there is no what?",
        "Y": "Python dependency",
        "Z": "Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Python programs may be disadvantageous for what reasons?",
        "Y": "performance and multi-threading reasons",
        "Z": "Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "TorchScript is a way to create serializable and optimizable models from PyTorch code.",
        "Y": "TorchScript",
        "Z": "TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used to compile TorchScript code?",
        "Y": "TorchScript compiler",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used to optimize a script?",
        "Y": "just-in-time compilation",
        "Z": "Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When it is first called during tracing?",
        "Y": "Compilesfn",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an executableScriptModule that will be optimized using?",
        "Y": "just-in-time compilation",
        "Z": "Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a Wrapper around C++torch::jit::Module?",
        "Y": "represents a single function and does not have any attributes or Parameters",
        "Z": "Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a gentle introduction to?",
        "Y": "TorchScript",
        "Z": "Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When it is first called during tracing, what does Compilesfn do?",
        "Y": "Compilesfn",
        "Z": "Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is created by the asynchronous task executingfuncand a reference to the value of the result of this execution?",
        "Y": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the wrapper around C++torch::jit::Module?",
        "Y": "Functionally equivalent to aScriptModule",
        "Z": "Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the benefit of using TorchScript?",
        "Y": "Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency",
        "Z": "PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a TorchScript program that can be run independently from Python?",
        "Y": "a standalone C++ program",
        "Z": "PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Python Functions and Modules Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently As",
        "Y": "Python Language Reference Comparison",
        "Z": "Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where can a TorchScript program be run independently from Python?",
        "Y": "a standalone C++ program",
        "Z": "Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix",
        "Y": "Python Language Reference Comparison Debugging",
        "Z": "Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "A wrapper around what. Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or",
        "Y": "C++torch::jit::Module",
        "Z": "Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does JIT do for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Append",
        "Y": "Disable JIT",
        "Z": "Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What action does TorchScript perform?",
        "Y": "Forces completion of atorch.jit.Future[T]asynchronous task",
        "Z": "Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Freezing aScriptModulewill what?",
        "Y": "clone it",
        "Z": "Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create",
        "Y": "Known Issues",
        "Z": "Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is saved an offline version of aScriptModule?",
        "Y": "Save an offline version",
        "Z": "Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Any TorchScript program can be saved from a what process?",
        "Y": "Python",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you do to save an offline version of this module?",
        "Y": "Save an offline version of this module",
        "Z": "Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will be used to optimize a script?",
        "Y": "just-in-time compilation",
        "Z": "Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What version of the module can be saved for use in the TorchScript IR Graph?",
        "Y": "offline",
        "Z": "Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Freezing aScriptModulewill clone it and attempt to inline what as constants in the TorchScript IR",
        "Y": "attempt to inline the cloned module\u2019s submodules, parameters, and attributes",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What version of the module can be saved for use in a separate process?",
        "Y": "offline",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does aScriptModule do?",
        "Y": "Load aScriptModuleorScriptFunction",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Scripting a function ornn.Module compile as using the TorchScript compiler?",
        "Y": "TorchScript code",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What function is called when it is first called during tracing?",
        "Y": "Compilesfn",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the method that provides for conatiner type refinement in TorchScript?",
        "Y": "a pass-through function that returnsvalue",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does asynchronous task executingfuncand a reference to the value of the result of this execution do?",
        "Y": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the TorchScript method that indicates to the compiler that the left-hand expression is a class instance attribute with type oftype?",
        "Y": "a pass-through function that returnsvalue",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a pass-through function that indicates to the TorchScript compiler that the left-hand side expression is a class instance attribute with type",
        "Y": "returnsthe_value",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used when it is first called during tracing?",
        "Y": "Compilesfn",
        "Z": "Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Trace a module and return an executableScriptModule that will be optimized using just-in-time compilation?",
        "Y": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution",
        "Z": "Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the difference between C++torch::jit::Module and aScriptModule?",
        "Y": "represents a single function and does not have any attributes or Parameters",
        "Z": "Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does tracing create?",
        "Y": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution",
        "Z": "Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of version of a script module can you save for use in a separate process?",
        "Y": "offline",
        "Z": "Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When are traced functions particularly useful?",
        "Y": "when you need to use control-flow around a simple feed-forward model",
        "Z": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a scripted function that can call an encoder module generated using tracing?",
        "Y": "a traced function in script",
        "Z": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of version of a module can you save for use in a separate process?",
        "Y": "offline",
        "Z": "Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The beam search of a sequence to sequence model will typically be written in script but can call what?",
        "Y": "an encoder module generated using tracing",
        "Z": "Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does C++torch::jit::Module have in common with aScriptModule?",
        "Y": "represents a single function and does not have any attributes or Parameters",
        "Z": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing and scripting can be composed to what?",
        "Y": "suit the particular requirements of a part of a model",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Scripted functions can call what?",
        "Y": "traced functions",
        "Z": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used to generate an encoder module?",
        "Y": "tracing",
        "Z": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the difference between aScriptModule and C++torch::jit::Module?",
        "Y": "represents a single function and does not have any attributes or Parameters",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can a traced function do?",
        "Y": "can call an encoder module generated using tracing",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a pass-through function that indicates to the TorchScript compiler the type of the_value?",
        "Y": "returnsthe_value",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can a traced function call?",
        "Y": "can call an encoder module generated using tracing",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can an encoder module be generated using?",
        "Y": "tracing",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Debugging withpdbworks except for what?",
        "Y": "when we invoke the@torch.jit.scriptfunction",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can we disable JIT?",
        "Y": "globally disable JIT",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript provide a code pretty-printer for allScriptModuleinstances?",
        "Y": "Python syntax",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will you need to access.codeon if theScriptModulehas more than one method?",
        "Y": "the module",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the output produced by the example above?",
        "Y": "TorchScript\u2019s compilation of the code for theforwardmethod",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "TorchScript has a representation at a lower level than the code pretty- printer, in the form of what?",
        "Y": "IR graphs",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a static single assignment?",
        "Y": "SSA",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the C++ backend of PyTorch?",
        "Y": "ATen",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The graph follows the same rules described in what section?",
        "Y": "Inspecting Codesection",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "We will be able to step into the@torch.jit.scriptfunction as what?",
        "Y": "normal Python function",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the code pretty-printer give an interpretation of the script method\u2019s code as valid?",
        "Y": "Python syntax",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "If theScriptModulehas more than one method, you will need to what?",
        "Y": "access.codeon the method itself and not the module",
        "Z": "AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript's static single assignment?",
        "Y": "SSA",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What document describes the rules for forwardmethod lookup?",
        "Y": "theInspecting Codesection",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the schema for?",
        "Y": "built-in functions likeaten",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What language does TorchScript provide a code pretty-printer for allScriptModuleinstances?",
        "Y": "Python syntax",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What produces this output?",
        "Y": "The example above",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the output of the example above?",
        "Y": "TorchScript\u2019s compilation of the code for theforwardmethod",
        "Z": "AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you use this output for?",
        "Y": "to ensure TorchScript (tracing or scripting) has captured your model code correctly",
        "Z": "AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where are these associatedblocks found?",
        "Y": "In the graph print-out",
        "Z": "AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you use TorchScript\u2019s compilation of the code for theforwardmethod?",
        "Y": "to ensure TorchScript (tracing or scripting) has captured your model code correctly",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Why are operators formatted in the graph print-out?",
        "Y": "to reflect their equivalent source code forms to facilitate easy debugging",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is dependent on the underlying code?",
        "Y": "Tracing of control flow",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "TorchScript follows the same rules described in theInspecting Codesection with regard to what?",
        "Y": "forwardmethod lookup",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of in-place operations of tensor views (e.g. what?",
        "Y": "indexing",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript use to assign the output to a (unique) value namedrv.1?",
        "Y": "%rv.1:Tensormeans",
        "Z": "TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What are some cases where the trace of a given Python function/module will not be representative of the underlying code?",
        "Y": "edge cases",
        "Z": "TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations",
        "Y": "edge cases",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of in-place operations of tensor views (e.g., what on the left-hand side of an assignment)",
        "Y": "indexing",
        "Z": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this message indicate to us that the computation differed between when we first traced it and when we traced it with traced?",
        "Y": "diagnostic information",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where is the location in the original source file that generated this instruction?",
        "Y": "on line 9, and at character 10",
        "Z": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this message indicate to us that the computation differed between when we first traced it and when we traced it with thecheck_in",
        "Y": "diagnostic information",
        "Z": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What section describes the rules for forwardmethod lookup?",
        "Y": "theInspecting Codesection",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the tracer produce?",
        "Y": "The tracer produces warnings for several problematic patterns in traced computation",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Which version of TorchScript does this section detail the changes to TorchScript in?",
        "Y": "PyTorch 1.2",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "If you are new to TorchScript you can do what?",
        "Y": "skip this section",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Torch.jit.script now do?",
        "Y": "attempt to recursively compile functions, methods, and classes",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule?",
        "Y": "2.torch.jit.script(nn_module_instance)",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Methods called fromforwardare what in the order they are used inforward?",
        "Y": "lazily compiled",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To compile a method other thanforwardthat is not called fromforward, what is done?",
        "Y": "add@torch.jit.export",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To stop the compiler from what, add@torch.jit.ignoreor@torch.jit.unused. @ignor",
        "Y": "compiling a method",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @ignoreleaves the method as a call to python?",
        "Y": "@ignoreleaves the method as a call to python",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "@ignoredcannot be exported;@unusedcan?",
        "Y": "@ignoredcannot be exported;@unusedcan",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Annotate their types usingPEP 526-styleclass annotations.",
        "Y": "empty container types",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is now the preferred way to createScriptModules?",
        "Y": "2.torch.jit.script(nn_module_instance)",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Methods called fromforwardare what?",
        "Y": "lazily compiled",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Add@torch.jit.ignoreor@torch.jit.unused. @ignoreleaves the method as a",
        "Y": "compiling a method",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "For empty container types, what should empty container types do?",
        "Y": "annotate their types usingPEP 526-styleclass annotations",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Before PyTorch 1.2 what decorator was used to make a function or method callable from code that is exported?",
        "Y": "@ignore",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What did PyTorch 1.2 use to get the @ignore decorator back?",
        "Y": "use@torch.jit.unused()",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To stop the compiler from compiling a method, add what?",
        "Y": "@torch.jit.ignoreor@torch.jit.unused",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached).",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What eleaves the method as a call to python?",
        "Y": "@ignor",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached).",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can empty container types do?",
        "Y": "annotate their types",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached).",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "See@torch.jit.ignoreand@torch.jit.unusedfor what?",
        "Y": "details",
        "Z": "The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What compiler compiles the module?",
        "Y": "TorchScript",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached).",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is compiled in the order they are used inforward?",
        "Y": "@torch.jit.exportmethods",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used as an entry point into aScriptModuleand should be compiled?",
        "Y": "annn.Module",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a function that does not need a decorator?",
        "Y": "@torch.jit.exporton a method",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To get this functionality back, what did PyTorch 1.2 use to get it back?",
        "Y": "use@torch.jit.unused()",
        "Z": "Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used as an entry point into aScriptModule and should be compiled?",
        "Y": "annn.Module",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a Python 3 type hints?",
        "Y": "@torch.jit.exporton a method",
        "Z": "Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "TorchScript class support is what?",
        "Y": "experimental",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Warning TorchScript class support is experimental. Currently it is best suited for what?",
        "Y": "simple record-like types",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Everything in a user definedTorchScript Classis exported what way?",
        "Y": "by default",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When does the @torch.jit.ignoreannotation\u2019s behavior change?",
        "Y": "PyTorch 1.2",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To get this functionality back, use what?",
        "Y": "@torch.jit.unused()",
        "Z": "The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Functions don\u2019t change much, they can be decorated with what?",
        "Y": "@torch.jit.ignoreortorch.jit.unusedif needed",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript class support best suited for?",
        "Y": "simple record-like types",
        "Z": "The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the TorchScript compiler need to know the types ofmodule attributes?",
        "Y": "Most types",
        "Z": "The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What decorator was used to make a function or method callable from code that is exported?",
        "Y": "@ignore",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @torch.jit.ignore and@torch.jit.unused provide?",
        "Y": "details",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What cannot have their types inferred from the value of the member?",
        "Y": "Empty lists and dicts",
        "Z": "Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What cannot have their types inferred and must have their types annotated withPEP 526-styleclass?",
        "Y": "Empty lists and dicts",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in what?",
        "Y": "PyTorch 1.2",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used to get the @ignore functionality back?",
        "Y": "@torch.jit.unused()",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "See@torch.jit.ignoreand@torch.jit.unused for what?",
        "Y": "details",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Warning TorchScript class support is what?",
        "Y": "experimental",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "If a type cannot be inferred and is what?",
        "Y": "not explicitly annotated",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a method that does not need the @ignore decorator?",
        "Y": "@torch.jit.exporton a method",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What specifies the subscripts for each dimension of the inputoperandsin the same order as the dimensions?",
        "Y": "Theequationstring",
        "Z": "Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the exception to the Einstein summation convention?",
        "Y": "if a subscript is repeated for the same input operand",
        "Z": "Sums the product of the elements of the inputoperandsalong dimensions specified using a notation\nbased on the Einstein summation convention. Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on what?",
        "Y": "Einstein",
        "Z": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the summation subscript in Einsum astorch.einsum?",
        "Y": "j",
        "Z": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the exception to Einsum's broadcastable format?",
        "Y": "if a subscript is repeated for the same input operand",
        "Z": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What does the PyTorchaudiopackage consist of?",
        "Y": "Package Reference PyTorch Libraries",
        "Z": "This library is part of thePyTorchproject. PyTorch is an open source\nmachine learning framework. Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally\nbe no major performance limitations or gaps in documentation.\nWe also expect to maintain backwards compatibility (although\nbreaking changes can happen and notice will be given one release ahead\nof time). Beta:Features are tagged as Beta because the API may change based on\nuser feedback, because the performance needs to improve, or because\ncoverage across operators is not yet complete. For Beta features, we are\ncommitting to seeing the feature through to the Stable classification.\nWe are not, however, committing to backwards compatibility. Prototype:These features are typically not available as part of\nbinary distributions like PyPI or Conda, except sometimes behind run-time\nflags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries",
        "source": "https://pytorch.org/audio/stable/index.html"
    },
    {
        "X": "Return the what value for the given nonlinearity function?",
        "Y": "recommended gain value",
        "Z": "Return the recommended gain value for the given nonlinearity function.\nThe values are as follows: nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the recommended gain value for the given nonlinearity function?",
        "Y": "nonlinearity gain",
        "Z": "Return the recommended gain value for the given nonlinearity function.\nThe values are as follows: nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is nonlinearity?",
        "Y": "non-linear function",
        "Z": "nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the Tensor mean?",
        "Y": "the mean of the normal distribution std",
        "Z": "Return the recommended gain value for the given nonlinearity function.\nThe values are as follows: nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013",
        "Y": "n-dimensionaltorch",
        "Z": "Return the recommended gain value for the given nonlinearity function.\nThe values are as follows: nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is Linear / Identity 111 Conv1,2,3D 111 Sigmoid 111 Tanh 53frac",
        "Y": "nonlinearity gain",
        "Z": "nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the default gain forSELUsacrifices?",
        "Y": "the normalisation effect",
        "Z": "nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is 111 Conv1,2,3D 111 Sigmoid 111 Tanh?",
        "Y": "Linear / Identity",
        "Z": "Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the name of the non-linear function?",
        "Y": "nonlinearity",
        "Z": "Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the value to fill the tensor with Examples Fills the input Tensor with?",
        "Y": "scalar value1",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does Linear / Identity 111 Conv1,2,3,3D 111 Tanh 53frac5335",
        "Y": "tensor",
        "Z": "Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is another name for 111 Sigmoid 111 Tanh?",
        "Y": "111 Sigmoid 111 Tanh",
        "Z": "111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What effect does the default gain forSELUsacrifices have for more stable gradient flow in rectangular layers?",
        "Y": "the normalisation effect",
        "Z": "Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the",
        "Y": "nonlinearity",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does a Tensor Example Fill the input Tensor with?",
        "Y": "scalar value0",
        "Z": "111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is Sigmoid 111 Tanh?",
        "Y": "Sigmoid 111 Tanh",
        "Z": "Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does a tensor fill the input Tensor with?",
        "Y": "scalar value0",
        "Z": "Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What do Tensor Examples Fill?",
        "Y": "2-dimensional input",
        "Z": "111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What should you do in order to implementSelf-Normalizing Neural Networks?",
        "Y": "usenonlinearity='linear'instead ofnonlinearity='selu'",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does an n-dimensionaltorch.Tensor Examples Fill the input Tensor with?",
        "Y": "scalar value0",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does an n-dimensionaltorch.Tensor Examples Fill?",
        "Y": "2-dimensional inputTensorwith the identity matrix",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What Preserves the identity of the inputs inLinearlayers?",
        "Y": "Preserves the identity of the inputs inLinearlayers,",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does an n-dimensionaltorch.Tensor Example Fills the input Tensor with?",
        "Y": "scalar value1",
        "Z": "nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the 2-dimensional inputTensor filled with?",
        "Y": "identity matrix",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Preserves the identity of the inputs inLinearlayers, where where as many inputs are preserved as possible?",
        "Y": "as many inputs are preserved as possible",
        "Z": "Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does the 3, 4, 5-dimensional inputTensorwith?",
        "Y": "Dirac delta function",
        "Z": "nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Preserves the identity of the inputs in what?",
        "Y": "Convolutionallayers",
        "Z": "nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "In case of what, each group of channels preserves identity tensor\u2013a 3, 4, 5-dimensionaltorch.T",
        "Y": "groups>1",
        "Z": "nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does param stand for?",
        "Y": "optional parameter",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What function does the tensor fill the 3, 4, 5-dimensional inputTensorwith?",
        "Y": "Dirac delta function",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Glorot, what is Glorot?",
        "Y": "X",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Who is Glorot, X. & Ben?",
        "Y": "& Ben",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does tensor Examples Fill the input Tensor with?",
        "Y": "scalar value0",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does fills the 2-dimensional inputTensorwith?",
        "Y": "identity matrix",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What function does the 3, 4, 5-dimensional inputTensor have?",
        "Y": "Dirac delta function",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Who was the author of Understanding the difficulty of training deep feedforward neural networks?",
        "Y": "Y.",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How does Glorot describe the difficulty of training deep feedforward neural networks?",
        "Y": "using a uniform distribution",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "a\u2013 what is the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal distribution?",
        "Y": "the lower bound of the uniform distribution",
        "Z": "a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does an n-dimensionaltorch.Tensor Examples Fills the input Tensor with?",
        "Y": "scalar value0",
        "Z": "a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Who is the author of Understanding the difficulty of training deep feedforward neural networks?",
        "Y": "Bengio",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How does Glorot describe training deep feedforward neural networks?",
        "Y": "using a uniform distribution",
        "Z": "b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What function does a 2-dimensionaltorch.Tensor Examples Fills the 3, 4, 5-dimensional inputTens",
        "Y": "Dirac delta function",
        "Z": "b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is it called when a tensor has values sampled fromU(a,a)mathcalU(",
        "Y": "Glorot initialization",
        "Z": "b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "In case of groups>1, each group of channels preserves identity <sep>",
        "Y": "In case of groups>1, each group of channels preserves identity",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Who were Glorot, X. & Bengio, Y. (2010)?",
        "Y": "& Bengio, Y.",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "In what year did Glorot, X. & Bengio, Y. begin training deep feedforward neural networks?",
        "Y": "2010",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is another name for Glorot?",
        "Y": "Glorot initialization",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is a Tensor gain?",
        "Y": "an optional scaling factor",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "When was Glorot, X. & Bengio, Y. born?",
        "Y": "2010",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the result of the resulting tensor?",
        "Y": "The resulting tensor",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Fills the 3, 4, 5-dimensional inputTensorwith what function?",
        "Y": "Dirac delta function",
        "Z": "Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is another name for Glorot initialization?",
        "Y": "Glorot initialization",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is a tensor gain?",
        "Y": "an optional scaling factor",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Glorot, X. & Bengio, Y. (2010), using what distribution?",
        "Y": "normal distribution",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is an optional scaling factor in a tensor gain?",
        "Y": "an optional scaling factor",
        "Z": "Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does tensor fill the 2-dimensional inputTensorwith?",
        "Y": "identity matrix",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What function does tensor fill the 3, 4, 5-dimensional inputTensorwith?",
        "Y": "Dirac delta function",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the resulting tensor's values sampled fromN(0,std2)mathcalN(",
        "Y": "Glorot initialization",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is an optional tensor gain?",
        "Y": "scaling factor",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is batch1(Tensor)?",
        "Y": "first batch of matrices to be multiplied batch2(Tensor)",
        "Z": "Performs a batch matrix-matrix product of matrices stored\ninbatch1andbatch2,\nwith a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).inputis added to the final result. batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) \u2013 the first batch of matrices to be multiplied batch2(Tensor) \u2013 the second batch of matrices to be multiplied beta(Number,optional) \u2013 multiplier forinput(\u03b2\\beta\u03b2) input(Tensor) \u2013 matrix to be added alpha(Number,optional) \u2013 multiplier forbatch1 @ batch2(\u03b1\\alpha\u03b1) out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "What is an example of a batch matrix-matrix product?",
        "Y": "Example",
        "Z": "Performs a batch matrix-matrix product of matrices stored\ninbatch1andbatch2,\nwith a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).inputis added to the final result. batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) \u2013 the first batch of matrices to be multiplied batch2(Tensor) \u2013 the second batch of matrices to be multiplied beta(Number,optional) \u2013 multiplier forinput(\u03b2\\beta\u03b2) input(Tensor) \u2013 matrix to be added alpha(Number,optional) \u2013 multiplier forbatch1 @ batch2(\u03b1\\alpha\u03b1) out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "What does this class wrap?",
        "Y": "arbitraryoptim.Optimizer",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What happens after parameters are updated locally?",
        "Y": "each rank will broadcast its parameters to all other peers",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What kind of algorithm does ZeroRedundancyOptimizer use?",
        "Y": "greedy",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Each parameter belongs to what?",
        "Y": "a single rank",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the partition of each parameter at each rank?",
        "Y": "arbitrary",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(tor",
        "Y": "local optimizer",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What group(ProcessGroup, optional) \u2013torch.distributedProcessGroup?",
        "Y": "group(ProcessGroup, optional) \u2013torch.distributedProcessGroup",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "When enabled, parameters will be packed into what?",
        "Y": "larger buckets",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "When disabled, each what will be communicated separately?",
        "Y": "individual parameter",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "When can a param group be useful?",
        "Y": "when fine tuning a pre-trained network",
        "Z": "params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How many update the consolidated state_dict list?",
        "Y": "one per rank",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Restore the global parameter groups. to(int) \u2013 the rank that receives the global states. (default: 0) <sep>",
        "Y": "Restore the global parameter groups",
        "Z": "This class wraps an arbitraryoptim.Optimizerand shards its states across ranks in the group as described byZeRO. The optimizer instance in each rank is only responsible for\nupdating1/world_sizeparameters and hence only needs to keep1/world_sizeoptimizer states. After parameters are updated locally,\neach rank will broadcast its parameters to all other peers to keep all\nmodel replicas in the same state.ZeroRedundancyOptimizercan be used\nin conjunction withtorch.nn.parallel.DistributedDataparallelto\nreduce per-rank peak memory consumption. ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What uses a greedy algorithm to pack a number of parameters at each rank?",
        "Y": "ZeroRedundancyOptimizer",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the partition of the algorithm?",
        "Y": "arbitrary",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the default behavior of ZeroRedundancyOptimizer?",
        "Y": "all trailing arguments will be forwarded to the given optimizer",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "When can adding a param group to theOptimizersparam_groups be useful?",
        "Y": "when fine tuning a pre-trained network",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How many consolidated state_dict lists are updated per rank?",
        "Y": "one per rank",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "To(int) \u2013 the rank that receives the global states. (default: what?",
        "Y": "0",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "state_dict(dict) \u2013 what state should be an object returned from a call tostate_dict() Gets this rank\u2019sstate",
        "Y": "optimizer state",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What should state_dict(dict) be?",
        "Y": "an object returned from a call tostate_dict() Gets this rank\u2019sstate_dict",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a list ofparam_groups?",
        "Y": "a list ofparam_groups",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What corresponds to the param_groups for a rank?",
        "Y": "Element 0",
        "Z": "ZeroRedundancyOptimizeruse a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is params(Iterable) \u2013 anIterableoftorch.Tensors?",
        "Y": "optimizer",
        "Z": "params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "When disabled, what happens to each individual parameter?",
        "Y": "each individual parameter will be communicated separately",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Update the consolidated state_dict list, how many per rank?",
        "Y": "one per rank",
        "Z": "params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Restore the global parameter groups as well as the what?",
        "Y": "shard",
        "Z": "params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "state_dict(dict) \u2013 what?",
        "Y": "optimizer state",
        "Z": "params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "State_dict(dict) \u2013 optimizer state. Should be what?",
        "Y": "an object returned from a call tostate_dict() Gets this rank\u2019sstate_dict",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Which element corresponds to rank 0, etc. We need all the ranks for the broadcast insidestep(). Returns the local_state_dict for",
        "Y": "Element 0",
        "Z": "params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Returns what for a given rank?",
        "Y": "local_state_dict",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the class of the local optimizer?",
        "Y": "optimizer_class",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What should be optimized along with group specific optimization options?",
        "Y": "Tensors",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What corresponds to rank 0, etc. We need all the ranks for the broadcast insidestep(). Returns the local_state_dict for a",
        "Y": "Element 0",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "globalstate_dict is what?",
        "Y": "last known global optimizer state",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What performs performs?",
        "Y": "Performs",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the name of the index that returns a new tensor which indexes theinputtensor along dimensiondimusing",
        "Y": "aLongTensor",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "What is the boolean maskmask that returns a new 1-D tensor which indexes theinputtensor",
        "Y": "aBoolTensor",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "What function returns a tensor with the same data and number of elements asinput, but with the specified shape?",
        "Y": "Alias oftorch.vstack()",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "What is the name of the index which indexes theinputtensor along dimensiondimusing the entries inindex?",
        "Y": "aLongTensor",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "What is returned with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive",
        "Y": "a tensor",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "Sets the seed for generating random numbers. Returns the random number generator state as atorch.ByteTensor.",
        "Y": "random number generator state",
        "Z": "Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean what?",
        "Y": "0 and variance 1",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "What does Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Y": "a tensor",
        "Z": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "What is returned if a tensor is filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)",
        "Y": "a tensor",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "Returns the indices of what value of all elements in theinputtensor?",
        "Y": "the maximum value",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. Returns the mean",
        "Y": "p-norm of (input-other) Returns the log of summed exponentials",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "Computes what quantiles of each row of theinputtensor along the dimensiondim?",
        "Y": "q-th",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "What is the result of Computes the q-th quantiles of each row of theinputtensor along the dimensiondim",
        "Y": "variant oftorch.quantile()that \u201cignores\u201dNaNvalues",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#pointwise-ops"
    },
    {
        "X": "Computes the error function ofinput. The error function is defined as follows: what is the input tensor?",
        "Y": "input(Tensor) \u2013 the input tensor",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input ten",
        "Y": "output tensor",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "What is the name of the function that Computes the error function ofinput?",
        "Y": "Computes the complementary error function ofinput",
        "Z": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "Computes the complementary error function ofinput. The complementary error function is defined as follows: what is the input tensor?",
        "Y": "input(Tensor) \u2013 the input tensor",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "What is the name of the function that Computes the inverse error function ofinput?",
        "Y": "Computes the inverse error function ofinput",
        "Z": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "What is an example of a Computes the exponential of the elements minus 1 ofinput?",
        "Y": "Computes the exponential of the elements minus 1 ofinput",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "Computes the elements minus 1 ofinput. Note This function provides greater precision than exp(x) - 1 for small values of",
        "Y": "exponential",
        "Z": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "What is an example of a function that computes the error function ofinput?",
        "Y": "Computes the complementary error function ofinput",
        "Z": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "Computes the first kind of what function for each element ofinput?",
        "Y": "exponentially scaled zeroth order modified Bessel function",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "Computes the first kind for each element ofinput. input(Tensor) \u2013 the input tensor. out(",
        "Y": "exponentially scaled zeroth order modified Bessel function",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "What is an example of a function that computes the inverse error function ofinput?",
        "Y": "Computes the inverse error function ofinput",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erf"
    },
    {
        "X": "What is another name for the median(dim=0) input?",
        "Y": "input tensor",
        "Z": "Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What contains the median of each row of input in the dimension dim?",
        "Y": "values",
        "Z": "Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "By default, what is the last dimension of the input tensor?",
        "Y": "dim",
        "Z": "By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is the last dimension of the input tensor?",
        "Y": "dim",
        "Z": "By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is another name for median(dim=0) input?",
        "Y": "input tensor",
        "Z": "This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, output tensors are of the same size as input except in the dimension dim where they are of size 1,",
        "Y": "keepdim is True",
        "Z": "Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, the outputs tensor has how many dimensions less than input?",
        "Y": "1 fewer dimension",
        "Z": "Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What does torch.squeeze() do to result in the outputs tensor having 1 fewer dimension than input?",
        "Y": "Note",
        "Z": "Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "By default, dim is what dimension of the input tensor?",
        "Y": "last dimension",
        "Z": "Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, output tensors are of what size?",
        "Y": "1 fewer dimension",
        "Z": "input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size",
        "Y": "1",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What does keepdim do when the output tensors are of the same size as input?",
        "Y": "Note",
        "Z": "By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "When dim is squeezed, the outputs tensor has how many dimensions less than input?",
        "Y": "1 fewer dimension",
        "Z": "This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, the output tensors are of what size?",
        "Y": "the same size as input",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What does keepdim do if keepdim is True?",
        "Y": "Note",
        "Z": "By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, what happens to the output tensors if dim is the last dimension of the input tensor",
        "Y": "squeezed",
        "Z": "By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, output tensors have how many dimensions less than input?",
        "Y": "1 fewer dimension",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is not unique for input tensors with an even number of elements in the dimension dim?",
        "Y": "The median is not unique",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is returned when the median is not unique for input tensors with an even number of elements in the dimension dim?",
        "Y": "lower of the two medians",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What does torch.quantile() use to compute the mean of both medians in input?",
        "Y": "q=0.5",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is the warning that the output tensors are not unique for input tensors with an even number of elements in the dimension",
        "Y": "Warning",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, output tensors have how much less dimension than input?",
        "Y": "1",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Which of the two medians is returned if keepdim is True?",
        "Y": "lower",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What function computes the mean of both medians in input?",
        "Y": "torch.quantile()",
        "Z": "Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is the name of the warning that the output tensors are not unique for input tensors with an even number of elements",
        "Y": "Warning",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What does it mean that the median is not unique for input tensors with an even number of elements in the dimension dim?",
        "Y": "Note",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Which of the two medians is returned in the case of input tensors with an even number of elements in dimension dim?",
        "Y": "lower",
        "Z": "The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is used to compute the mean of both medians in input?",
        "Y": "torch.quantile()",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is the warning that the median is not unique for input tensors with even number of elements in dimension dim?",
        "Y": "Warning",
        "Z": "The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "The median is not unique for input tensors with an what?",
        "Y": "even number of elements",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Which of the two medians is returned?",
        "Y": "lower",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What does torch.quantile() do?",
        "Y": "Eliminates all but the first element from every consecutive group of equivalent elements",
        "Z": "Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "The median is not unique for what with an even number of elements in the dimension dim?",
        "Y": "input tensors",
        "Z": "The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What does not necessarily contain the first occurrence of each median value found?",
        "Y": "Warning indices",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "The exact implementation details are what?",
        "Y": "device-specific",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Do not expect the same result when run on what two devices in general?",
        "Y": "CPU and GPU",
        "Z": "Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What do not expect to be deterministic?",
        "Y": "gradients",
        "Z": "indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What are the two main CPUs used in Warning indices?",
        "Y": "CPU and GPU",
        "Z": "Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Do not expect what to be deterministic?",
        "Y": "gradients",
        "Z": "indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Do not expect the same result when run on what two devices?",
        "Y": "CPU and GPU",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "indices does not contain the first occurrence of each median value found unless it is what?",
        "Y": "unique",
        "Z": "indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "On what devices do not expect the same result?",
        "Y": "CPU and GPU",
        "Z": "indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "The first tensor will be populated with the median values and the second tensor, with their indices in the dimension dim",
        "Y": "out",
        "Z": "input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is an example of a tensor that must have indices in the dimension dim of input?",
        "Y": "Example",
        "Z": "input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What type of data format does ONNX use?",
        "Y": "external data format",
        "Z": "Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of an Operator Export Type?",
        "Y": "Frequently Asked Questions",
        "Z": "Example: End-to-end AlexNet from PyTorch to ONNX Tracing vs Scripting Type Annotations Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is used to handle Named Arguments as model inputs?",
        "Y": "dictionaries",
        "Z": "Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What field does PyTorch avoid using?",
        "Y": ".data field",
        "Z": "Tracing vs Scripting Type Annotations Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do dictionaries handle as model inputs?",
        "Y": "Named Arguments",
        "Z": "Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of annotations are used to write PyTorch model in Torch way?",
        "Y": "Type Annotations",
        "Z": "Type Annotations Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you write a PyTorch model?",
        "Y": "Write PyTorch model in Torch way",
        "Z": "Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can be used to handle Named Arguments as model inputs?",
        "Y": "dictionaries",
        "Z": "Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Using dictionaries to handle what as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support",
        "Y": "Named Arguments",
        "Z": "Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the external data format?",
        "Y": "Training Functions",
        "Z": "Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the script save the traced model to?",
        "Y": "alexnet.onnx",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTH",
        "Y": "Operator Export Type",
        "Z": "Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How does the script export a pretrained AlexNet into ONNX?",
        "Y": "runs a single round of inference and then saves the resulting traced model to alexnet.onnx",
        "Z": "Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How does the script save the resulting traced model to alexnet?",
        "Y": "runs a single round of inference",
        "Z": "Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the support for Limitations?",
        "Y": "TorchVision",
        "Z": "TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the Operator Export Type?",
        "Y": "ONNX",
        "Z": "Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of operator does ONNX support?",
        "Y": "Operator Export Type",
        "Z": "Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the script run?",
        "Y": "single round of inference",
        "Z": "Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How does the script export a pretrained AlexNet?",
        "Y": "runs a single round of inference and then saves the resulting traced model to alexnet.onnx",
        "Z": "Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of operators does ONNX support?",
        "Y": "Operator Export Type",
        "Z": "Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the script export into ONNX?",
        "Y": "a pretrained AlexNet",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The script exports a pretrained AlexNet as defined in what?",
        "Y": "torchvision",
        "Z": "RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX",
        "Y": "Custom operators",
        "Z": "Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of Custom operators?",
        "Y": "Operator Export Type",
        "Z": "Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the script run and then saves the resulting traced model to alexnet?",
        "Y": "single round of inference",
        "Z": "ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH",
        "Y": "ONNX",
        "Z": "ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does _ATEN stand for?",
        "Y": "ONNX",
        "Z": "ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does _ATEN_FALLBACK RAW ONNX_FALLTHROUGH stand for?",
        "Y": "ONNX",
        "Z": "ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the RAW script that exports a pretrained AlexNet as defined in torchvision into?",
        "Y": "ONNX",
        "Z": "RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does _FALLTHROUGH stand for?",
        "Y": "ONNX",
        "Z": "ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a good place to start when using external data format Training Functions?",
        "Y": "Frequently Asked Questions",
        "Z": "Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The script exports a pretrained AlexNet as defined in torchvision into what?",
        "Y": "ONNX",
        "Z": "Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the external data format?",
        "Y": "Training Functions",
        "Z": "Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the script run and then saves the resulting traced model to alexnet.onnx?",
        "Y": "single round of inference",
        "Z": "Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the script export a pretrained AlexNet into?",
        "Y": "ONNX",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What keyword causes the exporter to print out a human-readable representation of the network?",
        "Y": "verbose=True",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a script that exports a pretrained AlexNet as defined in torchvision into ONNX?",
        "Y": "Training Functions",
        "Z": "Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Functions Here exports a pretrained AlexNet as defined in torchvision into what?",
        "Y": "ONNX",
        "Z": "Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where does the script save the traced model to?",
        "Y": "alexnet.onnx",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the binary protobuf file that contains both the network structure and parameters of the model you exported?",
        "Y": "alexnet.onnx",
        "Z": "Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you install ONNX with?",
        "Y": "conda",
        "Z": "ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the binary protobuf file that contains both the network structure and parameters of the model you exported?",
        "Y": "alexnet.onnx",
        "Z": "The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the program that installs ONNX?",
        "Y": "conda",
        "Z": "The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you verify using the ONNX library?",
        "Y": "the protobuf",
        "Z": "You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do you need to install to run the exported script?",
        "Y": "caffe2",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the backend for ONNX?",
        "Y": "Caffe2",
        "Z": "ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you use for ONNX Runtime?",
        "Y": "the backend",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What library can you use to verify the protobuf?",
        "Y": "ONNX library",
        "Z": "You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How can you install ONNX?",
        "Y": "conda",
        "Z": "You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will you need to install to run the exported model?",
        "Y": "ONNX Runtime",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Once these are installed, you can use the backend for what?",
        "Y": "ONNX Runtime",
        "Z": "Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the exported script need to run with?",
        "Y": "caffe2",
        "Z": "Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX..",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you use for Caffe2?",
        "Y": "the backend",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What model can you export to ONNX?",
        "Y": "SuperResolution model",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you use the backend for once these are installed?",
        "Y": "ONNX Runtime",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is needed to run the exported script?",
        "Y": "caffe2",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX..",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you use the backend for once caffe2 is installed?",
        "Y": "ONNX Runtime",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What backend can you use once these are installed?",
        "Y": "Caffe2",
        "Z": "Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will there be in the future for other frameworks?",
        "Y": "backends",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What types of exports can the ONNX exporter be?",
        "Y": "trace-based and script-based exporter",
        "Z": "The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the future, there will be what for other frameworks as well?",
        "Y": "backends",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Can the ONNX exporter be both trace-based and script-based?",
        "Y": "trace-based and script-based exporter",
        "Z": "Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you install ONNX Runtime?",
        "Y": "follow these instructions",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the backend for ONNX Runtime?",
        "Y": "ONNX Runtime",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you use the backend for?",
        "Y": "ONNX Runtime",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What types of exporter can the ONNX exporter be?",
        "Y": "trace-based and script-based exporter",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What model can be exported to ONNX?",
        "Y": "SuperResolution model",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another tutorial of?",
        "Y": "exporting the SuperResolution model to ONNX",
        "Z": "Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the exporter that can be trace-based and script-based?",
        "Y": "ONNX",
        "Z": "The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What means that the model you are trying to export is a ScriptModule?",
        "Y": "script-based",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What language is TorchScript a subset of?",
        "Y": "Python",
        "Z": "script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does TorchScript allow?",
        "Y": "mixing tracing and scripting",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you compose to suit the particular requirements of a part of a model?",
        "Y": "tracing and scripting",
        "Z": "We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of an ONNX graph that unrolls the for loop?",
        "Y": "trace-based exporter",
        "Z": "script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the core data structure in TorchScript?",
        "Y": "ScriptModule",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we allow mixing in TorchScript?",
        "Y": "tracing and scripting",
        "Z": "script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you do to suit the specific requirements of a part of a model?",
        "Y": "compose tracing and scripting",
        "Z": "script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What unrolls the for loop with trace-based exporter?",
        "Y": "ONNX graph",
        "Z": "script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we allow?",
        "Y": "mixing tracing and scripting",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What happens to the exported ONNX graph?",
        "Y": "the exported ONNX graph becomes:",
        "Z": "We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What two types of exports can be combined?",
        "Y": "tracing and scripting",
        "Z": "We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you compose tracing and scripting to?",
        "Y": "suit the particular requirements of a part of a model",
        "Z": "We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the result of trace-based exporter?",
        "Y": "ONNX graph",
        "Z": "We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Which exporter produces the ONNX graph that unrolls the for loop?",
        "Y": "trace-based exporter",
        "Z": "With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can we verify the dynamic control flow?",
        "Y": "backends with different loop range",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of exporter can we use to capture the dynamic loop?",
        "Y": "script",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can we verify in backends with?",
        "Y": "different loop range",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "TorchScript only supports a subset of what?",
        "Y": "Python types",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you find more details about here?",
        "Y": "type annotation",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of variables does TorchScript only support for script functions?",
        "Y": "single static types",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "By default, each variable is assumed to be what?",
        "Y": "Tensor",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If an argument to a ScriptModule function is not Tensor, its type should be specified using what?",
        "Y": "MyPy-style annotations",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does TorchScript compiler fail with if the type annotation is not specified?",
        "Y": "runtime error",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does TorchScript only support a subset of Python types?",
        "Y": "type annotation",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of variables does TorchScript only support?",
        "Y": "single static types",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What should be used if an argument to a ScriptModule function is not Tensor?",
        "Y": "MyPy-style annotations",
        "Z": "TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere. Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. If the type annotation is not specified, TorchScript compiler fails with the runtime error below.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can PyTorch models be written using?",
        "Y": "numpy manipulations",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does tracing treat the numpy values as?",
        "Y": "the constant node",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the PyTorch model need to implement?",
        "Y": "torch operators",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the PyTorch model not convert to?",
        "Y": "numpy types",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When do we convert PyTorch models to numpy manipulations?",
        "Y": "ONNX model",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "For the trace-based exporter, tracing treats the numpy values as what?",
        "Y": "the constant node",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do not convert to numpy tensors?",
        "Y": "numpy types",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do you always use?",
        "Y": "torch tensors and torch operators",
        "Z": "do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Do not convert to what?",
        "Y": "numpy types",
        "Z": "do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What need to be defined in init function so that inferencing can handle it properly?",
        "Y": "Dropout layer",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for torch operators?",
        "Y": "torch.concat",
        "Z": "do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do not convert to numpy types?",
        "Y": "torch tensors",
        "Z": "do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What layer needs to be defined in init function so that inferencing can handle it properly?",
        "Y": "Dropout layer",
        "Z": "do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a torch operator?",
        "Y": "torch.concat",
        "Z": "Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can the.data field produce?",
        "Y": "an incorrect trace graph",
        "Z": "The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a safer alternative to the.data field?",
        "Y": ".detach()",
        "Z": "The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a torch tensor and torch operator?",
        "Y": "torch.concat",
        "Z": "Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Why is the.data field kept?",
        "Y": "backward compatibility",
        "Z": "The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can the use of the.data field cause?",
        "Y": "an incorrect trace graph",
        "Z": "Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an old field that is kept for backward compatibility and should be avoided when writing models?",
        "Y": ".data field",
        "Z": "The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of an incorrect trace graph?",
        "Y": "ONNX graph",
        "Z": "The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the key represent?",
        "Y": "the name of the argument in the model signature",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the value represent?",
        "Y": "the value of the argument to be passed",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The second method is to represent the keyword arguments as key-value pairs where the key represents the name of the argument in the model signature and what else?",
        "Y": "the value represents the value of the argument to be passed",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where does the key represent the name of the argument?",
        "Y": "the model signature",
        "Z": "The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The first method is to pass all inputs in the same order as required by what?",
        "Y": "the model",
        "Z": "The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How many ways of exporting a model?",
        "Y": "two",
        "Z": "The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are the keyword arguments represented as?",
        "Y": "key-value pairs",
        "Z": "The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the last argument in the args tuple?",
        "Y": "Using a dictionary",
        "Z": "Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The dictionary is always the what in the args tuple?",
        "Y": "last argument",
        "Z": "Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "For cases in which there are no keyword arguments, models can be exported with what?",
        "Y": "empty or no dictionary",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an exception to this rule?",
        "Y": "cases in which the last input is also of a dictionary type",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In cases where the last input is also of a dictionary type, what is the last argument in the args tuple?",
        "Y": "empty dictionary",
        "Z": "An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of an empty dictionary as the last argument in the args tuple?",
        "Y": "example,",
        "Z": "Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of a case where an empty dictionary is the last argument in the args tuple?",
        "Y": "example",
        "Z": "For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of an empty dictionary as the last argument in an args tuple?",
        "Y": "example",
        "Z": "An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the export call assume is intended to represent the optional dictionary consisting of named arguments?",
        "Y": "the \u2018x\u2019 input",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In order to prevent this from being an issue, a constraint is placed to provide an empty dictionary as the last input in what?",
        "Y": "tuple args",
        "Z": "Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would the new call look like?",
        "Y": "The new call would look like this",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The export call assumes that the \u2018x\u2019 input is intended to represent the optional dictionary consisting of what?",
        "Y": "named arguments",
        "Z": "Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is placed to provide an empty dictionary as the last input in the tuple args?",
        "Y": "a constraint",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would look like this?",
        "Y": "The new call",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What indexing is very flexible and complicated in PyTorch?",
        "Y": "Tensor",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How many categories of indexing are there in PyTorch?",
        "Y": "two",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Both categories of indexing are largely supported in what?",
        "Y": "exporting",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the latest version of tensor indexing in PyTorch?",
        "Y": "opset",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where does this type of indexing occur?",
        "Y": "RHS",
        "Z": "This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Export is supported for what version of opset?",
        "Y": "ONNX opset version >= 11",
        "Z": "In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Below is the list of what for RHS indexing?",
        "Y": "supported patterns",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is below the list for RHS indexing?",
        "Y": "unsupported patterns",
        "Z": "This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In code, this type of indexing occurs on what?",
        "Y": "LHS",
        "Z": "In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Export is supported for what version of ONNX opset?",
        "Y": "ONNX opset version >= 11",
        "Z": "This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is below the list of unsupported patterns for LHS indexing?",
        "Y": "supported patterns for LHS indexing",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is below the list for LHS indexing?",
        "Y": "unsupported patterns",
        "Z": "In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Below is the list of what for LHS indexing?",
        "Y": "supported patterns",
        "Z": "And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the latest opset?",
        "Y": "opset_version=12",
        "Z": "And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Export is supported for what version >= 9?",
        "Y": "ONNX opset",
        "Z": "This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that you are exporting with the latest what?",
        "Y": "opset",
        "Z": "In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To what platform are all TorchVision models exportable?",
        "Y": "ONNX",
        "Z": "All TorchVision models, except for quantized versions, are exportable to ONNX.\nMore details can be found in TorchVision.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can be found in TorchVision?",
        "Y": "More details",
        "Z": "All TorchVision models, except for quantized versions, are exportable to ONNX.\nMore details can be found in TorchVision.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "All TorchVision models except quantized versions are exportable to what?",
        "Y": "ONNX",
        "Z": "All TorchVision models, except for quantized versions, are exportable to ONNX.\nMore details can be found in TorchVision.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "All TorchVision models except what are exportable to ONNX?",
        "Y": "quantized versions",
        "Z": "All TorchVision models, except for quantized versions, are exportable to ONNX.\nMore details can be found in TorchVision.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can you find more details about TorchVision models?",
        "Y": "TorchVision",
        "Z": "All TorchVision models, except for quantized versions, are exportable to ONNX.\nMore details can be found in TorchVision.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are only supported as JIT inputs/outputs?",
        "Y": "tuples, lists and Variables",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available. PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are also accepted but their usage is not recommended?",
        "Y": "Dictionaries and strings",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available. PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Users need to verify their dict inputs carefully and keep in mind what is not available?",
        "Y": "dynamic lookups",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Only what are supported as JIT inputs/outputs?",
        "Y": "tuples, lists and Variables",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is not available for dict inputs?",
        "Y": "dynamic lookups",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What backends often have implementations of operators with some numeric differences?",
        "Y": "PyTorch and ONNX backends",
        "Z": "PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Depending on model structure, the numeric differences in PyTorch and ONNX backends may be what?",
        "Y": "negligible",
        "Z": "PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Why do we allow Caffe2 to call directly to Torch implementations of operators?",
        "Y": "to help you smooth over these differences when precision is important",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available. PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Which backends often have implementations of operators with numeric differences?",
        "Y": "PyTorch and ONNX",
        "Z": "PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Depending on model structure, what are the differences between operators implemented by PyTorch and ONNX backends?",
        "Y": "negligible",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available. PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operators are supported?",
        "Y": "BatchNorm ConstantPadNd Conv Dropout Embedding",
        "Z": "The following operators are supported: BatchNorm ConstantPadNd Conv Dropout Embedding (no optional arguments supported) EmbeddingBag FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode is not supported by EmbeddingBag FeatureDropout?",
        "Y": "training mode",
        "Z": "EmbeddingBag FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the training mode not supported?",
        "Y": "EmbeddingBag FeatureDropout",
        "Z": "The following operators are supported: BatchNorm ConstantPadNd Conv Dropout Embedding (no optional arguments supported) EmbeddingBag FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack std sub (nonzero alpha not supported) sum t tan tanh threshold (non-zero threshold/non-zero value not supported) to topk transpose true_divide type_as unbind unfold (experimental support with ATen-Caffe2 integration) unique unsqueeze upsample_nearest1d upsample_nearest2d upsample_nearest3d view weight_norm where zeros zeros_like The operator set above is sufficient to export the following models: AlexNet DCGAN DenseNet Inception (warning: this model is highly sensitive to changes in operator\nimplementation) ResNet SuperResolution VGG word_language_model",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the feature that does not support optional arguments?",
        "Y": "Conv Dropout Embedding",
        "Z": "Conv Dropout Embedding (no optional arguments supported) EmbeddingBag FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode is not supported in EmbeddingBag FeatureDropout?",
        "Y": "training mode",
        "Z": "Conv Dropout Embedding (no optional arguments supported) EmbeddingBag FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of Embedding does not support optional arguments?",
        "Y": "Dropout Embedding",
        "Z": "Dropout Embedding (no optional arguments supported) EmbeddingBag FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "EmbeddingBag FeatureDropout (not supported) EmbeddingBag FeatureDropout (not supported",
        "Y": "training mode",
        "Z": "Embedding (no optional arguments supported) EmbeddingBag FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode is not supported by FeatureDropout?",
        "Y": "training mode",
        "Z": "FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What expand expand_as eye flatten floor floor_divide?",
        "Y": "eq erf exp",
        "Z": "Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the absolute abs of MaxPool1d MaxPool2d MaxPool3d RNN?",
        "Y": "acos",
        "Z": "MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does frobenius_norm stand for?",
        "Y": "frobenius_norm",
        "Z": "MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the absolute abs of MaxPool2d MaxPool3d RNN?",
        "Y": "acos",
        "Z": "MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is frobenius_norm full?",
        "Y": "frobenius_norm full",
        "Z": "MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg",
        "Y": "MaxPool3d RNN",
        "Z": "MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is frobenius_norm full full_like?",
        "Y": "frobenius_norm full full_like",
        "Z": "MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the abs absolute?",
        "Y": "acos",
        "Z": "RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is frobenius_norm full full?",
        "Y": "frobenius_norm full full",
        "Z": "absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the abs absolute acos?",
        "Y": "abs absolute acos",
        "Z": "abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are the names of the two addmm and arange argmax argmin?",
        "Y": "addmm and arange argmax argmin",
        "Z": "absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool",
        "Y": "acos",
        "Z": "acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_",
        "Y": "acos",
        "Z": "acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do you need to add to avg_pool1d?",
        "Y": "addmm and arange argmax argmin",
        "Z": "adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do argmax argmin asin atan support?",
        "Y": "addmm and arange",
        "Z": "adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do you need to add to avg_pool3d?",
        "Y": "addmm and arange argmax argmin",
        "Z": "adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What addmm and arange asin atan avg_pool1d avg_pool2d avg",
        "Y": "addmm and arange",
        "Z": "adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the addmm and arange argmax argmin asin atan avg_pool1d",
        "Y": "addmm and arange argmax argmin",
        "Z": "adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do argmax argmin asin asin atan?",
        "Y": "addmm and arange",
        "Z": "adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of alpha not supported?",
        "Y": "nonzero",
        "Z": "add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for adaptive_max_pool3d add?",
        "Y": "arange argmax argmin",
        "Z": "adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does nonzero alpha not support?",
        "Y": "add",
        "Z": "add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for argmax argmin?",
        "Y": "arange",
        "Z": "addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos",
        "Y": "avg_pool3d",
        "Z": "arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is argmax argmin?",
        "Y": "arange",
        "Z": "and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is avg_pool3d?",
        "Y": "avg_pool3d",
        "Z": "avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack std sub (nonzero alpha not supported)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for avg_pool1d?",
        "Y": "avg_pool2d",
        "Z": "avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack std",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is argmin asin atan?",
        "Y": "argmax",
        "Z": "argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is atan avg_pool1d avg_pool2d avg_pool3d?",
        "Y": "argmin",
        "Z": "argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is atan avg_pool1d avg_pool2d avg_pool3d as_s",
        "Y": "argmin",
        "Z": "argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the bitshift cat ceil?",
        "Y": "baddbmm",
        "Z": "baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the avg_pool2d?",
        "Y": "avg_pool2d",
        "Z": "avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is avg_pool3d as_strided?",
        "Y": "baddbmm",
        "Z": "avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the avg_pool3d as_strided baddbmm bitshift cat?",
        "Y": "avg_pool2d",
        "Z": "avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is as_strided baddbmm bitshift cat ceil?",
        "Y": "avg_pool3d",
        "Z": "avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack std sub (nonzero alpha not supported)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the bitshift cat?",
        "Y": "as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min",
        "Z": "as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is baddbmm bitshift cat ceil?",
        "Y": "as_strided",
        "Z": "as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does eq erf exp expand expand expand_as eye flatten floor floor_divide?",
        "Y": "div dropout",
        "Z": "cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu",
        "Y": "eq erf exp expand",
        "Z": "celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is elu empty empty like?",
        "Y": "eq",
        "Z": "celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the benefit of adding export support for operators?",
        "Y": "advance usage",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is required to add export support for operators?",
        "Y": "developers need to touch the source code of PyTorch",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do developers install PyTorch?",
        "Y": "follow the instructions for installing PyTorch from source",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the wanted operator is what, it should be easy to add support for exporting such operator?",
        "Y": "standardized in ONNX",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can be done to confirm if the operator is standardized or not?",
        "Y": "check the ONNX operator list",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an advance usage of PyTorch?",
        "Y": "export support",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To add export support, developers need to touch the source code of what?",
        "Y": "PyTorch",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the source code of PyTorch?",
        "Y": "PyTorch",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can be added to the operator if the operator is standardized in ONNX?",
        "Y": "symbolic function",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To confirm whether the operator is standardized or not, please check what?",
        "Y": "ONNX operator list",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do developers need to do to achieve this?",
        "Y": "to touch the source code of PyTorch",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What should be done to confirm if the operator is standardized or not?",
        "Y": "check the ONNX operator list",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To achieve this, developers need to touch the source code of what?",
        "Y": "PyTorch",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To confirm whether the operator is standardized, please check what?",
        "Y": "ONNX operator list",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you find in torch/csrc/autograd/generated/VariableType.h?",
        "Y": "the declaration of the function",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the operator is an operator of what type, what operator can you find the declaration of the function in torch/csrc/autograd/",
        "Y": "ATen",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does torch/onnx/symbolic_opsetversion>.py define?",
        "Y": "symbolic function",
        "Z": "Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Make sure the function has what name?",
        "Y": "the same name as the ATen operator/function defined in VariableType.h",
        "Z": "Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The first parameter is always the exported what?",
        "Y": "ONNX graph",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Why must parameter names match the names in VariableType.h?",
        "Y": "dispatch is done with keyword arguments",
        "Z": "Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the ATen operator/function defined in?",
        "Y": "VariableType.h",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is dispatch done with?",
        "Y": "keyword arguments",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is always the first parameter?",
        "Y": "the exported ONNX graph",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Why must parameter names EXACTLY match the names in VariableType.h?",
        "Y": "dispatch is done with keyword arguments",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Inputs are always first, what are non-tensor arguments?",
        "Y": "tensors",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is always the exported ONNX graph?",
        "Y": "first parameter",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Inputs are always first, then what else?",
        "Y": "tensors",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in ONNX, we only need to do what to represent the ONNX operator",
        "Y": "create a node",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does NOT match what is in VariableType.h?",
        "Y": "Parameter ordering",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Parameter ordering does NOT match what?",
        "Y": "VariableType.h",
        "Z": "Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in what, we only need to create a node to represent the ONNX",
        "Y": "ONNX",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the input argument if ONNX asks for a scalar?",
        "Y": "a tensor",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can _if_scalar_type_as turn into a PyTorch tensor?",
        "Y": "Python scalar",
        "Z": "If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the input argument is a tensor, but ONNX asks for what?",
        "Y": "scalar",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What helper function can convert a scalar tensor into a python scalar?",
        "Y": "_scalar",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who asks for a scalar?",
        "Y": "ONNX",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What language does the helper function _if_scalar_type_as use?",
        "Y": "Python",
        "Z": "If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What happens if the operator is a non-ATen operator?",
        "Y": "the symbolic function has to be added in the corresponding PyTorch Function class",
        "Z": "If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you add a symbolic function if the operator is a non-ATen operator?",
        "Y": "Create a symbolic function named symbolic in the corresponding Function class",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the operator is a non-ATen operator, what has to be added in the corresponding PyTorch Function class?",
        "Y": "the symbolic function",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the symbolic function in the corresponding PyTorch Function class?",
        "Y": "Create a symbolic function named symbolic",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where must the symbolic function be added if the operator is a non-ATen operator?",
        "Y": "PyTorch Function class",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must EXACTLY match the names in forward?",
        "Y": "Parameter names",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must match the outputs of forward?",
        "Y": "output tuple size",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you add a symbolic function to a PyTorch Function class?",
        "Y": "Create a symbolic function named symbolic",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Parameter names except the first must EXACTLY match what?",
        "Y": "the names in forward",
        "Z": "Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of function is created in the corresponding Function class?",
        "Y": "symbolic",
        "Z": "Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the first parameter of a symbolic function?",
        "Y": "The first parameter is always the exported ONNX graph",
        "Z": "Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in ONNX, we just need to do what to represent the ONNX operator",
        "Y": "create a node",
        "Z": "The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the first parameter?",
        "Y": "always the exported ONNX graph",
        "Z": "The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What parameter is always the exported ONNX graph?",
        "Y": "first",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Symbolic functions should be implemented in what language?",
        "Y": "Python",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are Python methods implemented via?",
        "Y": "C++-Python bindings",
        "Z": "Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in what, we just need to create a node to represent the ONNX",
        "Y": "ONNX",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Symbolic functions interact with what?",
        "Y": "Python methods",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Python methods are implemented via what bindings?",
        "Y": "C++-Python",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of missing symbolic function for?",
        "Y": "elu operator",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do we handle missing symbolic function for elu operator?",
        "Y": "export the model and see the error message as below",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What bindings are used to implement Python methods?",
        "Y": "C++-Python",
        "Z": "Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of handling for elu operator?",
        "Y": "missing symbolic function",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we try to do?",
        "Y": "export the model",
        "Z": "The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where is the ONNX graph C++ definition located?",
        "Y": "torch/csrc/jit/ir/ir.h",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What did we try to do?",
        "Y": "export the model",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is handled for elu operator?",
        "Y": "missing symbolic function",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we try to do to the model?",
        "Y": "export",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does not support exporting elu operator?",
        "Y": "PyTorch",
        "Z": "Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where is virtual Tensor elu found?",
        "Y": "VariableType.h",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of operator is elu?",
        "Y": "ATen operator",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator list does PyTorch check?",
        "Y": "ONNX",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where do we add the following lines to?",
        "Y": "symbolic_opset9.py",
        "Z": "The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In VariableType.h, what operator does PyTorch not support exporting?",
        "Y": "virtual Tensor elu",
        "Z": "The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator does elu have?",
        "Y": "ATen",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does PyTorch add the following lines to?",
        "Y": "symbolic_opset9.py",
        "Z": "Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Why did the export fail?",
        "Y": "PyTorch does not support exporting elu operator",
        "Z": "The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where does PyTorch find virtual Tensor elu?",
        "Y": "VariableType.h",
        "Z": "The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where are examples of elu operators found?",
        "Y": "symbolic_opset9.py, symbolic_opset10.py",
        "Z": "The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who does not support exporting elu operator?",
        "Y": "PyTorch",
        "Z": "The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "PyTorch is able to export what operator?",
        "Y": "elu operator",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are examples of PyTorch's elu operator?",
        "Y": "symbolic_opset9.py, symbolic_opset10.py",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where are more examples of ops in PyTorch?",
        "Y": "symbolic_opset9.py, symbolic_opset10.py",
        "Z": "There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the interface for specifying operator definitions?",
        "Y": "experimental",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What tutorial will help you create and register your own custom ops implementation in PyTorch?",
        "Y": "Extending TorchScript with Custom C++ Operators",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can you export a custom ops implementation?",
        "Y": "ONNX",
        "Z": "The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the answer to the question of how to export a custom ops implementation to ONNX?",
        "Y": ":",
        "Z": "The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the tutorial that allows you to create and register your own custom ops implementation in PyTorch?",
        "Y": "Extending TorchScript with Custom C++ Operators",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What platform can you export your custom ops implementation to?",
        "Y": "ONNX",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you export a custom ops implementation to ONNX?",
        "Y": ":",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is able to export elu operator?",
        "Y": "PyTorch",
        "Z": "Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where are more examples of elu operator in PyTorch?",
        "Y": "symbolic_opset9.py, symbolic_opset10.py",
        "Z": "Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another example of PyTorch's export elu operator?",
        "Y": "symbolic_opset9.py",
        "Z": "Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can operators with unsupported ONNX operators be exported?",
        "Y": "export API",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX?",
        "Y": "operator_export_type flag",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the operator_export_type flag useful when users try to export?",
        "Y": "ATen and non-ATen operators that are not registered and supported in ONNX",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of operators are exported using the operator_export_type flag?",
        "Y": "regular ONNX operators",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default mode for exporting ATen and non-ATen operators?",
        "Y": "operator_export_type mode",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default operator_export_type mode used to export all operators as?",
        "Y": "ATen ops",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where is the operator_export_type flag found?",
        "Y": "export API",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When is the operator_export_type flag useful?",
        "Y": "when users try to export ATen and non-ATen operators that are not registered and supported in ONNX",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the operator_export_type mode used for?",
        "Y": "to export all operators as regular ONNX operators",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode is used to export all operators as ATen ops?",
        "Y": "operator_export_type mode",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default operator_export_type mode used for?",
        "Y": "to export all operators as ATen ops",
        "Z": "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API.\nThis flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default operator_export_type mode?",
        "Y": "export all operators as regular ONNX operators",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode is used to export all operators as regular ONNX operators?",
        "Y": "operator_export_type mode",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To fall back on what in ONNX?",
        "Y": "unsupported ATen operators",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are exported to ONNX regularly?",
        "Y": "Supported operators",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator is not supported in ONNX?",
        "Y": "aten::triu",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who falls back on aten::triu?",
        "Y": "Exporter",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default operator_export_type mode used to export?",
        "Y": "raw ir",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is this mode used for?",
        "Y": "to export all operators as regular ONNX operators",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default operator_export_type mode used to do in ONNX?",
        "Y": "fallback on unsupported ATen operators",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operators are exported to ONNX regularly?",
        "Y": "Supported operators",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who falls back on ATen::triu?",
        "Y": "Exporter",
        "Z": "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To export a raw operator?",
        "Y": "ir",
        "Z": "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the mode used to export all operators as?",
        "Y": "ATen ops",
        "Z": "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does exporter fall back on in ONNX?",
        "Y": "unsupported ATen operators",
        "Z": "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of operator is exported to ONNX?",
        "Y": "raw ir",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is this mode used to do?",
        "Y": "export all operators as ATen ops",
        "Z": "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the mode used to avoid conversion to ONNX?",
        "Y": "fallback on unsupported ATen operators",
        "Z": "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the exporter fall back on in ONNX?",
        "Y": "unsupported ATen operators",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the exporter do to ONNX?",
        "Y": "fallback on unsupported ATen operators",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To export what?",
        "Y": "raw ir",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default mode for ONNX operators?",
        "Y": "operator_export_type mode",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "This mode is used to export all operators as regular what?",
        "Y": "ONNX operators",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are all operators exported as?",
        "Y": "ATen ops",
        "Z": "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the purpose of a raw ir?",
        "Y": "export",
        "Z": "To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the purpose of exporting a raw ir?",
        "Y": "export a raw ir",
        "Z": "To export a raw ir.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What model did I export but its input size seems to be fixed?",
        "Y": "lstm",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What records the example inputs shape in the graph?",
        "Y": "tracer",
        "Z": "The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What parameter does export api use in case the model should accept inputs of dynamic shape?",
        "Y": "dynamic_axes",
        "Z": "The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you export models with in it?",
        "Y": "loops",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a good way to export models with loops in it?",
        "Y": "Tracing vs Scripting",
        "Z": "The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of inputs can be exported?",
        "Y": "primitive type inputs",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What seems to be fixed in my lstm model?",
        "Y": "input size",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In case the model should accept inputs of dynamic shape, what parameter can you utilize in export api?",
        "Y": "dynamic_axes",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of inputs can be exported with Tracing vs Scripting?",
        "Y": "loops",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are two options for exporting models with loops in it?",
        "Y": "Tracing vs Scripting",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of inputs can you export models with?",
        "Y": "primitive type inputs",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of inputs can be exported with PyTorch?",
        "Y": "loops",
        "Z": "The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does exporter not support conversion of models with String inputs?",
        "Y": "primitive type inputs",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When will primitive type inputs be supported?",
        "Y": "PyTorch 1.9 release",
        "Z": "The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Exporter does not support conversion of models with what inputs?",
        "Y": "String inputs",
        "Z": "Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In case the model should accept inputs of dynamic shape, you can utilize what parameter in export api?",
        "Y": "dynamic_axes",
        "Z": "The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of inputs can be exported with tracer?",
        "Y": "loops",
        "Z": "The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of inputs are supported in PyTorch 1.9 release?",
        "Y": "primitive type inputs",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When will support for primitive type inputs be added?",
        "Y": "PyTorch 1.9",
        "Z": "Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does not support conversion of models with String inputs?",
        "Y": "exporter",
        "Z": "Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does ONNX export models with in it?",
        "Y": "loops",
        "Z": "Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does ONNX not support implicit scalar datatype casting?",
        "Y": "Tracing vs Scripting",
        "Z": "Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does ONNX not support conversion of models with String inputs?",
        "Y": "primitive type inputs",
        "Z": "Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When will primitive type inputs be added to ONNX?",
        "Y": "PyTorch 1.9",
        "Z": "Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does ONNX support?",
        "Y": "implicit scalar datatype casting",
        "Z": "Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you export models with loops in it?",
        "Y": "export models with loops in it",
        "Z": "Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does PyTorch 1.9 support?",
        "Y": "primitive type inputs",
        "Z": "Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Exporter does not support what?",
        "Y": "conversion of models with String inputs",
        "Z": "Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What should you check out?",
        "Y": "Tracing vs Scripting",
        "Z": "Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the two types of scripting?",
        "Y": "Tracing vs Scripting",
        "Z": "Please checkout Tracing vs Scripting. Q: How to export models with primitive type inputs? Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Exporter does not support conversion of models with what input?",
        "Y": "String inputs",
        "Z": "Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of inputs will PyTorch 1.9 support?",
        "Y": "primitive type inputs",
        "Z": "Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What company supports implicit scalar datatype casting?",
        "Y": "ONNX",
        "Z": "Q: Does ONNX support implicit scalar datatype casting?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are converted to constant tensors in ONNX?",
        "Y": "Scalars",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The exporter will try to figure out the right datatype for what?",
        "Y": "scalars",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do you need to do if the exporter fails to figure out the right datatype for scalars?",
        "Y": "manually provide the datatype information",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of models often fail to record datatypes?",
        "Y": "scripted models",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is not required in the future?",
        "Y": "manual changes",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who will try to handle that part?",
        "Y": "the exporter",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are Scalars converted to in ONNX?",
        "Y": "constant tensors",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who will try to figure out the right datatype for scalars?",
        "Y": "The exporter",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of a model where the datatypes are not recorded?",
        "Y": "scripted models",
        "Z": "Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the goal of ONNX?",
        "Y": "improve the datatype propagation in the exporter",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is supported for ONNX opset version >= 11?",
        "Y": "Indexing",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What version of ONNX is tensor in-place indexed assignment supported for?",
        "Y": ">= 11",
        "Z": "Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is tensor in-place indexed assignment like data[index] = new_data supported for?",
        "Y": "Indexing",
        "Z": "Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Is tensor list exportable to ONNX?",
        "Y": "exportable to ONNX",
        "Z": "Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is tensor in-place indexed assignment like?",
        "Y": "data[index] = new_data",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is tensor in-place indexed assignment supported for?",
        "Y": "ONNX opset",
        "Z": "Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is tensor list exportable to?",
        "Y": "ONNX",
        "Z": "Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What version of ONNX is tensor list supported for?",
        "Y": ">= 11",
        "Z": "Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is tensor list exportable to ONNX?",
        "Y": "Indexing",
        "Z": "Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is exportable to ONNX?",
        "Y": "tensor list",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What version of ONNX supports tensor list export?",
        "Y": "ONNX opset",
        "Z": "Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What allows export of models in ONNX external data format?",
        "Y": "export API",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the exporter do with the ONNX external data format option enabled?",
        "Y": "the exporter stores some model parameters in external binary files",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where are the external binary files stored?",
        "Y": "the same location as the ONNX file",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must be a string specifying the location of the model?",
        "Y": "Argument \u2018f\u2019",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The use_external_data_format argument in export API enables export of models in what format?",
        "Y": "ONNX external data format",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the exporter store some model parameters in?",
        "Y": "external binary files",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where are external binary files stored?",
        "Y": "same location as the ONNX file",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must argument f be?",
        "Y": "a string specifying the location of the model",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can large models be exported to?",
        "Y": "ONNX",
        "Z": "This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Why can't models larger than 2GB be exported in one file?",
        "Y": "the protobuf size limit",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What should users set use_external_data_format to?",
        "Y": "True",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Why cannot models larger than 2GB be exported in one file?",
        "Y": "protobuf size limit",
        "Z": "This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To export models larger than 2GB, users should set use_external_data_format to what?",
        "Y": "True",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What allows users to export models in a training-friendly mode?",
        "Y": "export API",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "TrainingMode.TRAINING exports model in what mode?",
        "Y": "training-friendly",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If model.training is what, it exports the model in inference mode?",
        "Y": "False",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode does TrainingMode.PRESERVE export the model in if model.training is False?",
        "Y": "training-friendly mode",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default mode for this argument?",
        "Y": "TrainingMode.EVAL",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What argument in export API allows users to export models in a training-friendly mode?",
        "Y": "Training",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What exports model in a training-friendly mode that avoids certain model optimizations?",
        "Y": "TrainingMode.TRAINING",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "TrainingMode.PRESERVE exports the model in inference mode if model.training is what?",
        "Y": "False",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does TrainingMode.PRESERVE do if model.training is False?",
        "Y": "it exports the model in a training-friendly mode",
        "Z": "Training argument in export API allows users to export models in a training-friendly mode.\nTrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model\noptimizations which might interfere with model parameter training. TrainingMode.PRESERVE\nexports the model in inference mode if model.training is False. Otherwise, it exports\nthe model in a training-friendly mode.\nThe default mode for this argument is TrainingMode.EVAL which exports the model in\ninference mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What format can a model be exported into?",
        "Y": "ONNX",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of models does torch.nn.Module support?",
        "Y": "a limited set of dynamic models",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the model to be exported?",
        "Y": "model",
        "Z": "model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a tuple of arguments or torch.Tensor?",
        "Y": "args",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What format can you export a model into?",
        "Y": "ONNX",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does ONNX support at the moment?",
        "Y": "a limited set of dynamic models",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for args?",
        "Y": "torch.Tensor",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a dictionary to specify the input to the corresponding named parameter?",
        "Y": "args",
        "Z": "model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a args?",
        "Y": "a dictionary consisting of named arguments",
        "Z": "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a tuple of arguments or torch?",
        "Y": "a dictionary consisting of named arguments",
        "Z": "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is used to specify the input to the corresponding named parameter?",
        "Y": "a dictionary",
        "Z": "a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the dictionary to specify the input to the corresponding named parameter?",
        "Y": "KEY: str",
        "Z": "a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is model(*args)?",
        "Y": "a valid invocation of the model",
        "Z": "ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Any non-Tensor arguments will be what in the exported model?",
        "Y": "hard-coded",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If args is a Tensor, this is equivalent to having called it with what of that Tensor?",
        "Y": "1-ary tuple",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does a TUPLE OF ARGUEMENTS have?",
        "Y": "DICTIONARY OF NAMED PARAMETERS",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a valid invocation of the model?",
        "Y": "model(*args)",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Any non-what type of arguments will be hard-coded into the exported model?",
        "Y": "Tensor",
        "Z": "model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If args is a Tensor, this is equivalent to having called it with what tuple of that Tensor?",
        "Y": "1-ary",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "A TUPLE OF ARGUEMENTS WITH WHAT?",
        "Y": "DICTIONARY OF NAMED PARAMETERS",
        "Z": "ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will be hard-coded into the exported model?",
        "Y": "Any non-Tensor arguments",
        "Z": "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Any non-what argument will be hard-coded into the exported model?",
        "Y": "Tensor",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the last value of a tuple?",
        "Y": "a dictionary consisting of named parameters and the corresponding inputs as key-value pairs",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What value is assigned if a certain named argument is not present in the dictionary?",
        "Y": "None",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is assigned if certain named argument is not present in the dictionary?",
        "Y": "default value",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is assigned if the default value is not provided?",
        "Y": "None",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the last value of the tuple?",
        "Y": "a dictionary",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If certain named argument is not present in the dictionary, what value is assigned to it?",
        "Y": "default value",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If a named argument is not present in the dictionary, what is assigned if the default value is not provided?",
        "Y": "None",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would cause a conflict when a dictionary of named parameters is used?",
        "Y": "args tuple",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What provides an example of a conflict when a dictionary of named parameters is used?",
        "Y": "model below",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the model below return?",
        "Y": "x m",
        "Z": "Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would the call to export API look like in the previous iteration?",
        "Y": "torch.onnx.export",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the last input of the args tuple?",
        "Y": "dictionary input",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the previous iteration, the call to export API would look like what?",
        "Y": "torch.onnx.export",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does torch.randn(2, 3) call?",
        "Y": "torch.tensor(1",
        "Z": "\u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the value of Model()?",
        "Y": "m",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is Model() k = torch.randn(2, 3) x = torch.tensor(1)",
        "Y": "m",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "m = Model() k = torch.randn(2, 3)?",
        "Y": "x",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The export function assumes that the x input is intended to represent what?",
        "Y": "the optional dictionary consisting of named arguments",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In order to prevent this from being an issue, a constraint is placed to provide what type of dictionary as the last input in the tup",
        "Y": "empty",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would the new call to export API look like?",
        "Y": "The new call would look like this",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the previous iteration, what would the call to export API look like?",
        "Y": "torch.onnx.export",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the export function assume is intended to represent the optional dictionary consisting of named arguments?",
        "Y": "the x input",
        "Z": "Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does torch.onnx.export(model, (k, x), and \u2018test.onnx\u2019 mean?",
        "Y": "test.onnx",
        "Z": "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would assume that the x input is intended to represent the optional dictionary?",
        "Y": "the export function",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of dictionary is provided as the last input in the tuple args?",
        "Y": "empty",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does torch.onnx.export(model, (k, x, ), call?",
        "Y": "test.onnx",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would work as intended?",
        "Y": "torch.onnx.export",
        "Z": "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) stand for?",
        "Y": "test.onnx",
        "Z": "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How would the export function work?",
        "Y": "as intended",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is torch.onnx.export f?",
        "Y": "file-like object",
        "Z": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will be written to the file?",
        "Y": "binary Protobuf",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the parameter that will be exported if specified?",
        "Y": "export_params",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If you want to export an untrained model, set export_params to what?",
        "Y": "False",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What specifies the ordering of the exported model's parameters?",
        "Y": "model.state_dict().values()",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does torch.onnx.export(model, (k, x, ), \u2018test.onnx\u2019)",
        "Y": "a file-like object",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will be written to this file?",
        "Y": "A binary Protobuf",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Set export_params to what if you want to export an untrained model?",
        "Y": "False",
        "Z": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does model.state_dict().values() do?",
        "Y": "model.state_dict().values()",
        "Z": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does a file-like object have to implement that returns a file descriptor?",
        "Y": "fileno",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a file-like object that has to implement fileno that returns a file descriptor?",
        "Y": "f",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a bool, default True value?",
        "Y": "export_params",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the ordering of the exported model's parameters specified by?",
        "Y": "model.state_dict().values()",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What happens if you want to export an untrained model?",
        "Y": "the exported model will first take all of its parameters as arguments",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will be exported if specified?",
        "Y": "export_params",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What verbose specifies the ordering of the exported model's parameters?",
        "Y": "model.state_dict().values()",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default value of verbose?",
        "Y": "bool, default False",
        "Z": "verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default TrainingMode.EVAL?",
        "Y": "training",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If training is False and to a training friendly mode if model, what is it?",
        "Y": "True",
        "Z": "verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "TrainingMode.TRAINING: export the model in what mode?",
        "Y": "training friendly mode",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When will we print out a debug description of the trace being exported?",
        "Y": "if specified",
        "Z": "verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "TrainingMode.PRESERVE: export the model in inference mode if model.training is what?",
        "Y": "True",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Is model.training true or false?",
        "Y": "False",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name to assign to the input nodes of the graph?",
        "Y": "input_names",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "TrainingMode.EVAL: export the model in what mode?",
        "Y": "inference mode",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If model.training is what, export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode",
        "Y": "False",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Names to assign to the input nodes of the graph?",
        "Y": "input_names",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Names to assign to the output nodes of the graph?",
        "Y": "output_names",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is used to export the model in aten mode?",
        "Y": "operator_export_type",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If using aten mode, all the ops original exported by the functions in symbolic_opsetversion>.py are exported as what",
        "Y": "ATen ops",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are the names to assign to the output nodes of the graph?",
        "Y": "output_names",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode does operator_export_type export the model in?",
        "Y": "aten mode",
        "Z": "aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name to assign to the output nodes of the graph?",
        "Y": "output_names",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the function that exports the internal IR directly instead of converting it to ONNX ops?",
        "Y": "export_raw_ir",
        "Z": "aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does operator_export_type convert the internal IR to?",
        "Y": "ONNX ops",
        "Z": "input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the function that exports the internal IR directly instead of ONNX ops?",
        "Y": "export_raw_ir",
        "Z": "aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does export_raw_ir convert the internal IR to instead of converting it to?",
        "Y": "ONNX ops",
        "Z": "export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the mode that exports the model in aten mode?",
        "Y": "aten",
        "Z": "aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default OperatorExportTypes.ONNX?",
        "Y": "operator_export_type",
        "Z": "input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default False value for exporting the model in aten mode?",
        "Y": "aten",
        "Z": "aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What ops does export_raw_ir convert the internal IR to?",
        "Y": "ONNX",
        "Z": "input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default operator_export_type?",
        "Y": "OperatorExportTypes.ONNX",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does bool, default False mean?",
        "Y": "export_raw_ir",
        "Z": "export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does export the internal IR instead of converting it to?",
        "Y": "ONNX ops",
        "Z": "verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default False value for the internal IR?",
        "Y": "export_raw_ir",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "OperatorExportTypes.ONNX_ATEN: All ops are exported as what?",
        "Y": "ATen ops",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What happens if an ATen op is not supported in ONNX or its symbolic is missing?",
        "Y": "fall back on ATen op",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How is an example graph exported?",
        "Y": "as:",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace).",
        "Y": "ONNX",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is not supported in ONNX?",
        "Y": "aten::triu",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is exported as: ONNX?",
        "Y": "Example graph",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "OperatorExportTypes.ONNX: All ops are exported as what?",
        "Y": "regular ONNX ops",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of ops are exported to ONNX regularly?",
        "Y": "Registered ops",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "OperatorExportTypes. What is the namespace for ONNX ops?",
        "Y": "ONNX",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is exported as: ONNX ops?",
        "Y": "graph",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the op that can be exported as a custom ONNX op?",
        "Y": "OperatorExportTypes.RAW",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the custom ONNX op?",
        "Y": "OperatorExportTypes.ONNX_FALLTHROUGH",
        "Z": "is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who can implement an ONNX op?",
        "Y": "the user",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the op that can be exported and implemented by the user for their runtime backend?",
        "Y": "OperatorExportTypes.ONNX_FALLTHROUGH",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is not supported in the above example?",
        "Y": "prim::ListConstruct",
        "Z": "In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default to export the model to the opset version of the onnx submodule?",
        "Y": "opset_version",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How many stable opset versions does ONNX export to?",
        "Y": "one",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the current supported stable opset version?",
        "Y": "9.",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must the opset_version be?",
        "Y": "_onnx_main_opset",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the submodule we export the model to?",
        "Y": "opset_version",
        "Z": "is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "By default, how many stable opset versions do we export to?",
        "Y": "one stable opset version",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must be _onnx_main_opset or _onnx_stable_opsets?",
        "Y": "opset_version",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the opset version of the onnx submodule?",
        "Y": "opset_version",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must be _onnx_main_opset or in _onnx_stable_opsets?",
        "Y": "opset_version",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the constant-folding optimization that is applied to the model during export?",
        "Y": "do_constant_folding",
        "Z": "do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will the constant-folding optimization replace some of the ops that have all constant inputs with?",
        "Y": "pre-computed constant nodes",
        "Z": "do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What happens if do_constant_folding is True?",
        "Y": "the constant-folding optimization is applied to the model during export",
        "Z": "do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will replace some of the ops that have all constant inputs with pre-computed constant nodes?",
        "Y": "Constant-folding optimization",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does example_outputs represent?",
        "Y": "Model\u2019s example outputs being exported",
        "Z": "example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are examples of outputs required when exporting?",
        "Y": "ScriptModule or TorchScript Function",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If there is more than one item, it should be passed in what format?",
        "Y": "tuple format",
        "Z": "example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What should be passed in tuple format if there is more than one item?",
        "Y": "example_outputs",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What should be passed as the example output if there is more than one item?",
        "Y": "only one item",
        "Z": "do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the value of ample_outputs?",
        "Y": "x",
        "Z": "example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Model's what is being exported?",
        "Y": "example outputs",
        "Z": "example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must be provided when exporting a ScriptModule or TorchScript Function?",
        "Y": "\u2018example_outputs\u2019",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If there is more than one item, what should be passed in tuple format?",
        "Y": "example_outputs",
        "Z": "do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How many items should be passed as the example output?",
        "Y": "only one item should be passed as the example output",
        "Z": "example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the item that should be passed as the example output?",
        "Y": "example_outputs",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What strip the field \"doc_string\" from the exported model?",
        "Y": "strip_doc_string",
        "Z": "example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does dictstring, dictpython:int, string> or dictstring, list(int)",
        "Y": "dynamic_axes",
        "Z": "strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When does strip_doc_string (bool, default True) strip the field \u201cdoc_string\u201d from the exported model?",
        "Y": "if True",
        "Z": "strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does dictstring, dictpython:int, string>> or dictstring, list(int",
        "Y": "dynamic_axes",
        "Z": "strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the term for dictstring, dictpython:int, string or dictstring, list(in",
        "Y": "dynamic_axes",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default value of dynamic_axes?",
        "Y": "empty",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are dictstring, dictpython:int, string>> or dictstring, list(int",
        "Y": "dynamic_axes",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE",
        "Y": "dynamic axes",
        "Z": "Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs, what is",
        "Y": "False",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can dynamic axes be defined as?",
        "Y": "keep_initializers_as_inputs",
        "Z": "Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If True, initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs?",
        "Y": "If False",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When are all the initializers in the exported graph added as inputs to the graph?",
        "Y": "If True",
        "Z": "ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What happens if keep_initializers_as_input is true?",
        "Y": "all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph",
        "Z": "INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When are all initializers in the exported graph added as inputs to the graph?",
        "Y": "If True",
        "Z": "INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the MIXED MODE of (1) and (2)?",
        "Y": "keep_initializers_as_inputs",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When are initializers not added as inputs to the graph?",
        "Y": "If False",
        "Z": "ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the lower triangular part of a matrix defined as?",
        "Y": "the elements on and below the diagonal",
        "Z": "Returns the indices of the lower triangular part of a row-by-\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The lower triangular part of the matrix is defined as the elements on and\nbelow the diagonal.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices"
    },
    {
        "X": "Returns what of the lower triangular part of a row-by-col matrix in a 2-by-N Tensor?",
        "Y": "the indices",
        "Z": "Returns the indices of the lower triangular part of a row-by-\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The lower triangular part of the matrix is defined as the elements on and\nbelow the diagonal.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices"
    },
    {
        "X": "What is the lower triangular part of the matrix defined as?",
        "Y": "the elements on and below the diagonal",
        "Z": "The lower triangular part of the matrix is defined as the elements on and\nbelow the diagonal.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices"
    },
    {
        "X": "What scale is used to subtract other from input?",
        "Y": "alpha",
        "Z": "Subtracts other, scaled by alpha, from input. Supports broadcasting to a common shape,\ntype promotion, and integer, float, and complex inputs. input (Tensor) \u2013 the input tensor. other (Tensor or Scalar) \u2013 the tensor or scalar to subtract from input alpha (Scalar) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub"
    },
    {
        "X": "Supports what?",
        "Y": "broadcasting",
        "Z": "Subtracts other, scaled by alpha, from input. Supports broadcasting to a common shape,\ntype promotion, and integer, float, and complex inputs. input (Tensor) \u2013 the input tensor. other (Tensor or Scalar) \u2013 the tensor or scalar to subtract from input alpha (Scalar) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub"
    },
    {
        "X": "What is an example of a scalar multiplier?",
        "Y": "Example",
        "Z": "Subtracts other, scaled by alpha, from input. Supports broadcasting to a common shape,\ntype promotion, and integer, float, and complex inputs. input (Tensor) \u2013 the input tensor. other (Tensor or Scalar) \u2013 the tensor or scalar to subtract from input alpha (Scalar) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub"
    },
    {
        "X": "For what type of tensor does it compute the logical AND?",
        "Y": "bool tensors",
        "Z": "Computes the bitwise AND of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical AND. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and"
    },
    {
        "X": "What is the output tensor of a bool tensor?",
        "Y": "output tensor",
        "Z": "Computes the bitwise AND of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical AND. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and"
    },
    {
        "X": "What will flow back from the result of this operation to input?",
        "Y": "gradients",
        "Z": "Returns a copy of input. Note This function is differentiable, so gradients will flow back from the\nresult of this operation to input. To create a tensor without an\nautograd relationship to input see detach(). input (Tensor) \u2013 the input tensor. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone"
    },
    {
        "X": "Detach() creates a tensor without what relationship to input?",
        "Y": "autograd relationship",
        "Z": "Returns a copy of input. Note This function is differentiable, so gradients will flow back from the\nresult of this operation to input. To create a tensor without an\nautograd relationship to input see detach(). input (Tensor) \u2013 the input tensor. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone"
    },
    {
        "X": "What is the default memory format of the returned tensor?",
        "Y": "torch.preserve_format",
        "Z": "Returns a copy of input. Note This function is differentiable, so gradients will flow back from the\nresult of this operation to input. To create a tensor without an\nautograd relationship to input see detach(). input (Tensor) \u2013 the input tensor. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone"
    },
    {
        "X": "To create a tensor without an autograd relationship to input see what?",
        "Y": "detach()",
        "Z": "Returns a copy of input. Note This function is differentiable, so gradients will flow back from the\nresult of this operation to input. To create a tensor without an\nautograd relationship to input see detach(). input (Tensor) \u2013 the input tensor. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone"
    },
    {
        "X": "What does memory_format return?",
        "Y": "the desired memory format of returned tensor",
        "Z": "Returns a copy of input. Note This function is differentiable, so gradients will flow back from the\nresult of this operation to input. To create a tensor without an\nautograd relationship to input see detach(). input (Tensor) \u2013 the input tensor. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone"
    },
    {
        "X": "What is obj a PyTorch tensor?",
        "Y": "PyTorch storage object",
        "Z": "Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If obj is a PyTorch tensor, Returns True if obj is what?",
        "Y": "PyTorch storage object",
        "Z": "Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If obj is a PyTorch tensor, what is it?",
        "Y": "PyTorch storage object",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the data type of input?",
        "Y": "floating point data type",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If the data type of input is a torch.float64, torch.float32, torch.float16, and torch.bfloat16,",
        "Y": "floating point data type",
        "Z": "Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the GPU that enables you to run your tensor computations?",
        "Y": "CUDA",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of data type does obj return True if it is a PyTorch tensor?",
        "Y": "complex data type",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If obj is a PyTorch storage object, what is it?",
        "Y": "PyTorch tensor",
        "Z": "Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the data type of input if it is a complex data type?",
        "Y": "one of torch.complex64, and torch.complex128",
        "Z": "Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns True if the input is a single element tensor which is not equal to what after type conversions?",
        "Y": "zero",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Sets the efault floating point dtype to what?",
        "Y": "d",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of input is not equal to zero after type conversions?",
        "Y": "a single element tensor",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of object is obj?",
        "Y": "PyTorch storage object",
        "Z": "Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Sets the default floating point torch.dtype do?",
        "Y": "Get the current default floating point torch.dtype",
        "Z": "Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns True if the data type of input is what?",
        "Y": "a floating point data type",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the efault floating point dtype?",
        "Y": "d",
        "Z": "Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned if the input is a single element tensor?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns True what if the data type of input is a floating point data type?",
        "Y": "if the data type of input is a floating point data type",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Returns True if the input is a single element tensor?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned by setting the default torch.Tensor type to floating point tensor type t?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what if the input is a single element tensor?",
        "Y": "True",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the torch.Tensor type return?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does this do on the CPU?",
        "Y": "Disables denormal floating numbers",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the Disable denormal floating numbers on CPU do?",
        "Y": "Note",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what if the input is a single element tensor which is not equal to zero after type conversions?",
        "Y": "True",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned by setting the default torch.Tensor type to t?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the default torch.Tensor type?",
        "Y": "floating point tensor type t",
        "Z": "Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what in the input tensor?",
        "Y": "the total number of elements",
        "Z": "Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What are listed under Random sampling and include: torch.rand() torch.rand_like() torch.randn() torch.rand",
        "Y": "Random sampling creation ops",
        "Z": "Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.empty() construct with data?",
        "Y": "a tensor",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does denormal floating numbers do on the CPU?",
        "Y": "Disables denormal floating numbers",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of data does torch.empty() construct?",
        "Y": "tensor",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor is constructed with data?",
        "Y": "tensor",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor is constructed in COO(rdinate) format with specified values at the given indices?",
        "Y": "sparse",
        "Z": "Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.empty() construct a tensor with?",
        "Y": "data",
        "Z": "Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of data does a torch.empty() construct?",
        "Y": "tensor",
        "Z": "Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is created by creating a Tensor input with specified size, stride and storage_offset?",
        "Y": "a view of an existing torch",
        "Z": "Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Creates a Tensor from what?",
        "Y": "numpy.ndarray",
        "Z": "Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor filled with the scalar value 0, with the same size as input?",
        "Y": "a tensor filled with the scalar value 0, with the same size as input",
        "Z": "Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor filled with the scalar value 1?",
        "Y": "a tensor filled with the scalar value 1, with the shape defined by the variable argument size",
        "Z": "Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the shape of the scalar value?",
        "Y": "the shape defined by the variable argument size",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the scalar value of the tensor filled with?",
        "Y": "the same size as input",
        "Z": "Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the scalar value 1?",
        "Y": "the same size as input",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when the scalar value is 0. Returns a tensor filled with the scalar value 0, with the",
        "Y": "a tensor",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor filled with the scalar value 0, with what size as input?",
        "Y": "same size",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor filled with the scalar value 1, with what size as input?",
        "Y": "same size",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the return value of a tensor filled with the scalar value?",
        "Y": "a tensor filled with the scalar value 0, with the same size as input",
        "Z": "Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What defines the shape of the scalar value?",
        "Y": "variable argument size",
        "Z": "Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what type of scalar value filled with the scalar value 1?",
        "Y": "a tensor",
        "Z": "Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what tensor of size?",
        "Y": "a 1-D tensor",
        "Z": "Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does return a tensor of size endstartstepleftlceil fractextend",
        "Y": "a 1-D tensor",
        "Z": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How many tensors does endstartstep+1leftlfloor fractextend return?",
        "Y": "1",
        "Z": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How large is the tensor of size?",
        "Y": "1-D",
        "Z": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a 2-D tensor with what values?",
        "Y": "ones on the diagonal and zeros elsewhere",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of size size filled with fill_value.   Returns a tensor with the same size as input filled with fill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used to split a tensor into multiple tensors depthwise?",
        "Y": "indices_or_sections",
        "Z": "Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What specifies the axis of a tensor?",
        "Y": "dim",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens to the given sequence of seq tensors in the given dimension?",
        "Y": "Concatenates",
        "Z": "Concatenates the given sequence of seq tensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens when a tensor is divided into chunks?",
        "Y": "Splits",
        "Z": "Concatenates the given sequence of seq tensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used to split a tensor into multiple tensors horizontally?",
        "Y": "indices_or_sections",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Splits a tensor into multiple tensors depthwise according to?",
        "Y": "indices_or_sections",
        "Z": "Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens to a tensor with three or more dimensions?",
        "Y": "Splits input",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used to split input into multiple tensors horizontally?",
        "Y": "indices_or_sections",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How are tensors stacked in sequence?",
        "Y": "horizontally",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Splits input into multiple tensors depthwise according to?",
        "Y": "indices_or_sections",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What axis are the values of a tensor gathered along?",
        "Y": "an axis",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Splits input into multiple tensors horizontally according to?",
        "Y": "indices_or_sections",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens to a tensor with one or more dimensions?",
        "Y": "Splits input",
        "Z": "Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the index that indexes the input tensor along dimension dim?",
        "Y": "LongTensor",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What axis is specified by dim?",
        "Y": "axis",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the tensor that indexes the input tensor along dimension dim?",
        "Y": "LongTensor",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Stack tensors in sequence in what direction?",
        "Y": "depthwise",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Stack tensors horizontally according to what?",
        "Y": "indices",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do to split tensors in sequence horizontally?",
        "Y": "Stack tensors",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is a new tensor that indexes the input tensor along dimension dim?",
        "Y": "LongTensor",
        "Z": "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What dimension does a BoolTensor index?",
        "Y": "1-D",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is specified by dim?",
        "Y": "axis",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do when a tensor is split into multiple tensors horizontally?",
        "Y": "Stack tensors",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do to split input into multiple tensors horizontally?",
        "Y": "Stack tensors",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Stack tensors in sequence how?",
        "Y": "horizontally",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor indexes the input tensor?",
        "Y": "1-D",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias for?",
        "Y": "torch.linalg.matrix_power",
        "Z": "Alias for torch.linalg.matrix_power()",
        "source": "https://pytorch.org/docs/stable/generated/torch.matrix_power.html#torch.matrix_power"
    },
    {
        "X": "What kind of version of the input tensor is the new tensor?",
        "Y": "narrowed",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is what?",
        "Y": "LongTensor",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which",
        "Y": "BoolTensor",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does move the dimension(s) of input at the position(s) in source to the position(s) in destination?",
        "Y": "Moves the dimension(s) of input at the position(s) in source to the position(s) in destination",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used to move the dimension(s) of input at the position(s) in source to the position(s) in destination?",
        "Y": "Alias",
        "Z": "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new tensor that indexes the input tensor according to what dimension?",
        "Y": "1-D",
        "Z": "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What kind of version of the input tensor does torch.movedim() return?",
        "Y": "narrowed",
        "Z": "Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor does torch.movedim() return?",
        "Y": "1-D",
        "Z": "Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.movedim() return?",
        "Y": "Alias",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the tensor return with the same data and number of elements as input but with?",
        "Y": "the specified shape",
        "Z": "Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the Alias for torch.vstack()?",
        "Y": "Alias of torch.vstack()",
        "Z": "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the version of torch.Tensor.scatter_()?",
        "Y": "Out-of-place",
        "Z": "Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the alias for torch.movedim()?",
        "Y": "torch.vstack()",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the out-of-place version of torch.vstack()?",
        "Y": "torch.Tensor.scatter_()",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.movedim() return for a new tensor that is a narrowed version of input tens",
        "Y": "Alias",
        "Z": "Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the Alias for torch.movedim()?",
        "Y": "torch.vstack()",
        "Z": "Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch split the tensor into?",
        "Y": "chunks",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what with the same data and number of elements as input, but with the specified shape?",
        "Y": "a tensor",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Out-of-place version of torch.Tensor.scatter_add_() Splits the tensor into what?",
        "Y": "chunks",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "A tensor with all the dimensions of input of what size is removed?",
        "Y": "size 1",
        "Z": "Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the Alias that returns a new tensor that is a narrowed version of input tens",
        "Y": "torch.movedim()",
        "Z": "Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor with all the dimensions of input of size what?",
        "Y": "1",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.transpose() do?",
        "Y": "Selects values from input at the 1-dimensional indices from indices along the given dim",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used for torch.transpose()?",
        "Y": "Alias",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What version of torch.Tensor.scatter_() splits the tensor into chunks?",
        "Y": "Out-of-place",
        "Z": "Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the tensor return with?",
        "Y": "the specified shape",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the size of the input tensor?",
        "Y": "size 1",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is another name for torch.transpose()?",
        "Y": "Alias for torch.transpose()",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned with the same data and number of elements as input, but with the specified shape?",
        "Y": "a tensor",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Sets the seed for generating random numbers to what type of random number?",
        "Y": "non-deterministic",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the torch.ByteTensor set?",
        "Y": "the random number generator state",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the torch.ByteTensor do?",
        "Y": "Draws binary random numbers (0 or 1) from a Bernoulli distribution",
        "Z": "Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does each row of a tensor contain?",
        "Y": "num_samples indices",
        "Z": "Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does ByteTensor do?",
        "Y": "Sets the random number generator state",
        "Z": "Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What indices does each row of a tensor contain?",
        "Y": "num_samples",
        "Z": "Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do to a Bernoulli distribution?",
        "Y": "Draws binary random numbers",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of distribution is the tensor sampled from?",
        "Y": "Poisson",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor where each row contains what indices sampled from the multinomial probability distribution located in the corresponding",
        "Y": "num_samples",
        "Z": "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor of random numbers drawn from separate normal distributions whose what are given?",
        "Y": "mean and standard deviation",
        "Z": "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "The tensor of the same size as input has each element sampled from what distribution?",
        "Y": "Poisson",
        "Z": "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "The tensor of the same size as input is sampled from what distribution with rate parameter given by the corresponding element in input?",
        "Y": "Poisson",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input?",
        "Y": "a tensor of the same size as input",
        "Z": "Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What kind of distribution is the tensor filled with random numbers from?",
        "Y": "uniform",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Random integers are generated uniformly between what two levels?",
        "Y": "low (inclusive) and high (exclusive)",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when a tensor is filled with random numbers from a uniform distribution on the interval?",
        "Y": "a tensor",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (what?",
        "Y": "exclusive",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Random integers are generated uniformly between what two intervals?",
        "Y": "low (inclusive) and high (exclusive)",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is filled with random numbers from a uniform distribution on the interval?",
        "Y": "a tensor",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive)?",
        "Y": "exclusive",
        "Z": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor with the same shape as input filled with random integers generated uniformly between low (inclusive) and high (",
        "Y": "exclusive",
        "Z": "Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when a tensor is filled with random integers from a uniform distribution on the interval?",
        "Y": "a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive)",
        "Z": "Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned with the same size as input that is filled with random numbers from a uniform distribution on the interval?",
        "Y": "a tensor",
        "Z": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).\ntorch.rand_like(input) is equivalent to\ntorch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"
    },
    {
        "X": "What is filled with random integers generated uniformly between low (inclusive) and high (exclusive)?",
        "Y": "tensor",
        "Z": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive)",
        "Y": "exclusive",
        "Z": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance",
        "Y": "a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.",
        "Z": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the range of the random permutation of integers in a tensor?",
        "Y": "0 to n",
        "Z": "Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a random permutation of integers from 0 to n - 1?",
        "Y": "a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the range of integers in a tensor?",
        "Y": "0 to n",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is another name for a tensor filled with random numbers from a normal distribution with mean 0 and variance 1?",
        "Y": "standard normal distribution",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the size of a tensor that is filled with random numbers from a normal distribution with mean 0 and variance 1?",
        "Y": "same size",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a random permutation of integers from what range?",
        "Y": "0 to n - 1",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Where are the random numbers sampled from?",
        "Y": "the discrete uniform distribution",
        "Z": "torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the distribution torch?",
        "Y": "exponential",
        "Z": "torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "To what does torch.save() save an object?",
        "Y": "a disk file",
        "Z": "Saves an object to a disk file.   Loads an object saved with torch.save() from a file.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "To what type of file does torch.save() save an object?",
        "Y": "disk",
        "Z": "Saves an object to a disk file.   Loads an object saved with torch.save() from a file.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Loads an object saved with what function from a file?",
        "Y": "torch.save()",
        "Z": "Saves an object to a disk file.   Loads an object saved with torch.save() from a file.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What context managers are helpful for locally disabling and enabling gradient computation?",
        "Y": "torch.no_grad(), torch.enable_grad(), and torch.set_grad_enabled()",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is a more detailed description of the context managers torch.no_grad(), torch.enable_grad(), and torch.set",
        "Y": "Locally disabling gradient computation",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of context managers are torch.no_grad(), torch.enable_grad(), and torch.set_grad_enable",
        "Y": "thread local",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Which context manager disables gradient calculation?",
        "Y": "Context-manager",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What context manager enables gradient calculation?",
        "Y": "Context-manager",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is another name for the context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_",
        "Y": "Locally disabling gradient computation",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What context manager disables gradient calculation?",
        "Y": "Context-manager",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Alias for torch.abs() compute of each element in input?",
        "Y": "absolute value",
        "Z": "Computes the absolute value of each element in input.   Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the tensor returned by Alias for torch.acos()?",
        "Y": "inverse hyperbolic cosine",
        "Z": "Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the element-wise division of tensor1 by?",
        "Y": "tensor2",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the value of each element in input?",
        "Y": "absolute value",
        "Z": "Computes the absolute value of each element in input.   Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Which function returns a new tensor with the inverse hyperbolic cosine of the elements of input?",
        "Y": "Alias for torch.acos()",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the inverse cosine of the elements of input?",
        "Y": "inverse hyperbolic cosine",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Alias for torch.acosh do?",
        "Y": "Adds the scalar other to each element of the input input",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what of each element in input?",
        "Y": "inverse cosine",
        "Z": "Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the Alias for torch.acosh() do?",
        "Y": "Adds the scalar other to each element of the input input",
        "Z": "Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of cosine is the inverse of the elements of input?",
        "Y": "hyperbolic",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs the element-wise division of what by tensor2?",
        "Y": "tensor1",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs the element-wise multiplication of what?",
        "Y": "tensor1 by tensor2",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What value does the element-wise multiplication of tensor1 by tensor2 multiply the result by?",
        "Y": "scalar",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.acosh do?",
        "Y": "Adds the scalar other to each element of the input input",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the inverse hyperbolic cosine of the elements of input?",
        "Y": "Alias",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.acosh() do?",
        "Y": "Adds the scalar other to each element of the input input",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs the element-wise division of what?",
        "Y": "tensor1 by tensor2, multiply the result by the scalar value and add it to input",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "A new tensor is returned with what of the elements of input?",
        "Y": "arcsine",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the element-wise division of tensor1 by tensor2 multiply the result by?",
        "Y": "scalar value",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the result of the element-wise multiplication of tensor1 by tensor2 multiplied by?",
        "Y": "scalar value",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do to return a new resulting tensor?",
        "Y": "Adds the scalar other to each element of the input input",
        "Z": "Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What element of the elements of input returns a new tensor?",
        "Y": "cosine",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.   Returns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.   Alias for torch.trunc()   Raises input to the power of exponent, elementwise, in double precision.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the element-wise division of tensor1 by tensor2?",
        "Y": "multiplication",
        "Z": "Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the element-wise multiplication of tensor1 by tensor2 multiply the result by?",
        "Y": "scalar value",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the element-wise angle of the given input tensor in what units?",
        "Y": "radians",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name for torch.asin()?",
        "Y": "Alias",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs the element-wise multiplication of what by tensor2?",
        "Y": "tensor1",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What element of the elements of input does Alias for torch.asinh() return a new tensor with?",
        "Y": "arctangent",
        "Z": "Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What value does the multiplication of tensor1 by tensor2 multiply the result by?",
        "Y": "scalar",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Alias for torch.asin() return of the elements of input?",
        "Y": "inverse hyperbolic sine",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What element of input does Alias for torch.asinh() return a new tensor with?",
        "Y": "arctangent",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the arctangent of the elements of input?",
        "Y": "inverse hyperbolic tangent",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Alias for torch.asinh(). Returns a new tensor with what of the elements of input?",
        "Y": "arctangent",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What tangent does Alias for torch.atan return?",
        "Y": "inverse hyperbolic tangent",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the tensor with the arctangent of the elements of input?",
        "Y": "inverse hyperbolic tangent",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What element-wise of inputi/otheritextinput_i / textother_i",
        "Y": "arctangent",
        "Z": "Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Alias for returns a new tensor with the inverse hyperbolic tangent of the elements of input?",
        "Y": "torch.atan()",
        "Z": "Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "The element-wise arctangent of inputi/otheritextinput_i / textother",
        "Y": "the quadrant",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the element-wise tangent of inputi/otheritextinput_i / text",
        "Y": "arctangent",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the Alias that returns a new tensor with the inverse hyperbolic sine of the elements of input",
        "Y": "torch.atanh()",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do to the given input tensor?",
        "Y": "Computes the bitwise NOT",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the Alias for torch.atan() compute?",
        "Y": "bitwise OR of input and other",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Tests if all elements in input evaluate to what?",
        "Y": "True",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what value of the input tensor in the given dimension(s) dim?",
        "Y": "the maximum value of each slice",
        "Z": "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the test return?",
        "Y": "the maximum value of all elements in the input tensor",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns the maximum value of each slice of the input tensor in the given dimension(s) dim?",
        "Y": "the indices of the minimum value(s) of the flattened tensor or along a dimension",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Where is the maximum value of each slice of the input tensor?",
        "Y": "the given dimension(s) dim",
        "Z": "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does return the maximum value of each slice of the input tensor in the given dimension(s) dim?",
        "Y": "the minimum value of each slice of the input tensor in the given dimension(s) dim",
        "Z": "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when the input tensor evaluates to True?",
        "Y": "the maximum value of all elements in the input tensor",
        "Z": "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the p-norm of (input - other) return?",
        "Y": "the minimum value of all elements in the input tensor",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the value of (input - other)?",
        "Y": "p-norm",
        "Z": "the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what value of each slice of the input tensor in the given dimension(s) dim?",
        "Y": "minimum value",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the maximum value of all elements in the input tensor. Returns the minimum value of all elements in what tensor?",
        "Y": "input tensor",
        "Z": "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the what of (input - other)?",
        "Y": "p-norm",
        "Z": "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the value of the log of summed exponentials of each row of the input tensor in the given dimension dim?",
        "Y": "p-norm",
        "Z": "Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what of (input - other)?",
        "Y": "the p-norm",
        "Z": "Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the value of the values in input?",
        "Y": "median",
        "Z": "Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what value of the values in input?",
        "Y": "the median",
        "Z": "Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the value of all elements in the input tensor?",
        "Y": "mean value",
        "Z": "Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the median of the values in input, ignoring what values?",
        "Y": "NaN",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the median of the values in input, ignoring what?",
        "Y": "NaN values",
        "Z": "Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does each row of the input tensor in the given dimension dim return?",
        "Y": "the log of summed exponentials",
        "Z": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what of the values in input?",
        "Y": "median",
        "Z": "Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the sum of all elements return?",
        "Y": "the product of all elements in the input tensor",
        "Z": "Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when the sum of all elements in the input tensor is returned?",
        "Y": "the product of all elements in the input tensor",
        "Z": "Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.gt() do?",
        "Y": "Computes",
        "Z": "Computes input>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new tensor with boolean elements representing if each element is what?",
        "Y": "finite",
        "Z": "Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.   Computes input\u2264other\\text{input} \\leq \\text{other}input\u2264other element-wise.   Alias for torch.le().   Computes input<other\\text{input} < \\text{other}input<other element-wise.   Alias for torch.lt().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Tests if each element of input is what?",
        "Y": "negative infinity",
        "Z": "Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.   Computes input\u2264other\\text{input} \\leq \\text{other}input\u2264other element-wise.   Alias for torch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Tests if each element of input is what or not?",
        "Y": "positive infinity",
        "Z": "Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.   Computes input\u2264other\\text{input} \\leq \\text{other}input\u2264other element-wise.   Alias for torch.le().   Computes input<other\\text{input} < \\text{other}input<other element-wise.   Alias for torch.lt().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens if each element of input is negative infinity or not?",
        "Y": "Tests",
        "Z": "Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.gt() use?",
        "Y": "Alias",
        "Z": "Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new tensor with what representation if each element of input is close to the corresponding element of another?",
        "Y": "boolean elements",
        "Z": "Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What element of input is returned if the new tensor is finite or not?",
        "Y": "NaN",
        "Z": "Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of infinity does each element of input have?",
        "Y": "negative",
        "Z": "Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.le() do?",
        "Y": "Computes",
        "Z": "Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.   Computes input\u2264other\\text{input} \\leq \\text{other}input\u2264other element-wise.   Alias for torch.le().   Computes input<other\\text{input} < \\text{other}input<other element-wise.   Alias for torch.lt().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new tensor with what representation if each element of input is NaN or not?",
        "Y": "boolean elements",
        "Z": "Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.   Computes input\u2264other\\text{input} \\leq \\text{other}input\u2264other element-wise.   Alias for torch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the window function that computes the Kaiser window?",
        "Y": "Hann",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window length window_length and shape parameter beta.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What two parameters does the Kaiser window have?",
        "Y": "window_length and shape parameter beta",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window length window_length and shape parameter beta.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what with window length and shape parameter beta?",
        "Y": "the Kaiser window",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window length window_length and shape parameter beta.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How large is the view of each input tensor with zero dimensions?",
        "Y": "2-dimensional",
        "Z": "Returns a 1-dimensional view of each input tensor with zero dimensions.   Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What do you do with provided tensors?",
        "Y": "Create a block diagonal matrix",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does broadcast the given tensors according to?",
        "Y": "Broadcasting semantics",
        "Z": "Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does broadcasts input to?",
        "Y": "shape shape",
        "Z": "Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is broadcast_tensors() used for?",
        "Y": "shapes",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the function that returns the frequency of each value in an array of non-negative ints?",
        "Y": "Count the frequency of each value in an array of non-negative ints",
        "Z": "Returns a 1-dimensional view of each input tensor with zero dimensions.   Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Broadcasts input to what shape?",
        "Y": "shape",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the value counted in an array of non-negative ints?",
        "Y": "frequency",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the function that returns a 3-dimensional view of each input tensor with zero dimensions?",
        "Y": "Count the frequency of each value in an array of non-negative ints",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What of the buckets are set by boundaries?",
        "Y": "boundaries",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what of each input tensor with zero dimensions?",
        "Y": "a 3-dimensional view",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does broadcast_tensors() use instead of broadcast_tensors()?",
        "Y": "shapes",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does broadcast_tensors() return?",
        "Y": "a copy of input",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Count the value in an array of non-negative ints?",
        "Y": "frequency",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of input is broadcast_tensors() used for?",
        "Y": "shapes",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Do what product of the given sequence of tensors?",
        "Y": "cartesian",
        "Z": "Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What do provided tensors do?",
        "Y": "Create a block diagonal matrix",
        "Z": "Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the sequence of tensors?",
        "Y": "Do cartesian product",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does batched the p-norm distance between each pair of the two collections of row vectors do?",
        "Y": "Computes",
        "Z": "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What input to the shape shape is similar to broadcast_tensors() but for shapes?",
        "Y": "Broadcasts",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Compute combinations of what?",
        "Y": "length rrr of the given tensor",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does broadcast_tensors() do?",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of object is broadcast_tensors() used for?",
        "Y": "shapes",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Compute combinations of what of the given tensor?",
        "Y": "length rrr",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the cross product of vectors in what ension dim of input?",
        "Y": "dim",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is similar to broadcast_tensors() but for?",
        "Y": "shapes",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what of the buckets to which each value in the input belongs?",
        "Y": "indices",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the given tensor?",
        "Y": "Compute combinations of length rrr",
        "Z": "Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what of input and other?",
        "Y": "cross product of vectors in dimension dim",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What of the given sequence of tensors?",
        "Y": "Do cartesian product",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when calculating the cross product of vectors in dimension dim of input and other?",
        "Y": "the cross product of vectors in dimension dim of input and other",
        "Z": "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the tensor that returns a copy of input?",
        "Y": "Compute combinations of length rrr",
        "Z": "Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the given sequence of tensors do?",
        "Y": "Do cartesian product",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the cumulative maximum of elements of input in the dimension dim?",
        "Y": "a namedtuple",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes batched what between each pair of the two collections of row vectors?",
        "Y": "p-norm distance",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Compute of the given tensor do?",
        "Y": "Compute combinations of length rrr",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the cross product of vectors in what dimension of input and other?",
        "Y": "dimension dim",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns values where values is the cumulative minimum of elements of input in the dimension dim?",
        "Y": "a namedtuple",
        "Z": "Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes batched the distance between each pair of the two collections of row vectors?",
        "Y": "p-norm",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what combinations of length rrr of the given tensor?",
        "Y": "combinations of length rrr of the given tensor",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the cumulative minimum of elements of input in the dimension dim?",
        "Y": "a namedtuple",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the function that returns the cross product of vectors in dimension dim of input and other?",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the product of vectors in dimension dim of input and other?",
        "Y": "cross product",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the namedtuple return?",
        "Y": "cumulative sum of elements of input in the dimension dim",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what type of input?",
        "Y": "copy",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the function that returns a copy of input?",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when calculating combinations of length rrr of the given tensor?",
        "Y": "the cross product of vectors in dimension dim of input and other",
        "Z": "Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what product of input and other?",
        "Y": "the cross product of vectors in dimension dim",
        "Z": "Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns the cumulative maximum of elements of input in the dimension dim?",
        "Y": "a namedtuple",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what of vectors in dimension dim of input and other?",
        "Y": "cross product",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a namedtuple where values is the cumulative minimum of elements of input in the dimension dim.",
        "Y": "values",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Returns a namedtuple return?",
        "Y": "the cumulative product of elements of input in the dimension dim",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what in the dimension dim?",
        "Y": "the cumulative sum of elements of input",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What reduces the number of steps required to perform a batch matrix-matrix product of matrices stored in batch1 and batch2",
        "Y": "add step",
        "Z": "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs a matrix multiplication of which matrices?",
        "Y": "mat1 and mat2",
        "Z": "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What product of the matrix mat and the vector vec is performed?",
        "Y": "matrix-vector product",
        "Z": "Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the matrix-vector product of the matrix mat and the vector vec perform?",
        "Y": "outer-product of vectors vec1 and vec2",
        "Z": "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens when a reduced add step is performed?",
        "Y": "all matrix multiplications get accumulated along the first dimension",
        "Z": "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What product of vectors vec1 and vec2 is added to the matrix input?",
        "Y": "outer-product",
        "Z": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the batch matrix-matrix product consist of?",
        "Y": "matrices stored in input and mat2",
        "Z": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs what of the matrices mat1 and mat2?",
        "Y": "matrix multiplication",
        "Z": "Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What product is added to the matrix input?",
        "Y": "outer-product of vectors vec1 and vec2",
        "Z": "Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What performs a batch matrix-matrix product of matrices in batch1 and batch2?",
        "Y": "batch matrix-matrix product of matrices",
        "Z": "Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs a matrix-vector product of what?",
        "Y": "matrix mat and the vector vec",
        "Z": "Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What product of matrices in batch1 and batch2 performs?",
        "Y": "batch matrix-matrix",
        "Z": "Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Who computes the decomposition of a symmetric positive-definite matrix AAA?",
        "Y": "Cholesky",
        "Z": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns the matrix product of the NNN 2-D tensors?",
        "Y": "matrix product of the NNN 2-D tensors",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices?",
        "Y": "Cholesky",
        "Z": "Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs the outer-product of which vectors and adds it to the matrix input?",
        "Y": "vec1 and vec2",
        "Z": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What product of matrices in batch1 and batch2 is performed?",
        "Y": "batch matrix-matrix",
        "Z": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs what of vectors vec1 and vec2 and adds it to the matrix input?",
        "Y": "outer-product",
        "Z": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "In what two batches is the batch matrix-matrix product of matrices performed?",
        "Y": "batch1 and batch2",
        "Z": "Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What decomposition of a symmetric positive-definite matrix AAA is computed?",
        "Y": "Cholesky",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used to compute the inverse of a symmetric positive-definite matrix AAA?",
        "Y": "LAPACK routines",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its\nCholesky factor uuu: returns matrix inv. The inverse is computed using\nLAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uuu is lower triangular\nsuch that the returned tensor is If upper is True or not provided, uuu is upper\ntriangular such that the returned tensor is input (Tensor) \u2013 the input 2-D tensor uuu, a upper or lower triangular\nCholesky factor upper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix out (Tensor, optional) \u2013 the output tensor for inv Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"
    },
    {
        "X": "Computes what decomposition of a symmetric positive-definite matrix AAA?",
        "Y": "Cholesky",
        "Z": "Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the Cholesky factor uuu do?",
        "Y": "returns matrix inv",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the Cholesky factor matrix uuu compute?",
        "Y": "dot product of two 1D tensors",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Solves what with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu?",
        "Y": "a linear system of equations",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the Cholesky factor uuuuuuuuuuuuuuuuuuuuuuu",
        "Y": "the dot product of two 1D tensors",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does a linear system of equations with a positive semidefinite matrix need to be inverted given?",
        "Y": "Cholesky factor matrix uuu",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the Cholesky factor uuu compute?",
        "Y": "dot product of two 1D tensors",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of matrix does it compute the eigenvalues and eigenvectors of?",
        "Y": "real square matrix",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias of?",
        "Y": "torch.outer",
        "Z": "Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.outer compute the dot product for?",
        "Y": "1D tensors",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the low-level function that computes the dot product for 1D tensors?",
        "Y": "Alias of torch.outer()",
        "Z": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.outer() compute the dot product for?",
        "Y": "1D tensors",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does linalg calculate of a square matrix or batches of square matrices?",
        "Y": "log determinant",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.linalg.det() calculate?",
        "Y": "log determinant",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the solution to what problems for a full rank matrix AAA of size (mn)(m times n)",
        "Y": "least squares and least norm",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what of a matrix or batches of matrices A?",
        "Y": "LU factorization",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Where are PyTorch casting rules described?",
        "Y": "type promotion documentation",
        "Z": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1   Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.   Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.   Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.   Returns True if the global warn_always flag is turned on.   A wrapper around Python\u2019s assert which is symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What kind of type does the torch.dtype return?",
        "Y": "scalar",
        "Z": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1   Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.   Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.   Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Sets whether PyTorch operations must use what kind of algorithms?",
        "Y": "deterministic",
        "Z": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1   Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.   Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.   Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the type that would result from performing an arithmetic operation on the provided input tensors?",
        "Y": "torch.dtype",
        "Z": "Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.   Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.   Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What kind of type returns the torch.dtype with the smallest size?",
        "Y": "scalar",
        "Z": "Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.   Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.   Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Sets whether PyTorch operations must use what algorithms?",
        "Y": "deterministic",
        "Z": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1   Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.   Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.   Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.   Returns True if the global warn_always flag is turned on.   A wrapper around Python\u2019s assert which is symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does PyTorch return with the smallest size and scalar kind?",
        "Y": "the torch.dtype",
        "Z": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1   Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.   Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.   Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.   Returns True if the global warn_always flag is turned on.   A wrapper around Python\u2019s assert which is symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If the global deterministic flag is turned on, what is returned?",
        "Y": "True",
        "Z": "Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.   Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.dtype return?",
        "Y": "smallest size and scalar kind",
        "Z": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.   Returns True if the global warn_always flag is turned on.   A wrapper around Python\u2019s assert which is symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the torch.dtype do?",
        "Y": "Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms",
        "Z": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.   Returns True if the global warn_always flag is turned on.   A wrapper around Python\u2019s assert which is symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "A wrapper around what program's assert that is symbolically traceable?",
        "Y": "Python",
        "Z": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.   Returns True if the global warn_always flag is turned on.   A wrapper around Python\u2019s assert which is symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2?",
        "Y": "the torch.dtype",
        "Z": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.   Returns True if the global warn_always flag is turned on.   A wrapper around Python\u2019s assert which is symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is set by the torch.dtype with the smallest size and scalar kind?",
        "Y": "whether PyTorch operations must use \u201cdeterministic\u201d algorithms",
        "Z": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.   Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.   Returns True if the global deterministic flag is turned on.   When this flag is False (default) then some PyTorch warnings may only appear once per process.   Returns True if the global warn_always flag is turned on.   A wrapper around Python\u2019s assert which is symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the current status of FX?",
        "Y": "Beta release",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX is a toolkit for developers to use to transform what?",
        "Y": "nn.Module instances",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How many main components does FX consist of?",
        "Y": "three",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a demonstration of the components in action?",
        "Y": "demonstration",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What release is FX currently in?",
        "Y": "Beta",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX is a toolkit for developers to use to do what?",
        "Y": "transform nn.Module instances",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the three main components of FX?",
        "Y": "symbolic tracer, an intermediate representation, and Python code generation",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What performs symbolic execution of the Python code?",
        "Y": "symbolic tracer",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the symbolic tracer feed through the code?",
        "Y": "fake values",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the operations recorded on?",
        "Y": "Proxies",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can more information about symbolic tracing be found?",
        "Y": "symbolic_trace() and Tracer documentation",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a toolkit for developers to use to transform nn.Module instances?",
        "Y": "FX",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the symbolic tracer perform?",
        "Y": "symbolic execution",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is recorded on Proxies?",
        "Y": "Operations",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can you find more information about symbolic tracing?",
        "Y": "symbolic_trace() and Tracer documentation",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the symbolic tracer feed through the Python code?",
        "Y": "fake values",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is recorded on the Proxies?",
        "Y": "Operations",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the container for the operations that were recorded during symbolic tracing?",
        "Y": "intermediate representation",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the format on which transformations are applied?",
        "Y": "The IR",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The intermediate representation consists of a list of what?",
        "Y": "Nodes",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can you find more information about the IR?",
        "Y": "documentation for Graph",
        "Z": "The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What makes FX a Python-to-Python transformation toolkit?",
        "Y": "Python code generation",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "For each Graph IR, we can create valid Python code matching what?",
        "Y": "Graph\u2019s semantics",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is GraphModule?",
        "Y": "nn.Module",
        "Z": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is what makes FX a Python-to-Python transformation toolkit?",
        "Y": "Python code generation",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a torch.nn.Module instance that holds a Graph and a forward method generated from the Graph?",
        "Y": "GraphModule",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can you find examples of transformations?",
        "Y": "examples repository",
        "Z": "Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a function that looks like this?",
        "Y": "FX transform",
        "Z": "Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an FX transform?",
        "Y": "function that looks like this",
        "Z": "Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will your transform acquire from the torch.nn.Module?",
        "Y": "a Graph",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you pass your transform to?",
        "Y": "TorchScript",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Ensuring that the inputs and outputs of your FX transform are a torch.nn.Module will allow for what?",
        "Y": "composability",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should you do to ensure that the inputs and outputs of your FX transform are a torch?",
        "Y": "Note",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does your transform acquire from the torch.nn.Module?",
        "Y": "Graph",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you pass your FX transform to?",
        "Y": "TorchScript",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What must you call to bring the generated forward() method on the GraphModule in sync with the modified Graph?",
        "Y": "GraphModule.recompile()",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What has been traced into a Graph?",
        "Y": "torch.nn.Module",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method?",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How many primary approaches can you take to building a new Graph?",
        "Y": "two",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What.nn.Module has been traced into a Graph?",
        "Y": "torch",
        "Z": "It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are we going to cover here?",
        "Y": "the basics",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method?",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A Graph represents a method on what?",
        "Y": "GraphModule",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method?",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What information does a Graph need to represent a method on a GraphModule?",
        "Y": "What are the inputs to the method",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the inputs to a Graph?",
        "Y": "operations that run inside the method",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method?",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can you find full treatment of the semantics of graphs?",
        "Y": "the Graph documentation",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a Graph?",
        "Y": "a data structure that represents a method on a GraphModule",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {}",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What runs inside a GraphModule?",
        "Y": "operations",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method?",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can the full treatment of the semantics of graphs be found?",
        "Y": "the Graph documentation",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does a Graph represent a method on?",
        "Y": "GraphModule",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the information that a Graph requires?",
        "Y": "What are the inputs to the method",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the operations that run inside a GraphModule?",
        "Y": "operations",
        "Z": "It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value from a method?",
        "Y": "output",
        "Z": "It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the output value from a method?",
        "Y": "return",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are all three concepts represented with?",
        "Y": "Node instances",
        "Z": "All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a graph?",
        "Y": "a short example",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the operations that run inside the method?",
        "Y": "inputs",
        "Z": "What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the inputs to the method?",
        "Y": "inputs to the method",
        "Z": "opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value from the method?",
        "Y": "output",
        "Z": "What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the output value from the method?",
        "Y": "return",
        "Z": "What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method is used to print out a table showing the nodes of the module?",
        "Y": "Graph.print_tabular()",
        "Z": "What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the operations that run inside a method?",
        "Y": "operations",
        "Z": "What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method is used to print out a table showing the nodes of the module MyModule?",
        "Y": "Graph.print_tabular()",
        "Z": "Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1}",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the placeholder x x?",
        "Y": "args kwargs",
        "Z": "args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is target args kwargs placeholder x x?",
        "Y": "opcode name",
        "Z": "opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the placeholder for the target args kwargs?",
        "Y": "x x",
        "Z": "name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Name target args kwargs placeholder what?",
        "Y": "x x",
        "Z": "name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the target args kwargs placeholder?",
        "Y": "x x",
        "Z": "target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are kwargs placeholders?",
        "Y": "args",
        "Z": "args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the kwargs placeholder?",
        "Y": "x x",
        "Z": "kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the placeholder x x?",
        "Y": "kwargs",
        "Z": "kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is linear_1 linear?",
        "Y": "call_module",
        "Z": "(x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the placeholder?",
        "Y": "x x",
        "Z": "placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is get_attr?",
        "Y": "linear",
        "Z": "x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does get_attr linear_weight stand for?",
        "Y": "linear.weight",
        "Z": "{} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does linear.weight stand for?",
        "Y": "linear_weight",
        "Z": "linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of.weight (x, linear_weight)  call_module linear_1 linear (add_1)  call_",
        "Y": "linear",
        "Z": "linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is linear_1 linear (add_1)?",
        "Y": "call_module",
        "Z": "call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the built-in function add?",
        "Y": "add_1",
        "Z": "add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the call_function?",
        "Y": "topk_1",
        "Z": "(relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the add_1 function?",
        "Y": "call_module linear_1 linear",
        "Z": "<built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the call_module linear_1 linear?",
        "Y": "add_1",
        "Z": "<built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of _weight is x?",
        "Y": "linear",
        "Z": "(x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the add_1 module?",
        "Y": "call_module linear_1 linear",
        "Z": "{} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is another name for linear_1 linear?",
        "Y": "add_1",
        "Z": "linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is another name for add_1?",
        "Y": "call_module linear_1 linear",
        "Z": "call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is another name for call_module linear_1 linear?",
        "Y": "add_1",
        "Z": "call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the built-in method sum...> (relu_1)?",
        "Y": "call_function sum_1",
        "Z": "(add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the term for add_1?",
        "Y": "linear",
        "Z": "linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the inputs to a method?",
        "Y": "inputs",
        "Z": "placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How are method inputs specified in FX?",
        "Y": "special placeholder nodes",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In this case, we have a single placeholder node with a target of what?",
        "Y": "x",
        "Z": "relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a built-in method sum...> (relu_1)?",
        "Y": "call_function sum_1",
        "Z": "linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the call_method?",
        "Y": "call_method relu_1 relu",
        "Z": "call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function that is built-in method sum...> (relu_1)?",
        "Y": "call_function sum_1",
        "Z": "linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does relu stand for?",
        "Y": "call_function sum_1",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In FX, method inputs are specified via what?",
        "Y": "special placeholder nodes",
        "Z": "opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the built-in method?",
        "Y": "call_function topk_1",
        "Z": "{} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does 'dim' mean?",
        "Y": "call_function topk_1",
        "Z": "call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is built-in method sum...> (relu_1) \u2018dim\u2019: -1 call_function topk",
        "Y": "sum_1",
        "Z": "sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the \u2018dim\u2019 of a built-in method?",
        "Y": "-1",
        "Z": "<built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is one way to build a new Graph?",
        "Y": "symbolic tracing",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can we help build a new Graph?",
        "Y": "simply take the Graph we obtain from symbolic tracing and modify it",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to replace torch.add() calls with?",
        "Y": "torch.mul() calls",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What calls do we want to replace torch.add() calls with?",
        "Y": "torch.mul()",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a Graph rewrite?",
        "Y": "deleting or appending nodes",
        "Z": "opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Who has utility functions for transforming the graph that can be found in the Graph documentation?",
        "Y": "FX",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of using the APIs to append a call to a graph?",
        "Y": "torch.relu()",
        "Z": "We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "For simple transformations that only consist of substitutions, what can you make use of?",
        "Y": "subgraph rewriter",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are some more involved Graph rewrites?",
        "Y": "deleting or appending nodes",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does FX have for transforming the graph?",
        "Y": "utility functions",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What API can be used to append a call to a graph?",
        "Y": "torch.relu()",
        "Z": "We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you use for simple transformations that only consist of substitutions?",
        "Y": "subgraph rewriter",
        "Z": "For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Subgraph rewriter can be used for simple transformations that only consist of what?",
        "Y": "substitutions",
        "Z": "For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How many op Conv/Batch Norm fusions are there?",
        "Y": "one",
        "Z": "Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the basic usage of the Quantization Invert Transformation?",
        "Y": "Basic usage",
        "Z": "Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the usage of Quantization Invert Transformation?",
        "Y": "Basic",
        "Z": "Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How many op Conv/Batch Norm fusion replace_pattern?",
        "Y": "one",
        "Z": "Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will capture the operations that are performed on them and append them to the Graph?",
        "Y": "Proxy objects",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will the Proxy objects append the operations to?",
        "Y": "Graph",
        "Z": "To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the Proxy objects called?",
        "Y": "arugments",
        "Z": "To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Proxy objects will capture the operations performed on them and append them to what?",
        "Y": "the Graph",
        "Z": "To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Proxys allows you to specify your rewrite rules as native Python code?",
        "Y": "avoiding explicit graph manipulation",
        "Z": "In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are two examples of a large amount of rewrite rules?",
        "Y": "vmap or grad",
        "Z": "In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of using for Graph manipulation?",
        "Y": "Proxys",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Proxys allows you to specify your rewrite rules as native what?",
        "Y": "Python code",
        "Z": "In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can using Proxys do for transformations that require a large amount of rewrite rules?",
        "Y": "improve readability and maintainability of the rules",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a working example of using for Graph manipulation?",
        "Y": "Proxys",
        "Z": "In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a useful code organizational pattern in FX?",
        "Y": "loop over all the Nodes in a Graph and execute them",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can a loop over all the Nodes in a Graph be used for?",
        "Y": "runtime analysis of values flowing through the graph or transformation of the code via retracing with Proxys",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What would we run and record the torch.Tensor shape and dtype properties on the nodes as we see them at runtime?",
        "Y": "GraphModule",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What might a GraphModule record the torch shape and dtype properties on the nodes as we see them at runtime look like",
        "Y": "look like:",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a full interpreter for?",
        "Y": "FX",
        "Z": "As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can certain aspects of the interpreter's execution be overridden?",
        "Y": "method overrides",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the class that can be overridden via method overrides?",
        "Y": "Interpreter class",
        "Z": "As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is not that complicated but can be very useful?",
        "Y": "a full interpreter",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can certain aspects of the interpreter\u2019s execution be overridden?",
        "Y": "method overrides",
        "Z": "As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can we generate a new Graph?",
        "Y": "feeding Proxy values through an interpreter",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is provided to encompass the pattern of generating a new Graph by feeding Proxy values through an interpreter?",
        "Y": "Transformer class",
        "Z": "In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How does Transformer compare to Interpreter?",
        "Y": "Transformer behaves similarly to Interpreter",
        "Z": "In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the class that can be used to generate a new Graph?",
        "Y": "Shape Propagation",
        "Z": "In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we feed through an interpreter to generate a new Graph?",
        "Y": "Proxy values",
        "Z": "As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What class is provided to encompass this pattern?",
        "Y": "Transformer",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What class behaves similarly to Interpreter?",
        "Y": "Transformer",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does Transformer.transform() do?",
        "Y": "Shape Propagation",
        "Z": "In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the tool that is used to measure propagation?",
        "Y": "Shape Propagation Performance Profiler",
        "Z": "Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does Shape Propagation Performance Profiler stand for?",
        "Y": "Shape Propagation Performance Profiler",
        "Z": "Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a good way to make sure your code is correct?",
        "Y": "authoring transformations",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What might we need to do in the course of authoring transformations when our code is not quite right?",
        "Y": "some debugging",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the first step in debugging?",
        "Y": "inspect and debug the generated code",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the next step in debugging?",
        "Y": "debug the process of transformations that led to the generated code",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you're not familiar with debuggers, please see what section?",
        "Y": "auxiliary section Available Debuggers",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In the course of authoring transformations, what will often not be quite right?",
        "Y": "our code",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In this case, we may need to do what?",
        "Y": "debugging",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the key to debugging?",
        "Y": "work backwards",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the first step in working backwards?",
        "Y": "inspect and debug",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the process of transformations that led to the generated code?",
        "Y": "debug",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you\u2019re not familiar with debuggers, please see what auxiliary section?",
        "Y": "Available Debuggers",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the auxiliary section for debuggers?",
        "Y": "Available Debuggers",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers. Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the output of most deep learning modules consist of?",
        "Y": "floating point torch",
        "Z": "Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "To motivate this, let\u2019s use what?",
        "Y": "an example",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers. Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the first tool in our toolbox to check?",
        "Y": "if transformed modules are behaving as we expect compared to a reference implementation",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX generates the forward() function on what?",
        "Y": "GraphModules",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we use for debugging the generated code?",
        "Y": "several techniques",
        "Z": "Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the first tool in our toolbox to do?",
        "Y": "check if transformed modules are behaving as we expect",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What traditional debugging techniques are not as straightforward?",
        "Y": "print statements or pdb",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we do with the generated code?",
        "Y": "debugging",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where does FX generate the forward() function?",
        "Y": "GraphModules",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How many techniques can we use for debugging the generated code?",
        "Y": "several",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Invoke pdb to do what?",
        "Y": "step into the running program",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The code that represents the Graph is not in any source file, but we can step into it manually using what?",
        "Y": "pdb",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What generates the forward() function on GraphModules?",
        "Y": "FX",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Invoke what to step into the running program?",
        "Y": "pdb",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is used to step into the Graph when the forward pass is invoked?",
        "Y": "pdb",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you want to run the same code multiple times, it can be a bit tedious to step to the right code with pdb.",
        "Y": "multiple times",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the easiest way to step into the right code with pdb?",
        "Y": "copy-paste",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is used to step into the running program?",
        "Y": "pdb",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be a bit tedious to step to the right code with if you want to run the same code multiple times?",
        "Y": "pdb",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is one way to step into the right code with pdb?",
        "Y": "copy-paste",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is one way to examine the generated forward pass into your code?",
        "Y": "copy-paste",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you want to run the same code multiple times, it can be a bit tedious to do what with pdb?",
        "Y": "step to the right code",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is one way to step to the right code with pdb?",
        "Y": "copy-paste the generated forward pass into your code and examine it from there",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a method in GraphModule that allows you to dump out the generated FX code to a folder?",
        "Y": "GraphModule.to_folder()",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the method that allows you to examine modules and parameters?",
        "Y": "to_folder",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can we look at the code within the generated code?",
        "Y": "foo/module.py",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be used to debug the generated code?",
        "Y": "pdb",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "GraphModule.to_folder() can be used to examine modules and parameters using what method?",
        "Y": "to_folder",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can we modify the code to debug the generated code?",
        "Y": "foo/module.py",
        "Z": "After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we use to debug the generated code?",
        "Y": "pdb",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can we look at the code to debug the generated code?",
        "Y": "foo/module.py",
        "Z": "After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be added to the code to debug the generated code?",
        "Y": "print statements",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can we look at the code?",
        "Y": "foo/module.py",
        "Z": "After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of statements can be added to foo/module.py?",
        "Y": "print statements",
        "Z": "After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the first step in identifying a transformation that is creating incorrect code?",
        "Y": "debug",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What section of the documentation will we check first?",
        "Y": "Limitations of Symbolic Tracing",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What transformation is the goal of debugging?",
        "Y": "GraphModule",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can we find a quick answer to this question?",
        "Y": "Writing Transformations",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the next step when a transformation is creating incorrect code?",
        "Y": "debug",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Once we verify what, the goal becomes figuring out what went wrong during our GraphModule transformation?",
        "Y": "tracing is working as expected",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we compare before and after we've applied our transformations?",
        "Y": "traced Module",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is enough to trace down a bug?",
        "Y": "a simple visual comparison",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a good debugger for when it's not clear what's going wrong?",
        "Y": "pdb",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What direction does the code in the example above go?",
        "Y": "off",
        "Z": "After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we compare our traced Module before and after we\u2019ve applied our transformations?",
        "Y": "utility functions",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can a simple visual comparison trace down?",
        "Y": "a bug",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What debugger can be a good next step if it\u2019s not clear what\u2019s going wrong?",
        "Y": "pdb",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the code that can be used to trace down a bug?",
        "Y": "consider the following code",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What showed us that there was an error in our transforms?",
        "Y": "call to print",
        "Z": "Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we use to find what goes wrong?",
        "Y": "a debugger",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What session does the debugger start?",
        "Y": "pdb",
        "Z": "Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can we see what's happening during the transform?",
        "Y": "breaking on transform_graph(traced), then pressing s to \u201cstep into\u201d the call to transform_graph(traced).",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What code showed us that there was an error in our transforms?",
        "Y": "print(traced)",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the call to print(traced)?",
        "Y": "transform_graph(traced)",
        "Z": "Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What did the call to traced show us that there was an error in our transforms?",
        "Y": "print",
        "Z": "Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What session does a debugger start?",
        "Y": "pdb",
        "Z": "Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What did the call to print(traced) show in our transforms?",
        "Y": "an error",
        "Z": "Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we break on a pdb session to see what goes wrong?",
        "Y": "transform_graph(traced)",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method can be used to print different attributes of the Nodes in the Graph?",
        "Y": "print_tabular",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What might we want to see in the print_tabular method?",
        "Y": "input_nodes and users",
        "Z": "We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method can be edited to print different attributes of the Nodes in the Graph?",
        "Y": "print_tabular",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the print_tabular method want to see?",
        "Y": "input_nodes",
        "Z": "We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What IDEs usually have a debugger built in?",
        "Y": "PyCharm or VSCode",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you use in your IDE by pulling up a terminal window in your IDE?",
        "Y": "pdb",
        "Z": "IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the built-in debugger usually called?",
        "Y": "graphical wrapper",
        "Z": "IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a terminal window in VSCode called?",
        "Y": "pdb",
        "Z": "IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a built-in debugger?",
        "Y": "graphical wrapper",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The code that represents the graph is not in any source file, but we can step into it manually using what when the forward pass is invoked?",
        "Y": "pdb",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is invoked to step into the running program?",
        "Y": "pdb",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When the forward pass is invoked, what is used to step into the running program?",
        "Y": "pdb",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does FX use to capture the semantics of programs in a transformable/analyzable form?",
        "Y": "symbolic tracing",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the real name of the program that is executed by symbolic tracing?",
        "Y": "torch.nn.Module or function",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The data flowing through the program during this execution is not real data, but what?",
        "Y": "symbols",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does symbolic tracing have?",
        "Y": "some limitations",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is symbolic tracing also known as?",
        "Y": "symbolic execution",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the data flowing through the program during the execution of a torch.nn.Module or function?",
        "Y": "symbols",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of flow does symbolic tracing not support?",
        "Y": "dynamic control flow",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of dynamic control flow does symbolic tracing not support?",
        "Y": "loops",
        "Z": "Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a program that does not support dynamic control flow?",
        "Y": "examine the following program",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the main limitation of symbolic tracing?",
        "Y": "it does not currently support dynamic control flow",
        "Z": "Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are two examples of dynamic control flow?",
        "Y": "loops or if statements",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a program that does not currently support dynamic control flow?",
        "Y": "the following program",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does not currently support dynamic control flow?",
        "Y": "symbolic tracing",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of control flow does symbolic tracing not support?",
        "Y": "loops or if statements",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the condition depend on?",
        "Y": "input values",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The condition to the if statement relies on the value of what?",
        "Y": "x.sum()",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can change if you pass a new input tensor to the traced function?",
        "Y": "x",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is it called when a new input tensor is passed to the traced function?",
        "Y": "dynamic control flow",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What walks back through your code to show you where this situation happens?",
        "Y": "traceback",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens if you pass a new input tensor to the traced function?",
        "Y": "x can change",
        "Z": "For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How does the value of x change?",
        "Y": "if you pass a new input tensor to the traced function",
        "Z": "For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the program that walks back through your code to show you where this situation happens?",
        "Y": "The traceback",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What shows you where a dynamic control flow happens?",
        "Y": "traceback",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a dynamic control flow?",
        "Y": "if you pass a new input tensor to the traced function",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function that shows you where a situation happens?",
        "Y": "traceback",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is supported on the other hand?",
        "Y": "static control flow",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a static control flow?",
        "Y": "if statements whose value cannot change across invocations",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In what program does static control flow arise for code making decisions about a model's architecture based on hyper-parameters?",
        "Y": "PyTorch programs",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of example is a static control flow?",
        "Y": "concrete",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is supported in PyTorch?",
        "Y": "static control flow",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is static control flow?",
        "Y": "loops",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does static control flow arise for code making decisions about a model's architecture based on?",
        "Y": "hyper-parameters",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a static control flow?",
        "Y": "concrete example",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the if-statement if self.do_activation?",
        "Y": "static",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can do_activation be considered to be?",
        "Y": "hyper-parameter",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a valid pattern supported by?",
        "Y": "symbolic tracing",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does not depend on any function inputs, thus it is static?",
        "Y": "if-statement if self.do_activation",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is supported by the if-statement if self.do_activation?",
        "Y": "symbolic tracing",
        "Z": "The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are many instances of dynamic control flow?",
        "Y": "semantically static control flow",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can dynamic control flow be made to support symbolic tracing?",
        "Y": "removing the data dependencies on input values",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In dynamic control flow, the sections of the program that contain code can be traced as calls to what?",
        "Y": "the Method",
        "Z": "In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What function can be used to trace sections of a dynamic control flow?",
        "Y": "wrap()",
        "Z": "In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the method that can be traced in a dynamic control flow?",
        "Y": "Customizing Tracing with the Tracer class",
        "Z": "In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a function that can be traced instead of tracing through them?",
        "Y": "wrap()",
        "Z": "In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does FX use to intercept calls?",
        "Y": "__torch_function__",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What would we still like to capture builtin Python functions in?",
        "Y": "symbolic tracing",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What error tells us about the built-in function len?",
        "Y": "the built-in function len is not supported",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can we make it so that functions like len are recorded in the trace as direct calls?",
        "Y": "wrap() API",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the mechanism by which FX intercepts calls?",
        "Y": "__torch_function__",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Some functions are not covered by __torch_function__, but we would still like to capture them in symbolic tracing.",
        "Y": "builtin Python functions or those in the math module",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What built-in function is not supported by __torch_function__?",
        "Y": "len",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What API can we use to make it so that functions like len are recorded in the trace as direct calls?",
        "Y": "wrap() API",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What built-in function is not supported?",
        "Y": "len",
        "Z": "The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The Tracer class is the class that underlies the implementation of what?",
        "Y": "symbolic_trace",
        "Z": "The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can we make it so that functions like this are recorded in the trace as direct calls?",
        "Y": "wrap() API",
        "Z": "The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can the behavior of tracing be customized?",
        "Y": "subclassing Tracer",
        "Z": "In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The error tells us that what is not supported?",
        "Y": "built-in function len",
        "Z": "The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the class that underlies the implementation of symbolic_trace?",
        "Y": "The Tracer class",
        "Z": "In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What API can be used to make it so that functions like len are recorded in the trace as direct calls?",
        "Y": "wrap()",
        "Z": "The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the modules that appear as calls in the symbolic trace rather than being traced through?",
        "Y": "Leaf Modules",
        "Z": "Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the default set of leaf modules?",
        "Y": "standard torch.nn module instances",
        "Z": "Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The set of leaf modules can be customized by overriding what?",
        "Y": "Tracer.is_leaf_module()",
        "Z": "The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can the set of leaf modules be customized?",
        "Y": "overriding Tracer.is_leaf_module()",
        "Z": "The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are torch.zeros, torch.ones, torch.rand, torch.randn, torch.sparse_co",
        "Y": "Tensor constructors",
        "Z": "The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is currently not traceable?",
        "Y": "Tensor constructors",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be used and the value they produce will be embedded in the trace as a constant?",
        "Y": "deterministic constructors",
        "Z": "The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do the arguments to deterministic constructors refer to?",
        "Y": "dynamic input sizes",
        "Z": "The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What may be a viable substitute for deterministic constructors?",
        "Y": "ones_like or zeros_like",
        "Z": "The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are torch.zeros, torch.ones, torch.rand, torch.randn, and torch.sparse_",
        "Y": "traceable",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do the arguments to constructors refer to?",
        "Y": "dynamic input sizes",
        "Z": "The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will have a single random value embedded in the trace?",
        "Y": "Nondeterministic constructors",
        "Z": "Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Is this the intended behavior of nondeterministic constructors?",
        "Y": "not",
        "Z": "Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a workaround?",
        "Y": "wrap torch.randn in a torch.fx.wrap function and call that instead",
        "Z": "Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When will this behavior be fixed?",
        "Y": "future release",
        "Z": "The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type annotations will be preserved by symbolic tracing?",
        "Y": "Python 3-style type annotations",
        "Z": "Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is used to preserve type annotations?",
        "Y": "symbolic tracing",
        "Z": "Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Nondeterministic constructors (rand, randn) will have a single what value embedded in the trace?",
        "Y": "random",
        "Z": "Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the likely result of a single random value embedded in the trace?",
        "Y": "not the intended behavior",
        "Z": "Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be wrapped in a torch.fx.wrap function?",
        "Y": "torch.randn",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When will the behavior of nondeterministic constructors be fixed?",
        "Y": "in a future release",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type annotations are supported?",
        "Y": "Python 3-style type annotations",
        "Z": "The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type annotations are supported and will be preserved by symbolic tracing?",
        "Y": "Python 3-style type annotations",
        "Z": "The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How are Python 3-style type annotations preserved?",
        "Y": "symbolic tracing",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is not currently supported?",
        "Y": "Annotations on local names within a function are not currently supported",
        "Z": "The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type annotations are not currently supported?",
        "Y": "Python 2-style comment type annotations",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Annotations on what are not currently supported?",
        "Y": "local names",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be overridden to customize the set of leaf modules?",
        "Y": "Tracer.is_leaf_module()",
        "Z": "Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will this function return if given an nn.Module or function instance root?",
        "Y": "GraphModule",
        "Z": "Symbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What allows you to partially specialize your function?",
        "Y": "concrete_args",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX can't typically trace through data structures due to the presence of what?",
        "Y": "control flow",
        "Z": "Symbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What allows you to specialize on the value of b to trace through?",
        "Y": "concrete_args",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will a function return given an nn.Module or function instance root?",
        "Y": "GraphModule",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of function can typically not trace through concrete_args because of the presence of control flow?",
        "Y": "FX",
        "Z": "Symbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we use to specialize on the value of b to trace through?",
        "Y": "concrete_args",
        "Z": "FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX can't typically trace through a GraphModule because of the presence of what?",
        "Y": "control flow",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of function can typically not trace through a GraphModule because of the presence of control flow?",
        "Y": "FX",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does concrete_args allow you to do with your function?",
        "Y": "partially specialize",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX can typically not trace through this because of the presence of what?",
        "Y": "control flow",
        "Z": "FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can we use concrete_args to trace through control flow?",
        "Y": "to specialize on the value of b",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does concrete_args do to specialize on the value of b?",
        "Y": "f = fx.symbolic_trace",
        "Z": "FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of function can't trace through control flow?",
        "Y": "FX",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What value can we use concrete_args to specialize on?",
        "Y": "b",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be passed in concrete_args, but they will be ignored?",
        "Y": "different values of b",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX can't typically trace through this because of the presence of what?",
        "Y": "control flow",
        "Z": "FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we use to specialize on the value of b to trace through this?",
        "Y": "concrete_args",
        "Z": "FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Why can't FX trace through this?",
        "Y": "control flow",
        "Z": "For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does f =?",
        "Y": "fx.symbolic_trace",
        "Z": "f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX can't trace through this because of the presence of what?",
        "Y": "control flow",
        "Z": "FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function to be traced?",
        "Y": "f = fx.symbolic_trace",
        "Z": "f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we use to eliminate data-structure handling from our function?",
        "Y": "concrete_args",
        "Z": "Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does concrete_args use to flatten your input?",
        "Y": "pytrees",
        "Z": "f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "To avoid overspecializing, pass in what for values that shouldn't be specialized?",
        "Y": "fx.PH",
        "Z": "Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the root of a module or function to be traced and converted into?",
        "Y": "Graph representation",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What value can still be passed in, but they will be ignored?",
        "Y": "b",
        "Z": "Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will use pytrees to flatten your input?",
        "Y": "concrete_args",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does concrete_args use to flatten input?",
        "Y": "pytrees",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What eliminates data-structure handling from our function?",
        "Y": "concrete_args",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens when you pass in different values of b?",
        "Y": "they will be ignored",
        "Z": "Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "To avoid overspecializing, pass in what for values that shouldn\u2019t be specialized?",
        "Y": "fx.PH",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the representation of a module or function to be traced and converted into?",
        "Y": "Graph",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Inputs to be partially specialized are called what?",
        "Y": "concrete_args",
        "Z": "Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should be passed in for values that shouldn't be specialized?",
        "Y": "fx.PH",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should be passed in for values that shouldn\u2019t be specialized?",
        "Y": "fx.PH",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Inputs to be what type of ly specialized?",
        "Y": "partial",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does root convert into?",
        "Y": "Graph representation",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the module to be partially specialized enable_cpatching?",
        "Y": "concrete_args",
        "Z": "root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the module to be traced and converted into a Graph representation?",
        "Y": "GraphModule",
        "Z": "root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the representation of a Module or function to be traced and converted into?",
        "Y": "Graph",
        "Z": "Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What enables C-level patching of functions?",
        "Y": "enable_cpatching",
        "Z": "enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the Module created from the recorded operations from root?",
        "Y": "GraphModule",
        "Z": "root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a module created from the recorded operations from root?",
        "Y": "a Module created from the recorded operations from root",
        "Z": "concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can GraphModule be called?",
        "Y": "module-level scope",
        "Z": "concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will a \u201cleaf function\u201d be preserved as in the FX trace instead of being traced through?",
        "Y": "CallFunction",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized what",
        "Y": "enable_cpatching",
        "Z": "root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d?",
        "Y": "GraphModule",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can this function be used as?",
        "Y": "a decorator",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What allows C-level patching of functions?",
        "Y": "enable_cpatching",
        "Z": "concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can a \u201cleaf function\u201d be used as?",
        "Y": "a decorator",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a Module created from?",
        "Y": "a Module created from the recorded operations from root",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "GraphModule This function can be called at what level to register fn_or_name as a \"leaf function\"",
        "Y": "module-level scope",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A \u201cleaf function\u201d will be preserved as what node in the FX trace?",
        "Y": "CallFunction",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is created from the recorded operations from root?",
        "Y": "a Module",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can a function be called to register fn_or_name as a \u201cleaf function\u201d?",
        "Y": "module-level scope",
        "Z": "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can a wrapped function be used as?",
        "Y": "decorator",
        "Z": "This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the function or name of the global function to insert into the graph when it\u2019s called?",
        "Y": "fn_or_name",
        "Z": "This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the nn.Module generated from?",
        "Y": "fx.Graph",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the function or name of the global function to insert into the graph when it's called GraphModule?",
        "Y": "nn.Module",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What has a graph attribute, as well as code and forward attributes generated from that graph?",
        "Y": "Graphmodule",
        "Z": "concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does a Graphmodule contain?",
        "Y": "Warning",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does Graphmodule have?",
        "Y": "a graph attribute, as well as code and forward attributes generated from that graph",
        "Z": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When a graph is reassigned, code and forward will be what?",
        "Y": "automatically regenerated",
        "Z": "Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function that will update the generated code if you edit the contents of the graph without reassigning the graph attribute",
        "Y": "recompile()",
        "Z": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does recompile() do?",
        "Y": "Construct a GraphModule",
        "Z": "Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an nn.Module generated from an fx.Graph?",
        "Y": "GraphModule",
        "Z": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What attributes will be automatically regenerated when a graph is reassigned?",
        "Y": "code and forward",
        "Z": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you edit the contents of the graph without reassigning the graph attribute itself, you must call what to update the generated code?",
        "Y": "recompile()",
        "Z": "Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is another name for a GraphModule?",
        "Y": "Construct a GraphModule",
        "Z": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do you need to construct to update the generated code?",
        "Y": "GraphModule",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will be automatically regenerated when a graph is reassigned?",
        "Y": "code and forward",
        "Z": "Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When a graph is regenerated, code and forward will be automatically regenerated?",
        "Y": "reassigned",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of module can you construct?",
        "Y": "GraphModule",
        "Z": "Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What purpose does class_name denote the name of this GraphModule for?",
        "Y": "debugging",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If unset, all error messages will report as originating from where?",
        "Y": "GraphModule",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the original name of the GraphModule?",
        "Y": "root",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does this install if they are subpaths of target?",
        "Y": "empty Modules",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What denotes the name of this GraphModule for debugging purposes?",
        "Y": "class_name",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will report as originating from GraphModule if it's unset?",
        "Y": "all error messages",
        "Z": "root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the GraphModule?",
        "Y": "root\u2019s original name or a name that makes sense within the context of your transform",
        "Z": "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does it do to the given submodule?",
        "Y": "Adds the given submodule to self",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If the class_name is unset, all error messages will report as originating from where?",
        "Y": "GraphModule",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If class_name is unset, all error messages will report as originating from what?",
        "Y": "GraphModule",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should the class_name be set to?",
        "Y": "root\u2019s original name or a name that makes sense within the context of your transform",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the full-qualified string name of the new submodule?",
        "Y": "target",
        "Z": "Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the submodule itself?",
        "Y": "the actual object we want to install in the current Module",
        "Z": "Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the target of the new submodule?",
        "Y": "fully-qualified string name",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does m stand for?",
        "Y": "submodule",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the nn.Module?",
        "Y": "bool",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does target stand for?",
        "Y": "The fully-qualified string name of the new submodule",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the submodule itself?",
        "Y": "m",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the fully-qualified string of the new submodule?",
        "Y": "target",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does this method return?",
        "Y": "True",
        "Z": "m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What returns the Python code generated from the Graph underlying this GraphModule?",
        "Y": "Return the Python code generated from the Graph underlying this GraphModule",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens to the unused submodules?",
        "Y": "Deletes all unused submodules from self",
        "Z": "m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does bool return?",
        "Y": "Graph",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the Python code do to all unused submodules from self?",
        "Y": "Deletes all unused submodules",
        "Z": "m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value for each object in the chain denoted by target?",
        "Y": "True",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does self do to unused submodules?",
        "Y": "Deletes all unused submodules",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Python code generated from what underlying GraphModule?",
        "Y": "Graph",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "To return True, each object in the chain must either a) not exist yet, or b) reference an what?",
        "Y": "nn.Module",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does self do?",
        "Y": "Deletes all unused submodules",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is considered \"used\" if any of the following is true?",
        "Y": "A Module",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does a Module have that are used?",
        "Y": "children",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How is the forward of a Module called?",
        "Y": "call_module node 3",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can this method be called to do?",
        "Y": "clean up an nn.Module without manually calling delete_submodule on each unused submodule",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does delete_submodule do?",
        "Y": "Deletes the given submodule from self",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How is a Module considered to be used?",
        "Y": "A Module is considered \u201cused\u201d if any one of the following is true",
        "Z": "Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How is a Module's forward called?",
        "Y": "call_module node 3",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Deletes the given submodule from self. Deletes the given submodule from self.",
        "Y": "delete_submodule",
        "Z": "bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is returned from the Graph underlying this GraphModule?",
        "Y": "Python code",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can this method be called to do without manually calling delete_submodule on each unused submodule?",
        "Y": "clean up an nn.Module",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Deletes the given submodule from self without manually calling what?",
        "Y": "delete_submodule",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does delete all unused submodules from self?",
        "Y": "Deletes all unused submodules from self",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How is the forwarding of a module called?",
        "Y": "call_module node 3",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method can be called to clean up an nn.Module without manually calling?",
        "Y": "delete_submodule",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens when a submodule is not a valid target?",
        "Y": "Deletes the given submodule from self",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When will the module not be deleted?",
        "Y": "if target is not a valid target",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What node is used to call a Module's forward?",
        "Y": "call_module",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does delete_submodule do to a given submodule from self?",
        "Y": "Deletes",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When is the module not deleted?",
        "Y": "if target is not a valid target",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When will a module not be deleted?",
        "Y": "if target is not a valid target",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method can be used to clean up an nn.Module without manually calling on each unused submodule?",
        "Y": "delete_submodule",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does delete_submodule do to a given submodule?",
        "Y": "Deletes",
        "Z": "This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to delete?",
        "Y": "submodule",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A return value of what indicates that the target was not a valid reference to a submodule?",
        "Y": "False",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name>",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the method that can be called to clean up an nn.Module without manually calling delete_submodule?",
        "Y": "bool",
        "Z": "This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method can be called to clean up an nn.Module without manually calling on each unused submodule?",
        "Y": "delete_submodule",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will the module not be deleted?",
        "Y": "if target is not a valid target",
        "Z": "Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is target?",
        "Y": "The fully-qualified string name of the new submodule",
        "Z": "The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A return value of what means that the target was not a valid reference to a submodule?",
        "Y": "False",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the submodule that is not a valid reference to a submodule?",
        "Y": "bool",
        "Z": "This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where is the given submodule deleted from?",
        "Y": "self",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens to the given submodule from self?",
        "Y": "Deletes",
        "Z": "Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What returns the Graph underlying this GraphModule?",
        "Y": "bool",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What causes the module to not be deleted?",
        "Y": "if target is not a valid target",
        "Z": "The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to do with a submodule?",
        "Y": "delete",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When should bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute?",
        "Y": "after editing the contained graph",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the fully-qualified string name of the new submodule?",
        "Y": "target",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute?",
        "Y": "bool",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The generated code of this GraphModule will be what?",
        "Y": "out of date",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When should bool Return the Graph underlying this GraphModule be called?",
        "Y": "after editing the contained graph",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name>",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does dump out module to folder with?",
        "Y": "module_name",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name>",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What return value means that the target was not a valid reference to a submodule?",
        "Y": "False",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Dumps out module to folder with what so that it can be imported with from folder> import module_name>?",
        "Y": "module_name",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name>",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is each chunk of the tensor?",
        "Y": "a view of the original tensor",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "If what is an integer type, tensor will be split into equally sized chunks?",
        "Y": "split_size_or_sections",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "What will be smaller if the tensor size along the given dimension dim is not divisible by split_size?",
        "Y": "Last chunk",
        "Z": "If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "If split_size_or_sections is a list, tensor will be split into what chunks?",
        "Y": "len",
        "Z": "If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "What does split the tensor into?",
        "Y": "chunks",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "What is each chunk of a tensor?",
        "Y": "a view of the original tensor",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "If split_size_or_sections is an integer type, tensor will be split into what?",
        "Y": "equally sized chunks",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "If the tensor size along the given dimension dim is not divisible by split_size, what will make the last chunk smaller?",
        "Y": "if the tensor size along the given dimension dim is not divisible by split_size",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "If split_size_or_sections is what, then tensor will be split into len(split_size_or",
        "Y": "a list",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "If tensor is an integer type, then tensor will be split into equally sized chunks?",
        "Y": "split_size_or_sections",
        "Z": "If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "What is another name for tensor to split?",
        "Y": "Tensor",
        "Z": "If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "What will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_",
        "Y": "tensor",
        "Z": "If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "What is split_size_or_sections?",
        "Y": "int",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "What is an example of a tensor to split?",
        "Y": "Example",
        "Z": "If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "Returns what with the signs of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the signs of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign"
    },
    {
        "X": "What is the name of the extension created for C++?",
        "Y": "setuptools",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How many arguments does setuptools.Extension have to build a C++ extension?",
        "Y": "bare minimum",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Where are all arguments forwarded to?",
        "Y": "setuptools.Extension constructor",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What language does setuptools.Extension build for?",
        "Y": "CUDA/C++",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How many arguments does setuptools.Extension have to build a CUDA/C++ extension?",
        "Y": "bare minimum",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is included in the setuptools.Extension for CUDA/C++?",
        "Y": "path, library path and runtime library",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the Convenience method that creates a setuptools.Extension for C++?",
        "Y": "setuptools.Extension for C++",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension?",
        "Y": "Convenience method",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "All arguments are forwarded to what?",
        "Y": "setuptools.Extension constructor",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What creates a setuptools.Extension with the bare minimum arguments to build a CUDA/C++ extension?",
        "Y": "Convenience method",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is included in the setuptools.Extension?",
        "Y": "CUDA include path, library path and runtime library",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What language does the example create a setuptools.Extension for?",
        "Y": "CUDA/C++",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is included in the setuptools.Extension to build a CUDA/C++ extension?",
        "Y": "path, library path and runtime library",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What extension is created by the convenience method that creates a setuptools.Extension?",
        "Y": "setuptools.Extension for CUDA/C++",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C",
        "Y": "Convenience method",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the extension created for CUDA/C++?",
        "Y": "setuptools",
        "Z": "Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an example of a setuptools.Extension?",
        "Y": "Compute capabilities",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the example create?",
        "Y": "setuptools.Extension for CUDA/C++",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What are the bare minimum arguments to build a CUDA/C++ extension?",
        "Y": "CUDA include path, library path and runtime library",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Where are all arguments for a setuptools.Extension for CUDA/C++?",
        "Y": "setuptools.Extension constructor",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an example of a setuptools.Extension constructor?",
        "Y": "Compute capabilities",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the extension for CUDA/C++?",
        "Y": "setuptools",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the method that creates a setuptools.Extension?",
        "Y": "Convenience",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the extension constructor?",
        "Y": "setuptools",
        "Z": "Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the example create for CUDA/C++?",
        "Y": "setuptools.Extension",
        "Z": "Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the method that creates a setuptools.Extension with the bare minimum arguments to build a CUDA",
        "Y": "Convenience",
        "Z": "Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Convenience method that creates a setuptools.Extension with the bare minimum (usually sufficient) arguments to build a CU",
        "Y": "Creates a setuptools.Extension for CUDA/C++",
        "Z": "Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What are the CUDA include?",
        "Y": "path, library path and runtime library",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an example of a CUDA/C++ extension?",
        "Y": "Compute capabilities",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What creates a setuptools.Extension?",
        "Y": "Convenience method",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is included in the Convenience method to build a CUDA/C++ extension?",
        "Y": "CUDA include path, library path and runtime library",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the constructor used by setuptools?",
        "Y": "Extension constructor",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an example of what?",
        "Y": "Compute capabilities",
        "Z": "Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is one of the functions of a computer?",
        "Y": "Compute capabilities",
        "Z": "Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What are the capabilities of what?",
        "Y": "Compute capabilities",
        "Z": "Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "By default, the extension will be what to run on all archs of the cards visible during the building process of the extension?",
        "Y": "compiled",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If a new card is installed, the extension may need to be what?",
        "Y": "recompiled",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What compute capability does a visible card have?",
        "Y": "CC",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "The extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus what else?",
        "Y": "PTX",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension .",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is a compute capability that is newer than the newest version for which nvcc can build fully-compiled binaries?",
        "Y": "CC",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can you use to override the default behavior?",
        "Y": "TORCH_CUDA_ARCH_LIST",
        "Z": "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the default behavior of TORCH_CUDA_ARCH_LIST?",
        "Y": "5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX",
        "Z": "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does TORCH_CUDA_ARCH_LIST mean?",
        "Y": "python build_my_extension.py",
        "Z": "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What do you want the extension to support?",
        "Y": "CCs",
        "Z": "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the TORCH_CUDA_ARCH_LIST?",
        "Y": "python build_my_extension.py",
        "Z": "TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does python build_my_extension.py contain?",
        "Y": "python build_my_extension.py",
        "Z": "TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What will happen if more archs are included?",
        "Y": "the slower the building process",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does setuptools.build_ext subclass take care of passing the minimum required compiler flags?",
        "Y": "custom setuptools build extension",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the setuptools.build_ext subclass take care of passing?",
        "Y": "mixed C++/CUDA compilation",
        "Z": "This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is slower the building process will be?",
        "Y": "the more archs get included",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does setuptools.build_ext take care of passing?",
        "Y": "minimum required compiler flags",
        "Z": "A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "The setuptools.build_ext subclass takes care of passing the minimum required compiler flags as well as what?",
        "Y": "mixed C++/CUDA compilation",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the setuptools.build_ext subclass take care of passing the minimum required compiler flags?",
        "Y": "custom setuptools build extension",
        "Z": "A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What subclass takes care of passing the minimum required compiler flags?",
        "Y": "setuptools.build_ext",
        "Z": "This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the setuptools.build_ext subclass support?",
        "Y": "CUDA files",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the setuptools.build_ext subclass handle?",
        "Y": "mixed C++/CUDA compilation",
        "Z": "This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "When using BuildExtension, it is allowed to supply a dictionary for what?",
        "Y": "extra_compile_args",
        "Z": "This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "When using BuildExtension, it is allowed to supply what for extra_compile_args?",
        "Y": "a dictionary",
        "Z": "When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "When is it possible to supply different flags to the C++ and CUDA compiler?",
        "Y": "mixed compilation",
        "Z": "This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does this make it possible to supply to the C++ and CUDA compiler during mixed compilation?",
        "Y": "different flags",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the default name for the Ninja backend?",
        "Y": "use_ninja",
        "Z": "use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What greatly speeds up compilation compared to the standard setuptools.build_ext?",
        "Y": "Ninja",
        "Z": "use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the standard setuptools.build_ext use if Ninja is not available?",
        "Y": "Fallbacks",
        "Z": "use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How many workers does the Ninja backend use to build the extension?",
        "Y": "#CPUS + 2 workers",
        "Z": "use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How much resources does the Ninja backend use on some systems?",
        "Y": "too many resources",
        "Z": "By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What environment variable can be set to a non-negative number?",
        "Y": "MAX_JOBS",
        "Z": "By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does Ninja greatly speed up compared to setuptools.build_ext?",
        "Y": "compilation",
        "Z": "use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Fallbacks to what backend if Ninja is not available?",
        "Y": "standard distutils",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "By default, the Ninja backend uses what to build the extension?",
        "Y": "#CPUS + 2 workers",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How can one control the number of workers?",
        "Y": "setting the MAX_JOBS environment variable to a non-negative number",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the Ninja backend use to build the extension?",
        "Y": "#CPUS + 2 workers",
        "Z": "By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the Ninja backend do?",
        "Y": "Loads a PyTorch C++ extension just-in-time (JIT)",
        "Z": "When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does this use up on some systems?",
        "Y": "too many resources",
        "Z": "By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does JIT stand for?",
        "Y": "just-in-time",
        "Z": "By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is another name for just-in-time?",
        "Y": "JIT",
        "Z": "Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What extension is loaded just-in-time?",
        "Y": "PyTorch C++",
        "Z": "Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is emitted to load an extension?",
        "Y": "a Ninja build file",
        "Z": "By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "In what process is the library loaded?",
        "Y": "Python",
        "Z": "By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the library loaded into the current Python process as?",
        "Y": "module",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "In what program is the library loaded?",
        "Y": "Python",
        "Z": "To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is used to compile the sources?",
        "Y": "the default system compiler",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can be provided to pass additional arguments to the compilation process?",
        "Y": "extra_cflags or extra_ldflags",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What would you use extra_cflags=['-O3'\"?",
        "Y": "to compile your extension with optimizations",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can you use to pass further include directories?",
        "Y": "extra_cflags",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can be set to override the default system compiler?",
        "Y": "CXX environment variable",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "To compile your extension with optimizations, pass what?",
        "Y": "extra_cflags=['-O3'].",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the extension to build?",
        "Y": "name",
        "Z": "name \u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module! sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What must the name of the extension to build be the same as?",
        "Y": "pybind11 module",
        "Z": "name \u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module! sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is sources?",
        "Y": "A list of relative or absolute paths to C++ source files",
        "Z": "name \u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module! sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Extra_cflags - optional list of what to forward to the build?",
        "Y": "compiler flags",
        "Z": "name \u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module! sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What do extra_cuda_cflags forward to when building CUDA sources?",
        "Y": "nvcc",
        "Z": "extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does extra_ldflags list to forward to the build?",
        "Y": "linker flags",
        "Z": "extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can extra_include_paths forward to the build?",
        "Y": "include directories",
        "Z": "extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Name \u2013 The name of the extension to build must be the same as the name of what module?",
        "Y": "pybind11",
        "Z": "name \u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module! sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is a list of relative or absolute paths to C++ source files?",
        "Y": "sources",
        "Z": "name \u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module! sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an optional list of compiler flags to forward to the build?",
        "Y": "extra_cflags",
        "Z": "extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does extra_cuda_cflags forward to when building CUDA sources?",
        "Y": "nvcc",
        "Z": "name \u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module! sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an optional list of linker flags to forward to the build?",
        "Y": "extra_ldflags",
        "Z": "extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "extra_include_paths \u2013 optional list of what to forward to the build?",
        "Y": "include directories",
        "Z": "extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is sources a list of?",
        "Y": "relative or absolute paths to C++ source files",
        "Z": "sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does extra_cflags forward to the build?",
        "Y": "compiler flags",
        "Z": "extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does extra_ldflags include to forward to the build?",
        "Y": "linker flags",
        "Z": "sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the optional path to use as build workspace?",
        "Y": "build_directory",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Sources \u2013 A list of what to C++ source files?",
        "Y": "relative or absolute paths",
        "Z": "sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an optional list of compiler flags to forward to nvcc when building CUDA sources?",
        "Y": "extra_cuda_cflags",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an optional path to use as build workspace?",
        "Y": "build_directory",
        "Z": "sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What turns on verbose logging of load steps?",
        "Y": "If True",
        "Z": "extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If True, turns on verbose logging of load steps.",
        "Y": "verbose",
        "Z": "extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does extra_cuda_cflags forward to nvcc when building CUDA sources?",
        "Y": "compiler flags",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does extra_ldflags forward to the build?",
        "Y": "linker flags",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does extra_include_paths provide to forward to the build?",
        "Y": "include directories",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Extra_include_paths \u2013 optional list of what to forward to the build?",
        "Y": "include directories",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Extra_ldflags - optional list of what to forward to the build?",
        "Y": "linker flags",
        "Z": "extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Extra_include_paths - optional list of what to forward to the build?",
        "Y": "include directories",
        "Z": "extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is build_directory used for?",
        "Y": "build workspace",
        "Z": "extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Is_python_module \u2013 If what, imports the produced shared library as a Python module?",
        "Y": "If True",
        "Z": "is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.)",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If False, behavior depends on what?",
        "Y": "is_standalone",
        "Z": "is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If False, what load the constructed extension into the process as a plain dynamic library?",
        "Y": "is_standalone",
        "Z": "is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What happens if False?",
        "Y": "build a standalone executable",
        "Z": "is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.)",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "The loaded PyTorch extension is returned as what?",
        "Y": "Python module",
        "Z": "is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.)",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the PyTorch extension return?",
        "Y": "nothing",
        "Z": "is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.)",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the shared library loaded into the process as?",
        "Y": "side effect",
        "Z": "is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.)",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the extension that is loaded into the process as a plain dynamic library?",
        "Y": "is_standalone",
        "Z": "is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What happens if is_standalone is True?",
        "Y": "build a standalone executable",
        "Z": "is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the loaded PyTorch extension as?",
        "Y": "Python module",
        "Z": "is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does is_python_module return?",
        "Y": "nothing",
        "Z": "is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is loaded into the process as a side effect?",
        "Y": "The shared library",
        "Z": "is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is added to the PATH environment variable on Windows?",
        "Y": "TORCH_LIB_PATH",
        "Z": "is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If is_python_module is what?",
        "Y": "True Example",
        "Z": "is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the loaded PyTorch extension return as?",
        "Y": "Python module",
        "Z": "is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does it return if is_python_module is True Example Loads a PyTorch extension just-",
        "Y": "nothing",
        "Z": "Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is returned when a PyTorch extension is loaded as a Python module?",
        "Y": "path to the executable",
        "Z": "Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Is_python_module is what?",
        "Y": "True Example",
        "Z": "Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is loaded as a Python module?",
        "Y": "PyTorch extension",
        "Z": "Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does Returns the loaded PyTorch extension as a Python module?",
        "Y": "nothing",
        "Z": "Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "On what platform is TORCH_LIB_PATH added as a side effect?",
        "Y": "Windows",
        "Z": "Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does TORCH_LIB_PATH do?",
        "Y": "Return the path to the executable",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does is_python_module load from string sources?",
        "Y": "PyTorch C++ extension just-in-time (JIT)",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does is_python_module behave exactly like?",
        "Y": "load()",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the function that loads a PyTorch C++ extension just-in-time?",
        "Y": "load_inline()",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is a good example of how to use load_inline()?",
        "Y": "tests",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does is_python_module do?",
        "Y": "True Example Loads a PyTorch C++ extension just-in-time",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the difference between load() and load()?",
        "Y": "takes its sources as strings rather than filenames",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Where are the strings stored?",
        "Y": "build directory",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What are the tests for using this function?",
        "Y": "good examples",
        "Z": "Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is different from torch.Tensor.repeat() but similar to numpy.repeat?",
        "Y": "Repeat elements",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is torch.Tensor.repeat() similar to?",
        "Y": "numpy.repeat",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is broadcasted to fit the shape of the given axis?",
        "Y": "repeats",
        "Z": "input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "Why is repeats broadcasted?",
        "Y": "to fit the shape of the given axis",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is the dimension along which to repeat values?",
        "Y": "dim",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is the default way to return a flat output array?",
        "Y": "flattened input array",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "Where is a repeated tensor different from an input tensor?",
        "Y": "the given axis",
        "Z": "repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is the name of the tensor that has the same shape as input?",
        "Y": "Tensor",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is the name of the tensor that is different from numpy.repeat?",
        "Y": "torch.Tensor.repeat()",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What does repeats refer to?",
        "Y": "The number of repetitions for each element",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "How are repeats broadcasted?",
        "Y": "to fit the shape of the given axis",
        "Z": "repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "By default, use the flattened input array, and return a what?",
        "Y": "flat output array",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "Where does the repeated tensor differ from the input tensor?",
        "Y": "axis",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "By default, what is used to return a flat output array?",
        "Y": "flattened input array",
        "Z": "repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is an example of a repeatable tensor?",
        "Y": "Tensor",
        "Z": "repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is similar to torch.Tensor.repeat()?",
        "Y": "numpy.repeat",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is this different from?",
        "Y": "torch.Tensor.repeat()",
        "Z": "Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "The repeated tensor has the same shape as input, except along what axis?",
        "Y": "axis",
        "Z": "input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is different from numpy.repeat?",
        "Y": "torch.Tensor.repeat()",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "Where is a repeated tensor different from the input tensor?",
        "Y": "the given axis",
        "Z": "input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is an example of an input tensor?",
        "Y": "Tensor",
        "Z": "input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What does repeats stand for?",
        "Y": "The number of repetitions for each element",
        "Z": "repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is another name for repeats?",
        "Y": "Tensor or int",
        "Z": "repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "Where is the repeated tensor different from the input tensor?",
        "Y": "axis",
        "Z": "repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "By default, use what to return a flat output array?",
        "Y": "flattened input array",
        "Z": "dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is different about a repeated tensor than an input tensor?",
        "Y": "the given axis",
        "Z": "dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "How many times does 0 appear in a tensor?",
        "Y": "n1 times",
        "Z": "dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "Along what axis does the tensor have the same shape as input?",
        "Y": "axis",
        "Z": "dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "If the repeats is tensor([0, 0,..., 1, 1,..., 2, 2,...,...]),",
        "Y": "n1",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What happens when the scalar other is added to each element of the input input and returns a new resulting tensor?",
        "Y": "Adds the scalar other to each element of the input input and returns a new resulting tensor",
        "Z": "Adds the scalar other to each element of the input input\nand returns a new resulting tensor. If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned.",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "If input is of type what must other be a real number?",
        "Y": "FloatTensor or DoubleTensor",
        "Z": "Adds the scalar other to each element of the input input\nand returns a new resulting tensor. If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned.",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "How is each element of the tensor other multiplied?",
        "Y": "multiplied by the scalar alpha and added to each element of the tensor input",
        "Z": "other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "If input is of type what, other must be a real number, otherwise it should be an integer?",
        "Y": "FloatTensor or DoubleTensor",
        "Z": "Adds the scalar other to each element of the input input\nand returns a new resulting tensor. If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned.",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "Each element of the tensor other is multiplied by what?",
        "Y": "scalar alpha",
        "Z": "Adds the scalar other to each element of the input input\nand returns a new resulting tensor. If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is returned after each element of the tensor input is multiplied by the scalar alpha?",
        "Y": "The resulting tensor",
        "Z": "Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What type of input must be a real number?",
        "Y": "FloatTensor or DoubleTensor",
        "Z": "If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable.",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What must the shapes of input and other be?",
        "Y": "broadcastable",
        "Z": "If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable.",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is returned after each element of input is multiplied by the scalar alpha?",
        "Y": "The resulting tensor",
        "Z": "other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "The shapes of input and other must be what?",
        "Y": "broadcastable",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "What is the first input tensor?",
        "Y": "input",
        "Z": "other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What must be a real number if other is of type FloatTensor or DoubleTensor?",
        "Y": "alpha",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor?",
        "Y": "other",
        "Z": "other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is returned after each element of the input tensor is multiplied by the scalar alpha?",
        "Y": "The resulting tensor",
        "Z": "input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What must alpha be if other is of type FloatTensor or DoubleTensor?",
        "Y": "real number",
        "Z": "Adds the scalar other to each element of the input input\nand returns a new resulting tensor. If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is the number to be added to each element of input out?",
        "Y": "output tensor",
        "Z": "other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What type of tensor must the alpha be a real number?",
        "Y": "FloatTensor or DoubleTensor",
        "Z": "other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is out (Tensor, optional)?",
        "Y": "output tensor",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is multiplied by each element of the tensor other?",
        "Y": "scalar alpha",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is the scalar multiplier for other?",
        "Y": "the second input tensor alpha",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "Out (Tensor, optional) \u2013 what is the output tensor?",
        "Y": "output tensor",
        "Z": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What is the second input tensor alpha?",
        "Y": "the second input tensor alpha",
        "Z": "Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What multiplier is added to each element of the tensor input?",
        "Y": "scalar alpha",
        "Z": "Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What returns the random number generator state as a torch?",
        "Y": "ByteTensor",
        "Z": "Returns the random number generator state as a torch.ByteTensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.get_rng_state.html#torch.get_rng_state"
    },
    {
        "X": "What is the Kronecker product denoted by?",
        "Y": "otimes",
        "Z": "Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other. If input is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n)(a0\u200b\u00d7a1\u200b\u00d7\u22ef\u00d7an\u200b) tensor and other is a\n(b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n)(b0\u200b\u00d7b1\u200b\u00d7\u22ef\u00d7bn\u200b) tensor, the result will be a\n(a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots \\times a_n*b_n)(a0\u200b\u2217b0\u200b\u00d7a1\u200b\u2217b1\u200b\u00d7\u22ef\u00d7an\u200b\u2217bn\u200b) tensor with the following entries:",
        "source": "https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron"
    },
    {
        "X": "What are the elements of a complex tensor?",
        "Y": "Cartesian coordinates",
        "Z": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle. abs (Tensor) \u2013 The absolute value the complex tensor. Must be float or\ndouble. angle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as\nabs. out (Tensor) \u2013 If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "What is the absolute value of the complex tensor?",
        "Y": "abs",
        "Z": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle. abs (Tensor) \u2013 The absolute value the complex tensor. Must be float or\ndouble. angle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as\nabs. out (Tensor) \u2013 If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "What must be the absolute value of the complex tensor?",
        "Y": "float or double",
        "Z": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle. abs (Tensor) \u2013 The absolute value the complex tensor. Must be float or\ndouble. angle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as\nabs. out (Tensor) \u2013 If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "What is the angle of the complex tensor?",
        "Y": "angle",
        "Z": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle. abs (Tensor) \u2013 The absolute value the complex tensor. Must be float or\ndouble. angle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as\nabs. out (Tensor) \u2013 If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "What must the angle of the complex tensor be?",
        "Y": "same dtype",
        "Z": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle. abs (Tensor) \u2013 The absolute value the complex tensor. Must be float or\ndouble. angle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as\nabs. out (Tensor) \u2013 If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "Out (Tensor) \u2013 If the inputs are torch.float32, must be what?",
        "Y": "torch.complex64",
        "Z": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle. abs (Tensor) \u2013 The absolute value the complex tensor. Must be float or\ndouble. angle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as\nabs. out (Tensor) \u2013 If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "What must the inputs be if the inputs are torch.float64?",
        "Y": "torch.complex128",
        "Z": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle. abs (Tensor) \u2013 The absolute value the complex tensor. Must be float or\ndouble. angle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as\nabs. out (Tensor) \u2013 If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "What is an example of a complex tensor?",
        "Y": "Example",
        "Z": "Constructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value abs and angle\nangle. abs (Tensor) \u2013 The absolute value the complex tensor. Must be float or\ndouble. angle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as\nabs. out (Tensor) \u2013 If the inputs are torch.float32, must be\ntorch.complex64. If the inputs are torch.float64, must be\ntorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "What is the sum of exponentiations of the inputs in base-2?",
        "Y": "Logarithm",
        "Z": "Logarithm of the sum of exponentiations of the inputs in base-2. Calculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right)log2\u200b(2x+2y). See\ntorch.logaddexp() for more details. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2"
    },
    {
        "X": "What is the name of the function that calculates the logarithm of the sum of exponentiations of the inputs in base-2?",
        "Y": "torch.logaddexp()",
        "Z": "Logarithm of the sum of exponentiations of the inputs in base-2. Calculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right)log2\u200b(2x+2y). See\ntorch.logaddexp() for more details. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2"
    },
    {
        "X": "What is the sum of exponentiations of the inputs in base-2 called?",
        "Y": "Logarithm",
        "Z": "Logarithm of the sum of exponentiations of the inputs in base-2. Calculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right)log2\u200b(2x+2y). See\ntorch.logaddexp() for more details. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2"
    },
    {
        "X": "What is the name of the function that calculates pointwise logarithm?",
        "Y": "torch.logaddexp()",
        "Z": "Logarithm of the sum of exponentiations of the inputs in base-2. Calculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right)log2\u200b(2x+2y). See\ntorch.logaddexp() for more details. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2"
    },
    {
        "X": "What does it do to determine if PyTorch operations must use deterministic algorithms?",
        "Y": "Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is a deterministic algorithm?",
        "Y": "always produce the same output",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "If only nondeterministic algorithms are available, what will PyTorch operations throw when called?",
        "Y": "RuntimeError",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is torch.nn.Conv1d called on?",
        "Y": "CUDA tensor",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "Sets whether PyTorch operations must use what?",
        "Y": "PyTorch operations must use \u201cdeterministic\u201d algorithms",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What are deterministic algorithms?",
        "Y": "algorithms which, given the same input, and when run on the same software and hardware, always produce the same output",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When nondeterministic algorithms are available, what will they throw when called?",
        "Y": "RuntimeError",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What operation will act deterministically when mode=True?",
        "Y": "torch.nn.Conv1d",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When mode=True, the following normally-nondeterministic operations will act what?",
        "Y": "deterministically",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What operations will act deterministically when mode=True?",
        "Y": "normally-nondeterministic operations",
        "Z": "The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When will normally-nondeterministic operations act deterministically?",
        "Y": "mode=True",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does index_put have to do with?",
        "Y": "accumulate=False",
        "Z": "torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is torch.nn.Conv1d when called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is torch.nn.Conv2d called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the index a list of tensors torch?",
        "Y": "a CPU tensor",
        "Z": "torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is torch.nn.Conv3d called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.ConvTranspose1d called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.ConvTranspose2d called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does a CUDA tensor require?",
        "Y": "grad torch",
        "Z": "torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does a CUDA tensor require when the input dimension is one and called on a CUDA tensor?",
        "Y": "grad",
        "Z": "torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is torch.bmm() called on sparse-dense?",
        "Y": "CUDA tensors torch",
        "Z": "torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is a list of tensors torch?",
        "Y": "index",
        "Z": "torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the index of a CPU tensor?",
        "Y": "a list of tensors",
        "Z": "torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is index_put() with accumulate=True called on?",
        "Y": "a CPU tensor torch",
        "Z": "torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is index_add() called?",
        "Y": "CUDA tensor torch",
        "Z": "torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What tensor requires grad torch?",
        "Y": "CUDA",
        "Z": "torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When will normally-nondeterministic operations throw a RuntimeError?",
        "Y": "mode=True",
        "Z": "torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True:",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What will normally-nondeterministic operations throw when mode=True?",
        "Y": "RuntimeError",
        "Z": "The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What requires grad torch?",
        "Y": "CUDA tensor",
        "Z": "torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is used when trying to differentiate a CUDA tensor torch?",
        "Y": "torch.repeat_interleave",
        "Z": "torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.index_add() called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does torch.index_select() attempt to differentiate?",
        "Y": "CUDA tensor",
        "Z": "torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is index_copy() called on?",
        "Y": "a CPU or CUDA tensor",
        "Z": "torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does torch.repeat_interleave() attempt to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is torch.Tensor.index_copy() called on?",
        "Y": "a CPU or CUDA tensor",
        "Z": "torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What will normally-nondeterministic operations throw when mode=True: torch?",
        "Y": "RuntimeError",
        "Z": "The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What will throw a RuntimeError when mode=True?",
        "Y": "normally-nondeterministic operations",
        "Z": "The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What type of torch does AvgPool3d attempt to differentiate?",
        "Y": "a CUDA tensor torch",
        "Z": "torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "AvgPool3d when attempting to differentiate what?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is AdaptiveAvgPool2d used to differentiate?",
        "Y": "a CUDA tensor torch",
        "Z": "torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is AdaptiveAvgPool2d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What mode is used when trying to differentiate a CUDA tensor?",
        "Y": "linear bilinear bicubic trilinear",
        "Z": "torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What mode is used when attempting to differentiate a CUDA tensor?",
        "Y": "mode='max'",
        "Z": "torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max'",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What type of torch is AdaptiveMaxPool2d when attempting to differentiate?",
        "Y": "CUDA tensor",
        "Z": "torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is a fractionalmaxPool2d when trying to differentiate?",
        "Y": "a CUDA tensor torch",
        "Z": "torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What type of torch does FunctionalMaxPool2d attempt to differentiate?",
        "Y": "CUDA tensor",
        "Z": "torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is used when attempting to differentiate a CUDA tensor torch?",
        "Y": "ReplicationPad1d",
        "Z": "torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What type of torch is used when trying to differentiate a CUDA tensor torch?",
        "Y": "torch.nn.AdaptiveMaxPool2d",
        "Z": "torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm()",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When attempting to differentiate a CUDA tensor torch, what does ReflectPad1d do?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What type of torch is used when attempting to differentiate a CUDA tensor torch?",
        "Y": "trilinear torch",
        "Z": "trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the ReflectionPad1d when trying to differentiate?",
        "Y": "a CUDA tensor torch",
        "Z": "torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is EmbeddingBag used to differentiate a CUDA tensor?",
        "Y": "mode='max'",
        "Z": "torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max'",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When attempting to differentiate a CUDA tensor torch, what is ReflectionPad2d?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max'",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is used when trying to differentiate a CUDA tensor when mode='max' torch?",
        "Y": "EmbeddingBag",
        "Z": "torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When attempting to differentiate a CUDA tensor torch, what does ReplicationPad1d do?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does ReplicationPad2d attempt to differentiate?",
        "Y": "a CUDA tensor torch",
        "Z": "torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does ReplicationPad3d attempt to differentiate?",
        "Y": "a CUDA tensor torch",
        "Z": "torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "ReplicationPad3d when attempting to differentiate a what?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is torch.nn.NLLLoss called on?",
        "Y": "a CUDA tensor torch",
        "Z": "torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is torch.nn.NLLLoss when called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does torch.nn.CTCLoss attempt to differentiate?",
        "Y": "a CUDA tensor torch",
        "Z": "torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is CTCLoss when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does median() output when called on a CUDA tensor?",
        "Y": "indices",
        "Z": "torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When mode='max' torch, what does torch.nn.EmbeddingBag attempt to differentiate?",
        "Y": "CUDA tensor",
        "Z": "torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is required when a CUDA tensor is called on a CUDA tensor?",
        "Y": "grad",
        "Z": "torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does a CUDA tensor require when the input dimension is larger than one?",
        "Y": "grad",
        "Z": "torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the result of the element-wise multiplication of tensor1 by tensor2?",
        "Y": "scalar value",
        "Z": "Performs the element-wise multiplication of tensor1\nby tensor2, multiply the result by the scalar value\nand add it to input. The shapes of tensor, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the tensor to be multiplied tensor2 (Tensor) \u2013 the tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"
    },
    {
        "X": "The shapes of tensor, tensor1, and tensor2 must be what?",
        "Y": "broadcastable",
        "Z": "Performs the element-wise multiplication of tensor1\nby tensor2, multiply the result by the scalar value\nand add it to input. The shapes of tensor, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the tensor to be multiplied tensor2 (Tensor) \u2013 the tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"
    },
    {
        "X": "What types of inputs must be a real number?",
        "Y": "FloatTensor or DoubleTensor",
        "Z": "Performs the element-wise multiplication of tensor1\nby tensor2, multiply the result by the scalar value\nand add it to input. The shapes of tensor, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the tensor to be multiplied tensor2 (Tensor) \u2013 the tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"
    },
    {
        "X": "What is the tensor to be multiplied?",
        "Y": "tensor to be multiplied tensor2",
        "Z": "Performs the element-wise multiplication of tensor1\nby tensor2, multiply the result by the scalar value\nand add it to input. The shapes of tensor, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the tensor to be multiplied tensor2 (Tensor) \u2013 the tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"
    },
    {
        "X": "What value is multiplied by the result of the element-wise multiplication of tensor1 by tensor2?",
        "Y": "scalar",
        "Z": "Performs the element-wise multiplication of tensor1\nby tensor2, multiply the result by the scalar value\nand add it to input. The shapes of tensor, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the tensor to be multiplied tensor2 (Tensor) \u2013 the tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"
    },
    {
        "X": "What is the tensor to be added to input?",
        "Y": "tensor1",
        "Z": "Performs the element-wise multiplication of tensor1\nby tensor2, multiply the result by the scalar value\nand add it to input. The shapes of tensor, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the tensor to be multiplied tensor2 (Tensor) \u2013 the tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"
    },
    {
        "X": "What is the same as this function?",
        "Y": "torch.maximum",
        "Z": "This is like torch.maximum() except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "If both elements are what is NaN propagated?",
        "Y": "NaN",
        "Z": "Computes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "What language is std::fmax a wrapper around?",
        "Y": "C++",
        "Z": "This is like torch.maximum() except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "Supports broadcasting to a common shape, type promotion, and what other inputs?",
        "Y": "integer and floating-point inputs",
        "Z": "This is like torch.maximum() except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "What is the same as the input-wise maximum of input and other?",
        "Y": "torch.maximum()",
        "Z": "Computes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "What C++ function is this function a wrapper around?",
        "Y": "std::fmax",
        "Z": "Computes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "What type promotion does std::fmax support?",
        "Y": "broadcasting",
        "Z": "This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "What does this function support to a common shape, type promotion, and integer and floating-point inputs?",
        "Y": "broadcasting",
        "Z": "Computes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "What is another (Tensor)?",
        "Y": "second input tensor",
        "Z": "This is like torch.maximum() except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "What program's fmax function is similar to std::fmax?",
        "Y": "NumPy",
        "Z": "This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "What program is the numpy.array_split() based on?",
        "Y": "NumPy",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is the tensor to split indices_or_sections?",
        "Y": "n",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What are the sub-tensors all of which split a tensor?",
        "Y": "views of input",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "Input (Tensor) \u2013 the tensor to split what?",
        "Y": "indices_or_sections",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is used to split indices or sections?",
        "Y": "tensor",
        "Z": "input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What does input split?",
        "Y": "indices_or_sections",
        "Z": "input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "If indices_or_sections is an integer n or a zero dimensional long tensor with value n, input",
        "Y": "n sections",
        "Z": "indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "If input is divisible by n along dimension dim, each section will be of equal size, what is the result?",
        "Y": "input.size(dim) / n",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is an integer n or a zero dimensional long tensor with value n?",
        "Y": "indices_or_sections",
        "Z": "indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "If indices_or_sections is divisible by n along dimension dim, each section will be of equal size?",
        "Y": "n",
        "Z": "indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is input split into if indices_or_sections is an integer n or a zero dimensional long tensor",
        "Y": "n sections",
        "Z": "indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "How many dices_or_sections is an integer n or a zero dimensional long tensor with value n?",
        "Y": "n",
        "Z": "If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "If i is divisible by n along dimension dim, each section will be of equal size, input.size(dim) /",
        "Y": "n",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "If indices_or_sections is a one-dimensional long tensor, what ensional long tensor",
        "Y": "dim",
        "Z": "If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What would indices_or_sections=[2, 3] and dim=0 result in?",
        "Y": "tensors",
        "Z": "If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "If indices_or_sections is a tensor, what must it be on the CPU?",
        "Y": "zero-dimensional or one-dimensional long tensor",
        "Z": "If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is the dimension along which to split the tensor?",
        "Y": "dim",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is the default value of a tensor?",
        "Y": "0",
        "Z": "If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is a list or tuple of ints, or a one-dimensional long tensor?",
        "Y": "indices_or_sections",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What would result in the tensors input[:2], input[2:3], and input[3:]?",
        "Y": "indices_or_sections",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "If indices_or_sections is what, it must be a zero-dimensional or one-dimensional long tensor on the",
        "Y": "a tensor",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is the default value for indices_or_sections?",
        "Y": "0",
        "Z": "If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What does torch.rand_like(input) return?",
        "Y": "a tensor",
        "Z": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).\ntorch.rand_like(input) is equivalent to\ntorch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"
    },
    {
        "X": "What is equivalent to torch.rand(input.size(), dtype=input.dtype, layout=input.lay",
        "Y": "torch.rand_like",
        "Z": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).\ntorch.rand_like(input) is equivalent to\ntorch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"
    },
    {
        "X": "dtype (torch.dtype, optional) \u2013 what is returned Tensor?",
        "Y": "the desired data type",
        "Z": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).\ntorch.rand_like(input) is equivalent to\ntorch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"
    },
    {
        "X": "If None, what does torch.rand_like(input.size()) do?",
        "Y": "defaults to the dtype of input",
        "Z": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).\ntorch.rand_like(input) is equivalent to\ntorch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"
    },
    {
        "X": "What is the size of input that determines size of output tensor?",
        "Y": "input",
        "Z": "input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"
    },
    {
        "X": "If unbiased is what, Bessel's correction will be used?",
        "Y": "True",
        "Z": "Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is calculated if Bessel's correction is True?",
        "Y": "the sample deviation",
        "Z": "dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "Out (Tensor, optional) - what is used to calculate the standard deviation of all elements in the input tensor?",
        "Y": "output tensor",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "What is the function that determines the standard deviation of all elements in the input tensor?",
        "Y": "Calculates the standard deviation of all elements in the input tensor",
        "Z": "dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "If unbiased is True, Bessel's correction will be used.",
        "Y": "If unbiased is True",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "What is calculated if unbiased is True?",
        "Y": "the sample deviation",
        "Z": "dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "Int or tuple of python:ints) \u2013 the dimension or dimensions to reduce.",
        "Y": "dim",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "Whether to use Bessel's correction (N=1delta N = 1N=1)?",
        "Y": "unbiased",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "If Bessel's correction is used, what will be used?",
        "Y": "If unbiased is True",
        "Z": "input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction.",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "What is unbiased?",
        "Y": "whether to use Bessel\u2019s correction",
        "Z": "dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "What happens if unbiased is True?",
        "Y": "the sample deviation is calculated, without any correction",
        "Z": "unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "What is an example of a Bessel's correction?",
        "Y": "Example",
        "Z": "unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "Returns what with the cosine of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the cosine  of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos"
    },
    {
        "X": "What is in beta and subject to change?",
        "Y": "Warning Quantization",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization is in what state and subject to change?",
        "Y": "beta",
        "Z": "Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What provides a way to represent quantized tensors and perform operations with them?",
        "Y": "PyTorch",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can quantized tensors be used for?",
        "Y": "directly construct models that perform all or part of the computation in lower precision",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is provided that incorporate typical workflows of converting FP32 model to lower precision with minimal accuracy loss?",
        "Y": "Higher-level APIs",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch support backends for?",
        "Y": "running quantized operators efficiently",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can PyTorch be used to do?",
        "Y": "directly construct models",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are provided that incorporate typical workflows of converting FP32 model to lower precision with minimal accuracy loss?",
        "Y": "Higher-level APIs",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch support for running quantized operators efficiently?",
        "Y": "backends",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What kind of CPUs are typically found in mobile/embedded devices?",
        "Y": "ARM CPUs",
        "Z": "Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What backends does PyTorch support to run quantized operators efficiently?",
        "Y": "x86 CPUs with AVX2 support or higher",
        "Z": "Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the backend that PyTorch supports?",
        "Y": "Note",
        "Z": "Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What CPU is typically found in mobile/embedded devices?",
        "Y": "ARM",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What CPUs have AVX2 support or higher?",
        "Y": "x86",
        "Z": "x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the difference between ARM and x86 CPUs?",
        "Y": "Note",
        "Z": "Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What CPUs are typically found in mobile/embedded devices?",
        "Y": "ARM CPUs",
        "Z": "x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What CPUs have inefficient implementations without AVX2?",
        "Y": "x86 CPUs with AVX2 support or higher",
        "Z": "x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "PyTorch doesn't provide quantized operator implementations on what?",
        "Y": "CUDA",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Why do you move the model to CPU?",
        "Y": "to test the quantized functionality",
        "Z": "x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch not provide on CUDA?",
        "Y": "quantized operator implementations",
        "Z": "At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Move the model to what in order to test the quantized functionality?",
        "Y": "CPU",
        "Z": "ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is typically found in mobile/embedded devices?",
        "Y": "ARM CPUs",
        "Z": "ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a way to test the quantized functionality?",
        "Y": "Move the model to CPU",
        "Z": "ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the Quantization-aware training that supports both CPU and CUDA?",
        "Y": "FakeQuantize",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does FakeQuantize do to support both CPU and CUDA?",
        "Y": "Note",
        "Z": "Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch not provide?",
        "Y": "quantized operator implementations on CUDA",
        "Z": "ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the corresponding implementation chosen automatically based on?",
        "Y": "PyTorch build mode",
        "Z": "The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Move the model to what to test the quantized functionality?",
        "Y": "CPU",
        "Z": "The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Which program doesn't provide quantized operator implementations on CUDA?",
        "Y": "PyTorch",
        "Z": "Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch do to test the quantized functionality?",
        "Y": "Move the model to CPU in order to test the quantized functionality",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where should the model be moved to to test the quantized functionality?",
        "Y": "CPU",
        "Z": "At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What company doesn't provide quantized operator implementations on CUDA?",
        "Y": "PyTorch",
        "Z": "At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where should the model be moved in order to test quantized functionality?",
        "Y": "CPU",
        "Z": "At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of Quantization-aware training?",
        "Y": "FakeQuantize",
        "Z": "Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does FakeQuantize support both CPU and CUDA?",
        "Y": "Note",
        "Z": "At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What program supports both CPU and CUDA?",
        "Y": "FakeQuantize",
        "Z": "Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is necessary to ensure when preparing a quantized model?",
        "Y": "qconfig and the engine used for quantized computations match the backend on which the model will be executed",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization currently supports two backends: what?",
        "Y": "fbgemm",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What backend can be used on the ARM QNNPACK library?",
        "Y": "qnnpack",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is it recommended to set if you are interested in quantizing a model to run on ARM?",
        "Y": "qconfig",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What must match the backend on which a quantized model will be executed?",
        "Y": "qconfig and the engine",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What platform does fbgemm work on?",
        "Y": "x86",
        "Z": "Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization currently supports two backends: fbgemm and what?",
        "Y": "qnnpack",
        "Z": "When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the backend that quantizes a model to run on ARM?",
        "Y": "qconfig",
        "Z": "When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "On what platform is fbgemm currently supported?",
        "Y": "x86",
        "Z": "The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for?",
        "Y": "quantization aware training",
        "Z": "qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What should the torch.backends.quantized.engine parameter match?",
        "Y": "the backend",
        "Z": "qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the backend set to for inference?",
        "Y": "qnnpack",
        "Z": "In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the backend set to qnnpack for inference?",
        "Y": "torch.backends.quantized.engine = 'qnnpack'",
        "Z": "qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') used for?",
        "Y": "quantization aware training",
        "Z": "for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What parameter should be set to match the backend?",
        "Y": "torch.backends.quantized.engine",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "PyTorch provides Eager Mode Quantization and what other mode of quantization?",
        "Y": "FX Graph Mode Quantization",
        "Z": "qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What should the torch.backends.quantized.engine parameter be set to?",
        "Y": "match the backend",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What provides two different modes of quantization?",
        "Y": "PyTorch",
        "Z": "When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is qconfig used for?",
        "Y": "quantization aware training",
        "Z": "qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the use of qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')",
        "Y": "quantization aware training",
        "Z": "qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the two modes of quantization provided by PyTorch?",
        "Y": "Eager Mode Quantization and FX Graph Mode Quantization",
        "Z": "When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of training does PyTorch provide?",
        "Y": "quantization aware training",
        "Z": "for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "PyTorch provides Eager Mode Quantization and what other mode?",
        "Y": "FX Graph Mode Quantization",
        "Z": "for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is PyTorch used for?",
        "Y": "quantization aware training",
        "Z": "for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the beta version of Eager Mode Quantization?",
        "Y": "beta",
        "Z": "When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does Eager Mode Quantization only support?",
        "Y": "modules and not functionals",
        "Z": "torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Along with Eager Mode Quantization, what other mode does PyTorch provide?",
        "Y": "FX Graph Mode Quantization",
        "Z": "torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What mode of quantization does PyTorch provide?",
        "Y": "Eager Mode Quantization",
        "Z": "torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What provides Eager Mode Quantization and FX Graph Mode Quantization?",
        "Y": "PyTorch",
        "Z": "PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the alternative to FX Graph Mode Quantization?",
        "Y": "eager mode quantization",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the first mode of quantization that new users are encouraged to try?",
        "Y": "FX Graph Mode Quantization",
        "Z": "New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of FX Graph Mode Quantization?",
        "Y": "Eager Mode Quantization",
        "Z": "New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the three types of Quantization modes?",
        "Y": "Static, Dynamic, Weight Only",
        "Z": "The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the beta prototype of Eager Mode Quantization FX Graph Mode?",
        "Y": "Quantization Release Status",
        "Z": "Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the beta prototype Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules Supported Supported",
        "Y": "FX Graph Mode Quantization",
        "Z": "FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What Quantization Release Status beta prototype Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Support for Customization Limited Support Fully Supported Quant",
        "Y": "FX Graph Mode",
        "Z": "FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the status of the Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules Supported Supported Quantizing",
        "Y": "FX Graph Mode Quantization Release Status beta prototype",
        "Z": "FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the status of the beta prototype Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules Supported Quantizing Functional",
        "Y": "Release Status",
        "Z": "Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of Post Training Quantization is supported?",
        "Y": "Static",
        "Z": "Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the status of the Operator Fusion Manual?",
        "Y": "beta prototype",
        "Z": "beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Supported Quant",
        "Y": "Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules",
        "Z": "Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of modules are Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Support",
        "Y": "Manual Automatic Quantizing Modules",
        "Z": "Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Supported Supported Supported Quantizing Functionals/Torch Ops Manual?",
        "Y": "Automatic Quantizing Modules",
        "Z": "Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of support does the Quant/DeQuant Placement Manual Automatic Support for?",
        "Y": "Customization",
        "Z": "Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of Quantization Mode Support Post Training Quantization?",
        "Y": "Dynamic",
        "Z": "Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Supported Quantization",
        "Y": "Automatic Quantizing Modules",
        "Z": "Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What supports Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Supported Quant",
        "Y": "Manual Automatic Quantizing Modules",
        "Z": "Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the type of Quantiztion Aware Training?",
        "Y": "Static Input/Output Model Type torch",
        "Z": "Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Supported Supported Quantizing Functionals/Torch Ops Manual?",
        "Y": "Automatic Quantizing Modules",
        "Z": "Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post",
        "Y": "Quantizing Modules",
        "Z": "Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization Mode Support Post Training Quantization: Dynamic, Dynamic, Weight Only Quantiztion Aware Training: Static Post Training Quantization",
        "Y": "Static",
        "Z": "Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantizing Functionals/Torch Ops Manual Automatic Support for what?",
        "Y": "Customization",
        "Z": "Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of support is limited to Fully Supported Quantization Mode Support?",
        "Y": "Manual Automatic Support for Customization",
        "Z": "Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support?",
        "Y": "Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support",
        "Z": "Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How many types of quantization are supported in Eager Mode Quantization?",
        "Y": "three types",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What mode supports three types of quantization?",
        "Y": "Eager Mode Quantization",
        "Z": "beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of Quantization is supported in Eager Mode Quantization?",
        "Y": "Dynamic",
        "Z": "Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of Quantiztion Aware Training is Static Post Training Quantization?",
        "Y": "Dynamic",
        "Z": "Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of quantization is supported in Eager Mode Quantization?",
        "Y": "dynamic quantization",
        "Z": "There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is required post training in static quantization?",
        "Y": "calibration",
        "Z": "FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of quantization is supported with activations read/stored in floating point and quantized for compute?",
        "Y": "dynamic quantization",
        "Z": "torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the type of quantization supported in Eager Mode Quantization?",
        "Y": "quantization aware training",
        "Z": "Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is required post training?",
        "Y": "calibration",
        "Z": "Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what format are activations read/stored?",
        "Y": "floating point",
        "Z": "There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where are activations read/stored in Eager Mode Quantization?",
        "Y": "floating point",
        "Z": "torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of training does Eager Mode Quantization support?",
        "Y": "quantization aware training",
        "Z": "torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of training is supported in Eager Mode Quantization?",
        "Y": "quantization aware training",
        "Z": "Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a type of quantization that is quantized with activations read/stored in floating point and quantized for compute?",
        "Y": "dynamic quantization",
        "Z": "dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is another type of quantization?",
        "Y": "static quantization",
        "Z": "dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What blog post provides a more comprehensive overview of the tradeoffs between static quantization and quantization aware training?",
        "Y": "Pytorch",
        "Z": "static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the term for weights quantized, activations quantized, calibration required post training?",
        "Y": "static quantization",
        "Z": "static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where can you find a more comprehensive overview of the tradeoffs between quantization types?",
        "Y": "Pytorch",
        "Z": "quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What blog post provides a more comprehensive overview of the tradeoffs between quantization types?",
        "Y": "Pytorch",
        "Z": "Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a more comprehensive overview of the tradeoffs between quantization types?",
        "Y": "Introduction to Quantization",
        "Z": "Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the blog post that provides a more comprehensive overview of the tradeoffs between quantization types?",
        "Y": "Pytorch",
        "Z": "Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the quantization type?",
        "Y": "Pytorch",
        "Z": "Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What happens to the activations during inference?",
        "Y": "the activations are dynamically quantized during inference",
        "Z": "This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What dominates the model execution time?",
        "Y": "loading weights from memory",
        "Z": "This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What types of models can use dynamic quantization?",
        "Y": "LSTM and Transformer type models with small batch size",
        "Z": "This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of a diagram used for dynamic quantization?",
        "Y": "API",
        "Z": "This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "When are the activations dynamically quantized?",
        "Y": "inference",
        "Z": "This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used when the model execution time is dominated by?",
        "Y": "loading weights from memory",
        "Z": "There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of model has a small batch size?",
        "Y": "Transformer",
        "Z": "This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the term for quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference?",
        "Y": "dynamic quantization",
        "Z": "This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can you learn more about in our dynamic quantization tutorial?",
        "Y": "dynamic quantization",
        "Z": "quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the term for dynamic quantization?",
        "Y": "dynamic quantization",
        "Z": "To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the API that you can use to learn more about?",
        "Y": "dynamic quantization",
        "Z": "API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can you learn more about in the static quantization tutorial?",
        "Y": "static quantization",
        "Z": "Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the technique used in the API?",
        "Y": "static quantization",
        "Z": "Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the static quantization tutorial cover?",
        "Y": "static quantization",
        "Z": "To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the tutorial that teaches quantization aware training?",
        "Y": "QAT",
        "Z": "Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of a diagram?",
        "Y": "API",
        "Z": "Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "To learn more about quantization aware training, please see what tutorial?",
        "Y": "QAT",
        "Z": "Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of quantization is applied after training, quantization parameters are calculated based on sample calibration data?",
        "Y": "Post Training Quantization",
        "Z": "Quantization types supported by FX Graph Mode can be classified in two ways: Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of quantization is supported by FX Graph Mode?",
        "Y": "Post Training Quantization",
        "Z": "Quantization types supported by FX Graph Mode can be classified in two ways: Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of training simulates quantization during training?",
        "Y": "Quantization Aware Training",
        "Z": "Quantization types supported by FX Graph Mode can be classified in two ways: Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of Quantization is included in Post Training Quantization?",
        "Y": "Weight Only Quantization",
        "Z": "Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the type of Quantization that only weight is statically quantized?",
        "Y": "Weight Only Quantization",
        "Z": "Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the type of quantization in which weight is statically quantized and activation is dynamically quantized?",
        "Y": "Dynamic Quantization",
        "Z": "Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of Quantization includes both weight and activations?",
        "Y": "Static Quantization",
        "Z": "Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the type of Quantization where only weight is statically quantized?",
        "Y": "Dynamic Quantization",
        "Z": "Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How many different types of quantization can we theoretically have?",
        "Y": "6",
        "Z": "And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is one of the supported quantization types?",
        "Y": "FX Graph Mode Quantization",
        "Z": "And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the supported quantization types in FX Graph Mode Quantization?",
        "Y": "Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization",
        "Z": "Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are both weight and activations in Static Quantization?",
        "Y": "statically quantized",
        "Z": "Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Are the two ways of classification independent or independent?",
        "Y": "independent",
        "Z": "These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How is the configuration done in post training quantization?",
        "Y": "qconfig_dict",
        "Z": "The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of quantization is supported by qconfig_dict?",
        "Y": "post training quantization",
        "Z": "The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of a quantization type in FX Graph Mode Quantization?",
        "Y": "API",
        "Z": "Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of quantization is supported by FX Graph Mode Quantization?",
        "Y": "post training quantization",
        "Z": "The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an argument of the prepare_fx function?",
        "Y": "qconfig_dict",
        "Z": "And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the tutorial that provides more information about FX Graph Mode Quantization?",
        "Y": "User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization",
        "Z": "And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the most common type of post training quantization?",
        "Y": "weight only",
        "Z": "Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What function does qconfig_dict belong to?",
        "Y": "prepare_fx",
        "Z": "Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the document that provides more information about FX Graph Mode Quantization?",
        "Y": "User Guide on Using FX Graph Mode Quantization",
        "Z": "Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the FX Graph Mode Post Training Static Quantization?",
        "Y": "User Guide on Using FX Graph Mode Quantization",
        "Z": "Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization FX Graph Mode Post Training Dynamic Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the document that describes FX Graph Mode Post Training Static Quantization?",
        "Y": "User Guide on Using FX Graph Mode Quantization",
        "Z": "Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization FX Graph Mode Post Training Dynamic Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What program supports both per tensor and per channel asymmetric linear quantization?",
        "Y": "PyTorch",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What means that all values within a tensor are scaled the same way?",
        "Y": "Per tensor",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What happens when the values in a tensor are scaled and offset by a different value?",
        "Y": "the scale and offset become vectors",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the benefit of using per channel quantization?",
        "Y": "lesser error",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the mapping performed by?",
        "Y": "converting the floating point tensors",
        "Z": "The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What operations do not cause additional quantization error?",
        "Y": "padding",
        "Z": "Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is converted by mapping?",
        "Y": "floating point tensors",
        "Z": "The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Operations like what do not cause additional quantization error?",
        "Y": "padding",
        "Z": "Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is represented with no error after quantization?",
        "Y": "zero",
        "Z": "Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what program do we need to be able to represent quantized data in Tensors?",
        "Y": "PyTorch",
        "Z": "The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Along with quantization parameters, what parameters can be stored in a Quantized Tensor?",
        "Y": "scale and zero_point",
        "Z": "In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does a Quantized Tensor allow for?",
        "Y": "serialization of data in a quantized format",
        "Z": "In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In order to do quantization in PyTorch, we need to be able to represent quantized data in what?",
        "Y": "Tensors",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What quantization parameters are stored in a Quantized Tensor?",
        "Y": "scale and zero_point",
        "Z": "The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantized Tensors allow for what type of data in a quantized format?",
        "Y": "serialization",
        "Z": "In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What supports a limited subset of data manipulation methods of the regular full-precision tensor?",
        "Y": "Quantized Tensors",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do Quantized Tensors restrict support to?",
        "Y": "8 bit weights",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What support a limited subset of data manipulation methods of the regular full-precision tensor?",
        "Y": "Quantized Tensors",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "For NN operators included in PyTorch, we restrict support to what?",
        "Y": "8 bit weights",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does data_type = qint8 support?",
        "Y": "8 bit weights",
        "Z": "8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is represented with no quantization error?",
        "Y": "zero",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How can additional data types and quantization schemes be implemented?",
        "Y": "custom operator mechanism",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Operator implementations currently only support what for weights of the conv and linear operators?",
        "Y": "per channel quantization",
        "Z": "Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of activations do operators currently only support per channel quantization for weights of the conv and linear operators?",
        "Y": "8 bit activations",
        "Z": "8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do operator implementations currently only support for weights of the conv and linear operators?",
        "Y": "per channel quantization",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Many operations for quantized tensors are available under the same API as what?",
        "Y": "full float version",
        "Z": "Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are quantized versions of that perform re-quantization?",
        "Y": "NN modules",
        "Z": "Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do quantized versions of NN modules explicitly take in the operation signature?",
        "Y": "output quantization parameters",
        "Z": "Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what language are quantized tensor operations available?",
        "Y": "torch",
        "Z": "Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where are quantized versions of NN modules that perform re-quantization available?",
        "Y": "torch.nn.quantized",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the output quantization parameters?",
        "Y": "scale and zero_point",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What performs re-quantization?",
        "Y": "NN modules",
        "Z": "Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do quantized NN modules explicitly take in the operation signature?",
        "Y": "output quantization parameters",
        "Z": "Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does torch.nn.intrinsic.quantized support?",
        "Y": "fused versions",
        "Z": "Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where are operations for quantized tensors available under the same API as full float version?",
        "Y": "torch.nn",
        "Z": "Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the fusion pattern that affects quantization?",
        "Y": "torch.nn.intrinsic.quantized",
        "Z": "Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are fused versions corresponding to?",
        "Y": "common fusion patterns",
        "Z": "In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "The list of supported operations is sufficient to cover what types of models?",
        "Y": "CNN and RNN models",
        "Z": "In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a common fusion pattern that impacts quantization?",
        "Y": "torch.nn.intrinsic.quantized",
        "Z": "In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where do we support modules prepared for quantization aware training?",
        "Y": "torch.nn.qat and torch.nn.intrinsic.qat",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the default implementations of to select the scale factor and bias based on observed tensor data?",
        "Y": "observers",
        "Z": "While default implementations of observers to select the scale factor and bias\nbased on observed tensor data are provided, developers can provide their own\nquantization functions. Quantization can be applied selectively to different\nparts of the model or configured differently for different parts of the model. We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How can quantization be applied to different parts of the model?",
        "Y": "selectively",
        "Z": "While default implementations of observers to select the scale factor and bias\nbased on observed tensor data are provided, developers can provide their own\nquantization functions. Quantization can be applied selectively to different\nparts of the model or configured differently for different parts of the model. We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do conv2d(), conv3d() and linear() support?",
        "Y": "per channel quantization",
        "Z": "While default implementations of observers to select the scale factor and bias\nbased on observed tensor data are provided, developers can provide their own\nquantization functions. Quantization can be applied selectively to different\nparts of the model or configured differently for different parts of the model. We also provide support for per channel quantization for conv2d(),\nconv3d() and linear()",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can developers provide?",
        "Y": "quantization functions",
        "Z": "While default implementations of observers to select the scale factor and bias\nbased on observed tensor data are provided, developers can provide their own\nquantization functions. Quantization can be applied selectively to different\nparts of the model or configured differently for different parts of the model. We also provide support for per channel quantization for conv2d(),\nconv3d() and linear()",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can be applied selectively to different parts of the model or configured differently for different parts of the model?",
        "Y": "Quantization",
        "Z": "While default implementations of observers to select the scale factor and bias\nbased on observed tensor data are provided, developers can provide their own\nquantization functions. Quantization can be applied selectively to different\nparts of the model or configured differently for different parts of the model. We also provide support for per channel quantization for conv2d(),\nconv3d() and linear()",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is supported for conv2d(), conv3d() and linear()?",
        "Y": "per channel quantization",
        "Z": "While default implementations of observers to select the scale factor and bias\nbased on observed tensor data are provided, developers can provide their own\nquantization functions. Quantization can be applied selectively to different\nparts of the model or configured differently for different parts of the model. We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do we provide support for for conv2d(), conv3d(), and linear()?",
        "Y": "per channel quantization",
        "Z": "We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization workflows work by adding or what?",
        "Y": "replacing",
        "Z": "We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is nn.Conv2d converted to?",
        "Y": "nn.quantized.Conv2d",
        "Z": "We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "The model stays a regular what-based instance throughout the process?",
        "Y": "nn.Module",
        "Z": "We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How do quantization workflows work?",
        "Y": "adding",
        "Z": "We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is added as a.observer submodule?",
        "Y": "observers",
        "Z": "We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does nn.Conv2d convert to?",
        "Y": "nn.quantized.Conv2d",
        "Z": "We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of instance does the model stay in throughout the process?",
        "Y": "nn.Module",
        "Z": "We also provide support for per channel quantization for conv2d(),\nconv3d() and linear() Quantization workflows work by adding (e.g. adding observers as\n.observer submodule) or replacing (e.g. converting nn.Conv2d to\nnn.quantized.Conv2d) submodules in the model\u2019s module hierarchy. It\nmeans that the model stays a regular nn.Module-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is it necessary to make some modifications to prior to quantization?",
        "Y": "model definition",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu).",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Why is it necessary to make some modifications to the model definition prior to quantization?",
        "Y": "currently quantization works on a module by module basis",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the user need to do to convert operations that require output requantization to module form?",
        "Y": "output requantization",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu).",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is required to the model definition prior to quantization?",
        "Y": "make some modifications",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How does quantization work?",
        "Y": "module by module",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu).",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is another name for torch.nn.functional.relu?",
        "Y": "torch.nn.ReLU",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do some operations require to be converted from functionals to module form?",
        "Y": "output requantization",
        "Z": "Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu).",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "To what form can any operations that require output requantization be converted?",
        "Y": "module form",
        "Z": "Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu).",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of a conversion from functionals to module form?",
        "Y": "torch.nn.ReLU",
        "Z": "Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu).",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Convert operations that require output requantization from functionals to what?",
        "Y": "module form",
        "Z": "Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu).",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is done by assigning.qconfig attributes on submodules or by specifying qconfig_dict?",
        "Y": "Specify which parts of the model need to be quantized",
        "Z": "Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What means that the model.conv layer will not be quantized?",
        "Y": "setting model.conv1.qconfig",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can be done by assigning.qconfig attributes on submodules or by specifying qconfig_dict?",
        "Y": "Specify which parts of the model need to be quantized",
        "Z": "Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What layer will not be quantized if setting model.conv1.qconfig = None?",
        "Y": "the model.conv layer",
        "Z": "Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What will the quantization settings for model.linear1 be using instead of the global qconfig?",
        "Y": "custom_qconfig",
        "Z": "Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the user need to do in addition to Specify where activations are quantized and de-quantized?",
        "Y": "static quantization techniques",
        "Z": "For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used to specify where activations are quantized and de-quantized?",
        "Y": "QuantStub and DeQuantStub modules",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used to wrap tensor operations that require special handling for quantization into modules?",
        "Y": "torch.nn.quantized.FloatFunctional",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What tensor operations require special handling to determine output quantization parameters?",
        "Y": "add and cat",
        "Z": "Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the user need to do in addition to quantizing activations?",
        "Y": "Specify where activations are quantized and de-quantized",
        "Z": "For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are used to specify where activations are quantized and de-quantized?",
        "Y": "QuantStub and DeQuantStub modules",
        "Z": "Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What operations require special handling to determine output quantization parameters?",
        "Y": "add and cat",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can be quantized and de-quantized using QuantStub and DeQuantStub modules?",
        "Y": "activations",
        "Z": "Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do QuantStub and DeQuantStub modules specify?",
        "Y": "where activations are quantized and de-quantized",
        "Z": "Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are two examples of tensor operations that require special handling to determine output quantization parameters?",
        "Y": "add and cat",
        "Z": "Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are two examples of operations that require special handling to determine output quantization parameters?",
        "Y": "add and cat",
        "Z": "Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is fusion?",
        "Y": "combine operations/modules into a single module to obtain higher accuracy and performance",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used to combine operations/modules into a single module?",
        "Y": "torch.quantization.fuse_modules() API",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What fusions do we currently support?",
        "Y": "Linear, Relu",
        "Z": "For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the torch.quantization.fuse_modules() API do?",
        "Y": "Fuse modules",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What API is used to fuse modules?",
        "Y": "torch.quantization.fuse_modules()",
        "Z": "Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Set the reduce_range argument on observers to what if you are using the fbgemm backend?",
        "Y": "True",
        "Z": "Set the reduce_range argument on observers to True if you are using the\nfbgemm backend.  This argument prevents overflow on some int8 instructions\nby reducing the range of quantized data type by 1 bit.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "By how much is the range of quantized data type reduced?",
        "Y": "1 bit",
        "Z": "Set the reduce_range argument on observers to True if you are using the\nfbgemm backend.  This argument prevents overflow on some int8 instructions\nby reducing the range of quantized data type by 1 bit.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "The reduce_range argument prevents what on some int8 instructions?",
        "Y": "overflow",
        "Z": "Set the reduce_range argument on observers to True if you are using the\nfbgemm backend.  This argument prevents overflow on some int8 instructions\nby reducing the range of quantized data type by 1 bit.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do you want to pass a non-quantized Tensor to?",
        "Y": "a quantized kernel",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a common workaround for a non-quantized tensor?",
        "Y": "torch.quantization.QuantStub",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How is torch.quantization.QuantStub done?",
        "Y": "manually",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of a non-quantized tensor?",
        "Y": "e2e",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "If you see an error similar to: This means that you are trying to pass what to a quantized kernel?",
        "Y": "a non-quantized Tensor",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used to quantize the tensor?",
        "Y": "torch.quantization.QuantStub",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what mode is torch.quantization.QuantStub used?",
        "Y": "Eager mode quantization",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of a non-quantized tensor to a quantized kernel?",
        "Y": "e2e",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a non-quantized Tensor trying to pass to?",
        "Y": "a quantized kernel",
        "Z": "This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a common workaround for passing a non-quantized Tensor to a quantized kernel?",
        "Y": "torch.quantization.QuantStub",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of an e2e error?",
        "Y": "an error similar to:",
        "Z": "This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are you trying to pass to a quantized kernel?",
        "Y": "a non-quantized Tensor",
        "Z": "This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a common workaround to pass a non-quantized tensor to a quantized kernel?",
        "Y": "torch.quantization.QuantStub",
        "Z": "This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of a non-quantized tensor being passed to a quantized kernel?",
        "Y": "e2e",
        "Z": "This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the error indicate you are trying to pass a quantized Tensor to?",
        "Y": "a non-quantized kernel",
        "Z": "If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a common workaround to dequantize the tensor?",
        "Y": "torch.quantization.DeQuantStub",
        "Z": "If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How is torch.quantization.DeQuantStub done?",
        "Y": "manually",
        "Z": "If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of a quantized tensor?",
        "Y": "e2e",
        "Z": "If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "If you see an error similar to: This means that you are trying to pass a quantized Tensor to what?",
        "Y": "a non-quantized kernel",
        "Z": "If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used to dequantize the tensor?",
        "Y": "torch.quantization.DeQuantStub",
        "Z": "If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the error indicate you are trying to pass a non-quantized Tensor to?",
        "Y": "a quantized kernel",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How is torch.quantization.QuantStub used?",
        "Y": "manually",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What model does torch.quantization convert to quantized form?",
        "Y": "FP32",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What module implements the functions you call to convert your model from FP32 to quantized form?",
        "Y": "torch.nn.intrinsic",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What converts the weights to int8 and replaces the operations with their quantized counterparts?",
        "Y": "convert()",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What critical fusion is performed by torch.quantization?",
        "Y": "conv+relu",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What model does torch.nn.intrinsic convert to quantized form?",
        "Y": "FP32",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a helper function for performing critical fusions?",
        "Y": "torch.nn.intrinsic",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What format does the module convert your model from?",
        "Y": "FP32",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What converts the weights to int8?",
        "Y": "convert()",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the helper function used to convert your model from FP32 to quantized form?",
        "Y": "torch.nn.intrinsic",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the combined modules of torch.nn.intrinsic?",
        "Y": "conv + relu",
        "Z": "torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What module implements the quantized implementations of fused operations?",
        "Y": "torch.nn.qat",
        "Z": "This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the fused modules implemented by this module?",
        "Y": "conv + relu",
        "Z": "This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What module implements the quantized implementations of fused operations like conv + relu?",
        "Y": "torch.nn.intrinsic.quantized",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are fused operations needed for?",
        "Y": "quantization aware training",
        "Z": "This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are some of the fused operations implemented by torch.nn.intrinsic.quantized?",
        "Y": "conv + relu",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    }
]