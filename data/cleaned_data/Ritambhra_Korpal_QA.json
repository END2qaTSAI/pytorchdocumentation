[
    {
        "X": "Returns a new tensor containing imaginary values of the self tensor. What Returns a new tensor",
        "Y": "Tensor.imag",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is added to the tensor?",
        "Y": "a scalar or tensor to self tensor",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for Tensor.argmax?",
        "Y": "Tensor.argmax",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is added to the self tensor?",
        "Y": "a scalar or tensor to self tensor",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the tensor that returns a scalar or tensor to self tensor?",
        "Y": "Tensor.addbmm",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Which tensor returns a new tensor containing imaginary values of the self tensor?",
        "Y": "Tensor",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the Tensor.add_ In-place version of?",
        "Y": "add() Tensor.addbmm",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a Tensor.add_ In-place version of?",
        "Y": "add() Tensor.addbmm",
        "Z": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]).",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What _ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolut",
        "Y": "Tensor.abs",
        "Z": "Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does add add?",
        "Y": "a scalar or tensor to self tensor",
        "Z": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is added to the Tensor.absolute Alias for abs() Tensor.absolute Alias for ab",
        "Y": "a scalar or tensor to self tensor",
        "Z": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.add_ In-place version of what add() Tensor.addbmm?",
        "Y": "add() Tensor.addbmm",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos",
        "Y": "Tensor.absolute",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of elements can be stored in a sparse array?",
        "Y": "non-zero",
        "Z": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers?",
        "Y": "memory",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How much memory saving does a 10 000 x 10 000 tensor with 100 000 non-zero floating point numbers have?",
        "Y": "200 fold",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the function that provides the size of a sparse COO tensor?",
        "Y": "torch.sparse_coo_tensor()",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the value of entry 4 in the sparse tensor?",
        "Y": "1, 0",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M stand for?",
        "Y": "s.sparse_dim()",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the location of the entry [5, 6] in a 2 + 1)-dimensional tensor?",
        "Y": "(1, 0",
        "Z": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of sparse COO tensor can be constructed by specifying its size only?",
        "Y": "empty",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where is the entry [5, 6] in a 2 + 1)-dimensional tensor?",
        "Y": "(1, 0",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Pytorch implements an extension of sparse tensors with scalar values to sparse tensor",
        "Y": "hybrid tensors",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the element type of the indices of specified elements?",
        "Y": "torch.int64",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the corresponding (tensor) values collected in?",
        "Y": "arbitrary integer or floating point number element type",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What sparse COO tensor format permits uncoalesced sparse tensors?",
        "Y": "PyTorch",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the tensor we want to create?",
        "Y": "(2 + 1)",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What format permits uncoalesced sparse tensors?",
        "Y": "PyTorch sparse COO tensor",
        "Z": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can lead to an 1-D uncoalesced tensor?",
        "Y": "multiple values",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the location of a 2 + 1-dimensional tensor?",
        "Y": "(1, 0",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Values are stored as what?",
        "Y": "strided tensors",
        "Z": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of uncoalesced tensor is created when multiple values are specified for the same index 1?",
        "Y": "1-D",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the lexicographical ordering of indices?",
        "Y": "lexicographical ordering of indices",
        "Z": "s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a sparse COO tensor a part of?",
        "Y": "torch.Tensor",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What instance of a sparse COO tensor is a sparse COO tensor?",
        "Y": "torch.Tensor",
        "Z": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of data can a sparse COO tensor acquire?",
        "Y": "COO format data",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the output of torch.Tensor.coalesce() method?",
        "Y": "a sparse tensor",
        "Z": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What of specified tensor elements are unique?",
        "Y": "indices",
        "Z": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What instance does a sparse COO tensor belong to?",
        "Y": "torch.Tensor",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The indices are sorted in what order?",
        "Y": "lexicographical order",
        "Z": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Why should sparse tensors be coalesced?",
        "Y": "to prevent them from growing too large",
        "Z": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How will most operations work with a coalesced or uncoalesced sparse tensor?",
        "Y": "most operations will work identically",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does PyTorch use for those array elements that are actually stored?",
        "Y": "specified elements",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can one acquire only when the tensor instance is coalesced?",
        "Y": "COO format data",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What could be implemented by multiplying all the uncoalesced values with the scalar?",
        "Y": "scalar multiplication",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of operation cannot be implemented by a square root?",
        "Y": "nonlinear operation",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The user must supply the row and column indices and values tensors separately or separately?",
        "Y": "separately",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is True if the Tensor uses sparse storage layout?",
        "Y": "Tensor.is_sparse",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Tensor.dense_dim Return the number of sparse dimensions in a sparse tensor self. Tens",
        "Y": "sparse",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned by the indices of the sparse tensor mask?",
        "Y": "a new sparse tensor with values from a strided tensor self",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced Returns what?",
        "Y": "True if self is a sparse COO tensor that is coalesced",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Tensor.is_sparse Is what?",
        "Y": "True if the Tensor uses sparse storage layout",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Return the number of sparse dimensions in a sparse tensor self?",
        "Y": "Tensor.sparse_dim",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Is True if self is a sparse COO tensor that is coalesced?",
        "Y": "True if self is a sparse COO tensor that is coalesced",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Tensor.sparse_mask Returns what with values from a strided tensor self filtered by the",
        "Y": "a new sparse tensor",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what does Tensor.is_coales",
        "Y": "True if self is a sparse COO tensor that is coalesced",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns the tensor containing the compressed row indices of the self tensor?",
        "Y": "Tensor.col_indices",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is strongly prefered as in a future pytorch release, this function will only return complex tensors?",
        "Y": "return_complex=True",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ignoring what, this method computes the following expression: where mmm is the index of the sliding window, and omega",
        "Y": "optional batch dimension",
        "Z": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "input must be either a what?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Window can be a what?",
        "Y": "1-D tensor",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is true, input will be padded on both sides to length n_fft before being applied?",
        "Y": "center is True",
        "Z": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "When was return_complex required to be given explicitly for real inputs?",
        "Y": "1.8.0",
        "Z": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the name of the function that will only return complex tensors?",
        "Y": "return_complex=True",
        "Z": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is None (default), it is treated as equal to n_fft. If center is True (default), input will",
        "Y": "n_fft",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is true, input will be padded on both sides so that the n_fft is padded before being applied?",
        "Y": "center is True",
        "Z": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "From what version did return_complex have to always be given explicitly for real inputs?",
        "Y": "1.8.0",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is true, input will be padded on both sides so that the t is the t?",
        "Y": "center is True",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is true, input will be padded on both sides so that the ttt-th frame is centered at time t",
        "Y": "center is True",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What begins at time thop_lengtht times texthop_length?",
        "Y": "ttt-th frame",
        "Z": "Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is None, window will be padded on both sides to length n_fft before being applied. If center is True",
        "Y": "n_fft",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "For all available options, see what for all available options. Default is \"reflect\"?",
        "Y": "torch.nn.functional.pad()",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is true (default for real input) only values for omega are used?",
        "Y": "onesided is True",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is None (default), window will be padded on both sides to length n_fft before being applied. If center",
        "Y": "n_fft",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting for the padding method used when center is True?",
        "Y": "\"reflect\"",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If onesided is what?",
        "Y": "True",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is None (default), it is treated as equal to n_fft?",
        "Y": "n_fft",
        "Z": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is true, input will be padded on both sides so that the ttt-th frame is centered?",
        "Y": "center is True",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting for input when center is True?",
        "Y": "\"reflect\"",
        "Z": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If True (default for real input), only values for omega are returned because the real-to-complex Fourier transform",
        "Y": "onesided",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is True (default for real input), only values for omega are returned because the real-to-complex Fourier",
        "Y": "onesided",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If the input or window tensors are complex, what is not possible?",
        "Y": "onesided output is not possible",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4))",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If normalized is True, the function returns the normalized STFT results, i.e., normalized STFT results, i.",
        "Y": "False",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If center is True, only values for omega are returned because the real-to-complex Fourier transform satisfie",
        "Y": "onesided",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4))",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If normalized is True, the function returns what?",
        "Y": "normalized STFT results",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If return_complex is True, the return is a input.dim() + what?",
        "Y": "1 dimensional complex tensor",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If return_complex is True, the output is what?",
        "Y": "input.dim() + 2 dimensional real tensor",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4))",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Returns either a complex tensor of size or what?",
        "Y": "a complex tensor of size",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4))",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the output if return_complex is True?",
        "Y": "input.dim() + 2 dimensional real tensor",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "n_fft (int) \u2013 the input tensor n_fft (int) \u2013 size",
        "Y": "Fourier transform",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for n_fft?",
        "Y": "None",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the output if return_complex is False?",
        "Y": "input.dim() + 2 dimensional real tensor",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for return half of results to avoid redundancy for real inputs?",
        "Y": "False onesided",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the window length of the Kaiser window?",
        "Y": "window_length",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What argument is intended as a useful shorthand to produce a periodic window as input to functions like torch.stft()?",
        "Y": "periodic",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the default layout of the returned window tensor?",
        "Y": "torch.strided",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Who selects a given stream?",
        "Y": "Context-manager",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns the currently selected Stream for a given device. Returns what for a given device?",
        "Y": "default Stream",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does comm.reduce_add Sums tensors from multiple GPUs?",
        "Y": "comm",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Gets the cuda capability of a device. Gets what?",
        "Y": "name of a device",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What Scatters tensors from multiple GPUs?",
        "Y": "comm.scatter",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns the index of a currently selected device. Returns what for a given device?",
        "Y": "currently selected Stream",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What Gathers tensors?",
        "Y": "comm.gather",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator?",
        "Y": "Wrapper around a CUDA stream",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "For details, see what for details. Computes the entropy on input (as defined below), elementwise?",
        "Y": "documentation of each function",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the entropy on input (as defined below), elementwise. Computes the entropy on input (a",
        "Y": "input (Tensor) \u2013 the input tensor",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the equivalent of Computes the exponential of the elements minus 1 of input?",
        "Y": "Computes the exponential of the elements minus 1 of input",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the natural logarithm of the absolute value of the gamma function on input?",
        "Y": "Computes the natural logarithm of the absolute value of the gamma function on input",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the natural logarithm of the absolute value of the gamma function on input. Example: Computes the inverse",
        "Y": "Computes",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the first kind (as defined below) for each element of input. input (Tensor) \u2013 the input ten",
        "Y": "exponentially scaled zeroth order modified Bessel function",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Out (Tensor) is what?",
        "Y": "optional",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does the output tensor do?",
        "Y": "Computes the error function of input",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the name of the function that computes the complementary error function of input?",
        "Y": "Computes the complementary error function of input",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Returns what with the logit of the elements of input?",
        "Y": "a new tensor",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects?",
        "Y": "Future type",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does torch._C.Future expose?",
        "Y": "APIs",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Warning GPU support is a what?",
        "Y": "beta",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the Future is already completed, the given callback will be run what way?",
        "Y": "inline",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "GPU support is a what?",
        "Y": "beta feature",
        "Z": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When calling wait()/value() on this Future, the exception set here will be raised what way?",
        "Y": "inline",
        "Z": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the reference to this Future?",
        "Y": "this Future",
        "Z": "is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does Future.done() do?",
        "Y": "record events on all the relevant current streams",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is used to enforce a certain order?",
        "Y": "chaining",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will be run immediately inline if this Future is already completed?",
        "Y": "the given callback",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does setting the result for this Future do?",
        "Y": "mark this Future as completed and trigger all attached callbacks",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If this Future is already completed, the given callback will be run immediately what?",
        "Y": "inline",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a Future not have to be done twice?",
        "Y": "a Future cannot be marked completed twice",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the Future's value contains tensors that reside on GPUs, the callback might be invoked when?",
        "Y": "while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What are the dedicated streams set to when the callback is invoked?",
        "Y": "current",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "The non-blocking behavior of what method is similar to the non-blocking behavior of?",
        "Y": "wait()",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the Future's value contains what that reside on GPUs, the callback might be invoked while the async kernels that are",
        "Y": "tensors",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "The non-blocking behavior of what is similar to the non-blocking behavior of?",
        "Y": "wait()",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens if the callback returns a value that contains tensors that reside on a GPU?",
        "Y": "if the callback returns a value that contains tensors that reside on a GPU",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a callback take as the only argument?",
        "Y": "Future",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a callback do to an already-completed future?",
        "Y": "Obtain the value of an already-completed future",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could happen if the callback is called after a call to wait() has completed?",
        "Y": "Future may not yet hold a value",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains what that reside on GPUs, this method will not perform any additional synchronization.",
        "Y": "tensors",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What method does not perform any additional synchronization if the value contains tensors that reside on GPUs?",
        "Y": "wait()",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does fut.wait() do?",
        "Y": "Obtain the value of an already-completed future",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, this method will not perform any additional synchronization. This should be done separately",
        "Y": "wait()",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What method will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the",
        "Y": "wait()",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, what is performed with the kernels?",
        "Y": "synchronization",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What method inserts the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the",
        "Y": "wait()",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is returned from  the inverse hyperbolic tangent of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the inverse hyperbolic tangent of the elements of input. Note The domain of the inverse hyperbolic tangent is(-1, 1)and values outside this range\nwill be mapped toNaN, except for the values1and-1for which the output is\nmapped to+/-INFrespectively. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh"
    },
    {
        "X": "What is the domain of the inverse hyperbolic tangent?",
        "Y": "-1, 1)",
        "Z": "Returns a new tensor with the inverse hyperbolic tangent of the elements of input. Note The domain of the inverse hyperbolic tangent is(-1, 1)and values outside this range\nwill be mapped toNaN, except for the values1and-1for which the output is\nmapped to+/-INFrespectively. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh"
    },
    {
        "X": "What is Roll the tensor along the given dimension?",
        "Y": "Elements that are shifted beyond the\nlast position are re-introduced at the first position.",
        "Z": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input(Tensor) \u2013 the input tensor. shifts(intortuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims(intortuple of python:ints) \u2013 Axis along which to roll Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "X": "What is computing error function?",
        "Y": "out(Tensor,optional) is the comuted error function of input",
        "Z": "out(Tensor,optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the sum of exponentiations of inputs called?",
        "Y": "Logarithm",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "Where is this function useful in statistics?",
        "Y": "the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers",
        "Z": "Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What of the calculated probability is stored in statistics where the probabilities of events may be so small as to exceed the range of normal floating point numbers?",
        "Y": "logarithm",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What does this function allow?",
        "Y": "adding probabilities stored in such a fashion",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What is the sum of exponentiations of the inputs?",
        "Y": "Logarithm",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What is calculated by the function (ex+ey)logleft(ex + eyright)log",
        "Y": "pointwiselog",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "Where is this function useful?",
        "Y": "statistics",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What is stored in statistics where the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers?",
        "Y": "the logarithm of the calculated probability",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "How is log(ex+ey)logleft(ex + eyright)log(ex+e",
        "Y": "pointwise",
        "Z": "Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What part of the calculated probability is stored?",
        "Y": "logarithm",
        "Z": "Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What performs a reduction on a single tensor?",
        "Y": "with torch.logsumexp()",
        "Z": "Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What does the function (ex+ey)logleft(ex + eyright)log(ex",
        "Y": "pointwiselog",
        "Z": "Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What is the name of the function that performs a reduction on a single tensor?",
        "Y": "with torch.logsumexp()",
        "Z": "Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What should this op be disambiguated?",
        "Y": "with torch.logsumexp()",
        "Z": "This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "Warning This module is in what state?",
        "Y": "PROTOTYPE",
        "Z": "Warning This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Is this module in a PROTOTYPE state?",
        "Y": "New functions are still being added",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are we actively looking for feedback for?",
        "Y": "UI/UX improvements or missing functionalities",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does  the  module Assert that actual and expected are closed based on what ?",
        "Y": "close if they have same device , same stride , non finite",
        "Z": "The module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  close, they are considered close if they are what?",
        "Y": "real-valued and finite",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What state is this module in?",
        "Y": "PROTOTYPE state",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are still being added to this module?",
        "Y": "New functions",
        "Z": "Warning This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is close when actual and expected are real-valued and finite?",
        "Y": "Asserts that actual and expected are close",
        "Z": "Asserts that actual and expected are close. If actual and expected are real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(what?",
        "Y": "if check_device is True",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is considered close if and only if they are equal?",
        "Y": "Non-finite values",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is considered close if they have the same device(if check_device is True), same dtype(If Check_dtype",
        "Y": "Asserts  that actual and expected are close",
        "Z": "Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "How are non-finite values considered close?",
        "Y": "if and only if they are equal",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Which values are considered close if they have the same device(if check_device is True), same dtype(ifcheck_d",
        "Y": "If actual and expected",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are real-valued and finite?",
        "Y": "If actual and expected",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are considered close if and only if they are equal?",
        "Y": "Non-finite values",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the name of the same device?",
        "Y": "if check_device is True",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are what, they are considered close if both their real and imaginary components are considered close according to the definition above?",
        "Y": "complex-valued",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does if check_device is True mean?",
        "Y": "same device",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are complex-valued?",
        "Y": "If actual and expected",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the definition of close if both real and imaginary components are considered close?",
        "Y": "complex-valued",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  complex-valued, they are considered close what?",
        "Y": "if both their real and imaginary components are considered close according to the definition above",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What can be constructed with torch.as_tensor()?",
        "Y": "whichtorch.Tensor\u2019s",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "When are be Sequence's or Mapping's considered close?",
        "Y": "their structure matches",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is expected(Any)?",
        "Y": "Expected input",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What can be Tensor's or any array-or-scalar-like of the same type?",
        "Y": "actual and expected",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What else can actual and expected can  be Tensor\u2019s or scalar-like of the same type?",
        "Y": "be Sequence\u2019s or Mapping\u2019s",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Actual(Any) \u2013 what type of input?",
        "Y": "Actual input",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is expected input?",
        "Y": "expected(Any)",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does expected(Any) mean?",
        "Y": "Expected input",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does rtol(Optional[float]) represent?",
        "Y": "Relative tolerance",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What must be specified if rtol(Optional[float]) is selected with the below table?",
        "Y": "specified atol must must  be specified",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if default values are omitted?",
        "Y": "default values based on the dtype are selected with given",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Atol(Optional[float]) is what?",
        "Y": "Absolute tolerance",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is optional if atol(Optional[float]) is also specified?",
        "Y": "specified rtol must",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if atol is omitted?",
        "Y": "default values based on the dtype are selected with the below table",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does actual(Any) mean?",
        "Y": "Actual input",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does rtol stand for?",
        "Y": "Relative tolerance",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What must be specified if rtol(Optional[float]) \u2013 Relative tolerance?",
        "Y": "specified atol must also be specified",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if rtol is omitted?",
        "Y": "default values based on the dtype are selected with the below table",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Atol(Optional[float]) \u2013 what?",
        "Y": "Absolute tolerance",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What must be specified if atol(Optional[float]) \u2013 Absolute tolerance?",
        "Y": "specified rtol must also be specified",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is omitted if specified rtol must also be specified?",
        "Y": "default values based on the dtype are selected with the below table",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What can be specified if expected(Any) - Expected input?",
        "Y": "specified atol must",
        "Z": "expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If atol(Optional[float]) is specified, what else must be specified?",
        "Y": "specified rtol must",
        "Z": "expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the default value based on the dtype selected with the below table?",
        "Y": "specified rtol must also be specified",
        "Z": "expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if rtol(Optional[float]) is omitted?",
        "Y": "default values based on the dtype are selected with the below table",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If omitted, what is selected with the below table?",
        "Y": "default values based on the dtype are selected with the below table",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does rtol(Optional[float]) mean?",
        "Y": "Relative tolerance",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What must be specified if rtol is a relative tolerance?",
        "Y": "specified atol must",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If atol(Optional[float]) is specified, what must also be specified?",
        "Y": "specified rtol must",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If true, two NaN values will be considered what?",
        "Y": "equal",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If \"relaxed\", complex values are considered as NaN if either the real or imaginary component is NaN or what?",
        "Y": "real or imaginary component is NaN",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is rtol?",
        "Y": "Relative tolerance",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If rtol(Optional[float]) \u2013 Relative tolerance, what must be specified?",
        "Y": "specified atol must also be specified",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Atol(Optional[float]) \u2013 What?",
        "Y": "Absolute tolerance",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the requirement for Absolute tolerance?",
        "Y": "specified rtol must also be specified",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if atol(Optional[float]) is omitted?",
        "Y": "default values based on the dtype are selected with the below table",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are two NaN values considered equal?",
        "Y": "equal",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If \"relaxed\" complex values are considered as NaN if either the what component is NaN?",
        "Y": "real or imaginary component",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is atol?",
        "Y": "Absolute tolerance",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What must also be specified?",
        "Y": "specified atol must",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are two NaN values considered to be if true?",
        "Y": "equal",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are considered as NaN if either the real or imaginary component is NaN?",
        "Y": "complex values",
        "Z": "equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Atol(Optional[float]) \u2013 What is atol?",
        "Y": "Absolute tolerance",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If atol(Optional[float]) \u2013 Absolute tolerance, what must also be specified?",
        "Y": "specified rtol",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are selected with the below table if omitted?",
        "Y": "default values based on the dtype",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does _nan(Union[bool,str]) mean?",
        "Y": "equal",
        "Z": "equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Complex values are considered as NaN if what component is NaN?",
        "Y": "real or imaginary component",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If true, two NaN values will be considered equal.",
        "Y": "equal",
        "Z": "equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is considered as NaN if either the real or imaginary component is NaN?",
        "Y": "complex values",
        "Z": "equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What asserts that corresponding tensors are on the same device?",
        "Y": "check_device(bool)",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If check_device is disabled, tensors on differentdevice's are moved to what?",
        "Y": "CPU",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If True(default) asserts that what is on the same device?",
        "Y": "corresponding tensors",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Where are tensors on different devices moved before being compared?",
        "Y": "CPU",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Where are tensors on different device's moved before being compared?",
        "Y": "CPU",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does check_dtype assert?",
        "Y": "corresponding tensors have the same dtype",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If the check is disabled, tensors with different type's are promoted to what?",
        "Y": "common dtype",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does check_device(bool) assert is on the same device?",
        "Y": "corresponding tensors",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If True(default), asserts that corresponding tensors have what?",
        "Y": "same dtype",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are tensors with different type's promoted to?",
        "Y": "a common dtype",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What asserts that corresponding tensors have the same dtype?",
        "Y": "check_dtype(bool)",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If this check is disabled, tensors with different type's are promoted to what?",
        "Y": "a common dtype",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does check_stride assert?",
        "Y": "corresponding tensors have the same stride",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What do check_dtype(bool) \u2013 If True(default) asserts that have the same dtype?",
        "Y": "corresponding tensors",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If this check is disabled, tensors with different type's are what?",
        "Y": "promoted",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If True(default) asserts that which tensors have the same stride?",
        "Y": "corresponding tensors have the same stride",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What asserts that corresponding tensors have the same stride?",
        "Y": "check_stride",
        "Z": "check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does msg(Optional[Union[str,Callable[[Tensor,Tensor,",
        "Y": "Optional error message",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Can be passed as what in which case it will be called with the mismatching tensors and a namespace of diagnostic info?",
        "Y": "callable",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What should you do if a torch.Tensorcan't be constructed from an array-or-scalar-like?",
        "Y": "use check_stride(bool)",
        "Z": "check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What if a torch.Tensorcan't be constructed from an array-or-scalar-like?",
        "Y": "have to  refer UsageError",
        "Z": "check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does msg do if the values of corresponding tensors mismatch?",
        "Y": "Optional error message",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What should you do if the mismatching tensors don't match?",
        "Y": "Refer Operational error  message  msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]])",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What message is used if a torch.Tensorcan't be constructed from an array-or-scalar-like?",
        "Y": "refer Usage  - msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]])",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does msg use if the values of corresponding tensors mismatch?",
        "Y": "Optional error message",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Can be passed as what in which case it will be called with the mismatching tensors and a namespace of diagnostic info about",
        "Y": "callable",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does a torch.Tensor contain?",
        "Y": "details",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of tensor can a torch.Tensor not be constructed from?",
        "Y": "array-or-scalar",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the UsageError?",
        "Y": "If only rtol or atol is specified",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If all tensor is quantized or what?",
        "Y": "It can  be sparse also",
        "Z": "UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is a UsageError if any tensor is quantized or sparse?",
        "Y": "there is a temporary restriction currently",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of tensor is specified already?",
        "Y": "only rtol or atol",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of tensor has different types?",
        "Y": "array-likes or scalar-like",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens to the inputs if they are Sequence's?",
        "Y": "their length does not match",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does not match if the inputs are Mapping's?",
        "Y": "their set of keys",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes:",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of a torch.Tensor can't be constructed from?",
        "Y": "array-or-scalar-like",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of tensor is a torch.Tensor?",
        "Y": "quantized or sparse",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the reason if any tensor is quantized or sparse?",
        "Y": "ther is temporary restriction",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of corresponding tensors have different types?",
        "Y": "array-likes",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is an example of an Assertion Error?",
        "Y": "If the inputs are Sequence\u2019s, but their length does not match",
        "Z": "Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of inputs do not match?",
        "Y": "Mapping\u2019s",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "UsageError\u2013 If any tensor is quantized or what?",
        "Y": "sparse",
        "Z": "UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Assertion Error\u2013 If corresponding array-likes have what?",
        "Y": "it can be of different types",
        "Z": "Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens when the inputs are Sequence's but their length does not match?",
        "Y": "their length does not match",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes:",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Assertion Error\u2013 If corresponding tensors do not have what?",
        "Y": "same shape",
        "Z": "UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is UsageError?",
        "Y": "UsageError\u2013 If only rtol or atol is specified",
        "Z": "UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of tensors have different types?",
        "Y": "array-likes",
        "Z": "UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Assertion Error- If the inputs are what type of input?",
        "Y": "Mapping",
        "Z": "Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What do not have the same shape?",
        "Y": "corresponding tensors",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes:",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What causes Assertion Error if corresponding tensors are not on the same device?",
        "Y": "If Check_device",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is an Assertion Error if corresponding tensors do not have the same dtype?",
        "Y": "If Check_dtype",
        "Z": "UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if only rtol or atol is is specified?",
        "Y": "UsageError is  to be referred",
        "Z": "UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is an Assertion Error?",
        "Y": "If the values of corresponding tensors are not close",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If the inputs are what?",
        "Y": "Mapping\u2019s",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes:",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What error occurs when corresponding tensors are not on the same device?",
        "Y": "If Check_device",
        "Z": "Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What error occurs when corresponding tensors do not have the same dtype?",
        "Y": "If Check_dtype",
        "Z": "Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does Assertion Error occur when corresponding tensors are not on the same device?",
        "Y": "If Check_device",
        "Z": "check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does Assertion Error do if corresponding tensors do not have the same dtype?",
        "Y": "If Check_dtype",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If the inputs are what type of's, but their set of keys do not match?",
        "Y": "Mapping",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does Assertion Error do if corresponding tensors are not on the same device?",
        "Y": "If Check_device",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Which tensor does not have the same dtype?",
        "Y": "If Check_dtype",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does Assertion Error do, but corresponding tensors do not have the same stride?",
        "Y": "If Check_stride",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if the inputs are Mapping\u2019s but their set of keys do not match?",
        "Y": "corresponding tensors do not have the same shape",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What error occurs when corresponding tensors do not have the same stride?",
        "Y": "If Check_stride",
        "Z": "Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does not match when the inputs are Mapping's?",
        "Y": "their set of keys",
        "Z": "Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What do corresponding tensors not have?",
        "Y": "same shape",
        "Z": "UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does Assertion Error do if corresponding tensors do not have the same stride?",
        "Y": "If Check_stride",
        "Z": "Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Assertion Error\u2013 If the values of corresponding tensors are not what?",
        "Y": "close",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If the inputs are Mapping's, but what?",
        "Y": "their set of keys do not match",
        "Z": "Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if corresponding tensors do not have the same shape?",
        "Y": "Assertion Error\u2013 If corresponding tensors do not have the same shape",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What error occurs if corresponding tensors are not on the same device?",
        "Y": "If Check_device",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Which tensor is not on the same device?",
        "Y": "If Check_device",
        "Z": "Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does Assertion Error occur when corresponding tensors do not have the same stride?",
        "Y": "If Check_stride",
        "Z": "Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does the following table display for different type's?",
        "Y": "default rtol and atol",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "the dtype refers to what type in case actual and expected do not have the same dtype?",
        "Y": "promoted type",
        "Z": "Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the dtype?",
        "Y": "rtol",
        "Z": "Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does the dtype refer to in case actual and expected do not have the same dtype?",
        "Y": "the dtype refersto the promoted type in case actual and expected do not have the same dtype",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type does the dtype refer to in case actual and expected do not have the same dtype?",
        "Y": "promoted type",
        "Z": "Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the default rtol and atolfor different type's?",
        "Y": "dtype rtol atol float16",
        "Z": "Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is an example of an Assertion Error when corresponding tensors do not have the same dtype?",
        "Y": "If Check_dtype",
        "Z": "Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What error occurs if corresponding tensors do not have the same stride?",
        "Y": "If Check_stride",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if the values of corresponding tensors are not close?",
        "Y": "Assertion Error",
        "Z": "Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What refers to the promoted type in case actual and expected do not have the same dtype?",
        "Y": "the dtype refersto the promoted type in case actual and expected do not have the same dtype",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "the dtype refersto what type in case actual and expected do not have the same dtype?",
        "Y": "promoted type",
        "Z": "Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the namespace of diagnostic information that will be passed to msg if its a callable?",
        "Y": "number_of_elements(int): Number of elements in each tensor being compared",
        "Z": "Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the namespace of diagnostic information that will be passed to msg if its a callable has?",
        "Y": "number_of_elements",
        "Z": "The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs. max_rel_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest relative difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the namespace of diagnostic information that will be passed to msg if?",
        "Y": "total_mismatches",
        "Z": "Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the name of the total mismatches divided by number of elements?",
        "Y": "mismatch_ratio",
        "Z": "The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs. max_rel_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest relative difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does max_abs_diff(Union[int, float]) represent?",
        "Y": "Greatest absolute difference of the inputs",
        "Z": "The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs. max_rel_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest relative difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What returns a 2-D square tensor with the elements of inputas the diagonal?",
        "Y": "If input is  a vector",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "If input is  a matrix, then returns what tensor with the elements of input?",
        "Y": "1-D",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What is the argument diagonal controls?",
        "Y": "If diagonal = 0, it is the main diagonal",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What is above the main diagonal?",
        "Y": "If diagonal > 0,",
        "Z": "If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "If diagonal  0, it is what?",
        "Y": "below the main diagonal",
        "Z": "If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What is the diagonal to consider?",
        "Y": "diagonal(int,optional)",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "If input is  a matrix (2-D tensor), then returns what with the diagonal elements of input?",
        "Y": "1-D tensor",
        "Z": "If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What always returns the diagonal of its input?",
        "Y": "torch.diagonal()",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "Where is the main diagonal If diagonal >0?",
        "Y": "above",
        "Z": "If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "Where is the main diagonal If diagonal 0?",
        "Y": "below",
        "Z": "If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What does out(Tensor,optional) refer to?",
        "Y": "output tensor",
        "Z": "If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What always constructs a tensor with diagonal elements specified by the input?",
        "Y": "torch.diagflat()",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What does torch.diagflat()always return where the input vector is the diagonal?",
        "Y": "the square matrix",
        "Z": "If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "If diagonal  0 is above the main diagonal, what is it?",
        "Y": "below the main diagonal",
        "Z": "If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "Out(Tensor,optional) \u2013 what is the diagonal to consider out(Tensor,optional)?",
        "Y": "the output tensor",
        "Z": "If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What is the diagonal of a given matrix?",
        "Y": "k-th",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What does True if obj is return?",
        "Y": "PyTorch storage object",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does True return if obj is a PyTorch tensor?",
        "Y": "PyTorch storage object",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Returns True if the data type of input is a single element tensor which is not equal to zero after type conversion",
        "Y": "Sets the default floating point dtype tod",
        "Z": "Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned if obj is a PyTorch storage object?",
        "Y": "PyTorch storage object",
        "Z": "Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns True what if the inputs a single element tensor?",
        "Y": "if the inputs a single element tensor",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does it do if the data type of input is a single element tensor which is not equal to zero after type conversions",
        "Y": "Sets the default floating point dtype tod",
        "Z": "Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what if the data type of input is a floating point data type?",
        "Y": "True",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned when the default torch.Tensortype is set to floating point tensor typet?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what if the inputs a single element tensor which is not equal to zero after type conversions?",
        "Y": "True",
        "Z": "Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned by setting the default torch.Tensortype to floating point tensor typet?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the default torch.Tensortype set to?",
        "Y": "floating point tensor typet",
        "Z": "Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Releases all unoccupied cached memory  do?",
        "Y": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What type of tensor does it construct from Random sampling?",
        "Y": "tensor with data",
        "Z": "Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What format is the a sparse tensor in?",
        "Y": "COO(rdinate) format",
        "Z": "Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does a sparse tensor in COO(rdinate) format contain?",
        "Y": "specified values at the given indices",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the a sparse tensor contain?",
        "Y": "specified values at the given indices",
        "Z": "Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "With what a  view of an existing torch.Tensor input is created?",
        "Y": "specified size,stride and storage_offset",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "what numpy can be used  to create a tensor?",
        "Y": "a numpy.ndarray",
        "Z": "Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the size of the scalar value 0?",
        "Y": "the same size as input",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the shape of the scalar value 1?",
        "Y": "the shape defined by the variable argument size",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element in input .",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the size of the scalar value 1?",
        "Y": "the same size as input",
        "Z": "Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the shape of the scalar value 0?",
        "Y": "the shape defined by the variable argument size",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element in input .",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the scalar value 0 return?",
        "Y": "the shape defined by the variable argument size",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned when the scalar value 0 is returned?",
        "Y": "a tensor",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a tensor filled with the scalar value 0, with what as input?",
        "Y": "same size",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size. Returns a",
        "Y": "same size",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the shape defined by the variable argument size?",
        "Y": "scalar value 0",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a tensor filled with the scalar value 1 with what?",
        "Y": "the shape defined by the variable argument size",
        "Z": "Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what tensor of size end start step left l ceil fractextend -",
        "Y": "1-D",
        "Z": "Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the tensor of size end start step left l ceil fractextend?",
        "Y": "1-D",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the tensor of size end start step+1 left l floor fractext end?",
        "Y": "1-D",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "How big is the tensor of size end start step?",
        "Y": "1-D",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the Splits input do?",
        "Y": "Concatenates the given sequence of seq tensor in the given dimension",
        "Z": "Concatenates the given sequence of seq tensor in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What splits a tensor with three or more dimensions into multiple tensors depth wise according toindices_or_section",
        "Y": "Splits input",
        "Z": "Concatenates the given sequence of seq tensor in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "How does one create a new tensor horizontally?",
        "Y": "horizontally stacking",
        "Z": "Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Stack tensors in sequence depth wise (along what axis)?",
        "Y": "third axis",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the given sequence of seq tensor in the given dimension do?",
        "Y": "Concatenates",
        "Z": "Concatenates the given sequence of seq tensor in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Splits input use to split a tensor into multiple tensors depth wise?",
        "Y": "indices_or_sections",
        "Z": "Concatenates the given sequence of seq tensor in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What creates a new tensor horizontally?",
        "Y": "horizontally stacking the tensors intensors",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "How many dimensions does a Splits input have?",
        "Y": "three or more",
        "Z": "Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Splits input do?",
        "Y": "Gathers values along an axis specified by dim",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "How many tensors does Splits input split into?",
        "Y": "multiple tensors depth wise",
        "Z": "Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does a tensor do in sequence depth wise?",
        "Y": "Stack tensors",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the axis specified by Splits input?",
        "Y": "by dim",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Splits input is a tensor with how many dimensions?",
        "Y": "one or more dimensions",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Stack tensors  in horizontal in sequence where?",
        "Y": "horizontally",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Splits input use to split a tensor?",
        "Y": "indices_or_sections",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Splits input do to create a new tensor?",
        "Y": "Stacks tensors",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What do tensors with one or more dimensions Splits input into?",
        "Y": "multiple tensors horizontally",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the index of the new tensor that indexes the input tensor along dimension dimusing the entries in index?",
        "Y": "a Long Tensor",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Where do tensors stack in sequence vertically?",
        "Y": "depth wise",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the tensor that Splits input into multiple tensors horizontally?",
        "Y": "indices_or_sections",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Which tensor indexes the input tensor according to the boolean mask mask?",
        "Y": "1-D",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is Splits input?",
        "Y": "a tensor with three or more dimensions",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the function that gathers values along an axis specified by dim?",
        "Y": "Gathers values along an axis specified by dim",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of a tensor that indexes the input tensor along dimension dimusing the entries in index?",
        "Y": "Stack",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the tensor that indexes the input tensor along dimension dimusing the entries in index?",
        "Y": "a Long Tensor",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of tensor indexes the input tensor according to the boolean mask mask?",
        "Y": "1-D",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of a tensor that Splits input into multiple tensors horizontally?",
        "Y": "Stack",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the new tensor that indexes the input tensor along dimension dimusing the entries in index?",
        "Y": "a Long Tensor",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does move the dimension(s) of in out at the  position(s) in source to the position(s) in destination?",
        "Y": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that returns a new tensor that is a narrowed version of input tensor?",
        "Y": "Alias for torch.movedim()",
        "Z": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What kind of version of input tensor is the new tensor?",
        "Y": "narrowed",
        "Z": "Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is used to move the dimension(s) of in out at the  position(s) in source to the position(s) in destination?",
        "Y": "Alias for torch.movedim()",
        "Z": "Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What method returns a new tensor that is a narrowed version of input tensor?",
        "Y": "Alias for torch.movedim()",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the index that returns a new tensor that indexes the input tensor along dimension dimusing",
        "Y": "a Long Tensor",
        "Z": "Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new 1-D tensor which indexes the input tensor according to what boolean mask mask",
        "Y": "a Bool Tensor",
        "Z": "Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor that indexes the input tensor according to what boolean mask mask?",
        "Y": "1-D",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does move the dimension(s) of input?",
        "Y": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination",
        "Z": "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does it do to the dimension(s) of input?",
        "Y": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination",
        "Z": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that moves the dimension(s) of input to the position(s) in destination?",
        "Y": "Alias for torch.movedim()",
        "Z": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Out-of-place version of torch.Tensor.scatter_add_() Splits the tensor into",
        "Y": "chunks",
        "Z": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What function returns a new tensor that is a narrowed version of input tensor?",
        "Y": "Alias for torch.movedim()",
        "Z": "Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.transpose() do?",
        "Y": "Alias for torch.transpose()",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is another name forfor torch.transpose()?",
        "Y": "Alias for torch.transpose()",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does a torch.Byte Tensor set?",
        "Y": "random number generator state",
        "Z": "Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.Byte Tensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does each row contain num_sample indices sampled from the multinomial probability distribution located in the corresponding row of",
        "Y": "tensor",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.Byte Tensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does a torch.Byte Tensor do?",
        "Y": "Sets the random number generator state",
        "Z": "Returns the random number generator state as a torch.Byte Tensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned when each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row",
        "Y": "tensor",
        "Z": "Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input ?",
        "Y": "a tensor of the same size as input",
        "Z": "Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned with the same shape as Tensor input?",
        "Y": "tensor",
        "Z": "Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is filled with random integers generated uniformly between low(inclusive) and high(exclusive)?",
        "Y": "tensor",
        "Z": "Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does with torch.save() save to a disk file?",
        "Y": "an object",
        "Z": "Saves an object to a disk file.   Loads an object saved with torch.save()from a file.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "what Returns the number of threads used for what parallelism on CPU?",
        "Y": "inter-op",
        "Z": "Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.abs() compute of each element in input ?",
        "Y": "absolute value",
        "Z": "Computes the absolute value of each element in input .   Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.acos() return a new tensor with?",
        "Y": "inverse hyperbolic cosine",
        "Z": "Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the element-wise division often s or 1 byte n s or 2 do?",
        "Y": "multiply the result by the scalar valueand add it toinput",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What computes the inverse cosine of each element in input ?",
        "Y": "Alias for torch.acos()",
        "Z": "Computes the absolute value of each element in input .   Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the element-wise division performed by Alias for torch.acosh?",
        "Y": "often s or 1 byte n s or 2",
        "Z": "Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.acos() return?",
        "Y": "inverse hyperbolic cosine",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.acosh() add to each element of inputinput?",
        "Y": "scalar other to",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the element-wise division performed by Alias for torch.acosh()?",
        "Y": "often s or 1 byte n s or 2",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.acos() compute of each element in input ?",
        "Y": "inverse cosine",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that computes the inverse cosine of each element in input ?",
        "Y": "Alias for torch.acos()",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of cosine does Alias for torch.acos() compute?",
        "Y": "hyperbolic",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of cosine of the elements of input is returned by Alias for torch.acos()?",
        "Y": "hyperbolic",
        "Z": "Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.acosh() add to each element of the inputinput?",
        "Y": "scalar other to",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.acos() perform?",
        "Y": "the element-wise multiplication",
        "Z": "Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of cosine does Alias for torch.acos() return?",
        "Y": "inverse hyperbolic",
        "Z": "Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the element-wise multiplication performed by Alias for torch.acosh?",
        "Y": "often s or 1 byte n s or 2",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What performs often s or 1 byte n s or 2?",
        "Y": "the element-wise division",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does often s or 1 byte n s or 2 perform?",
        "Y": "element-wise division",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "In what unit is the element-wise angle of the given input tensor calculated?",
        "Y": "radians",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that adds the scalar other to each element of the inputinput?",
        "Y": "Alias for torch.acosh()",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that adds the scalar other to each element of the input?",
        "Y": "Alias for torch.acosh()",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the element-wise division performed by Alias for torch.acosh?",
        "Y": "often s or 1 byte n s or 2",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the tensor that returns a new tensor with the arcsine of the elements of input",
        "Y": "Alias for torch.asin",
        "Z": "Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that returns a new tensor with the arcsine of the elements of input?",
        "Y": "Alias for torch.asin()",
        "Z": "Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the result of the element-wise multiplication often s or 1 byte n s or 2 multiplied by?",
        "Y": "scalar value",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does often s or 1 byte n s or 2 multiply the result by?",
        "Y": "scalar value",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Alias for torch.asinh(). Returns a new tensor with what of the elements of input",
        "Y": "arctangent",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.asinh() return?",
        "Y": "arctangent",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.atan() return?",
        "Y": "inverse hyperbolic tangent",
        "Z": "Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of sine does Alias for torch.asin() return?",
        "Y": "inverse hyperbolic",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What element of input does Alias for torch.asinh() return?",
        "Y": "arctangent",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the tensor returned by Alias for torch.asin?",
        "Y": "inverse hyperbolic sine",
        "Z": "Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What element of the elements of input does Alias for torch.asinh() return?",
        "Y": "arctangent",
        "Z": "Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "With consideration of what is the arctangent of inputi/otheri / textother_iinput",
        "Y": "the quadrant",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the Alias for torch.atanh() do?",
        "Y": "Computes the bitwise OR of inputandother",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.atanh() do?",
        "Y": "Computes the bitwise AND of inputandother",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "what Computes the bitwise OR of inputandother.",
        "Y": "AND",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What happens if all elements input evaluate toTrue?",
        "Y": "Tests",
        "Z": "Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what value of the input tensor in the given dimension(s)dim?",
        "Y": "the minimum value of each slice",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns the maximum value of all elements in input  tensor.",
        "Y": "Tests if all elements input evaluate toTrue",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does it do when all elements input evaluate toTrue?",
        "Y": "Tests if all elements input evaluate toTrue",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.",
        "Y": "p-norm",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does it test if all elements input evaluate toTrue?",
        "Y": "Tests if all elements input evaluate toTrue",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns the median of the values in input , ignoring what?",
        "Y": "NaN values",
        "Z": "Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned, ignoringNaN values?",
        "Y": "the median of the values in input",
        "Z": "Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what of the values in input , ignoringNaN values?",
        "Y": "the median",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim?",
        "Y": "p-norm",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does return the log of summed exponentials of each row of the input tensor in the given dimension dim?",
        "Y": "the p-norm",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns the median of the values in input , what?",
        "Y": "ignoringNaN values",
        "Z": "Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does this function check if all input and other satisfy the condition?",
        "Y": "Returns the indices that sort a tensor along a given dimension in ascending order by value",
        "Z": "This function checks if all input and other satisfy the condition:   Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.gt() do?",
        "Y": "Computesinput",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What function checks if all input and other satisfy the condition?",
        "Y": "Alias for torch.gt()",
        "Z": "This function checks if all input and other satisfy the condition:   Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that computes other text input> text other input>other element-",
        "Y": "Alias for torch.gt()",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that computes other text inputgeq text other inputother",
        "Y": "Alias for torch.ge()",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.ge() do?",
        "Y": "Computesinput",
        "Z": "Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264other element-wise.   Alias for torch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that computesinput>other text input> text other inputother",
        "Y": "Alias for torch.gt()",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does return a new tensor with if each element of input is \"close\" to the corresponding element of other?",
        "Y": "boolean elements",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor with what representation if each element is finite or not?",
        "Y": "boolean elements",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265other element-wise.   Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Computesinput>other text input> text other input>other element-wise?",
        "Y": "Alias for torch.ge()",
        "Z": "Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that computesinput>other text input> text other input>other",
        "Y": "Alias for torch.gt()",
        "Z": "Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the Alias for torch.gt() do?",
        "Y": "Computesinput",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that computes input>other text input> text other input>othere",
        "Y": "Alias for torch.gt()",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.gt() do if each element of input is negative infinity or not?",
        "Y": "Tests",
        "Z": "Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Tests if each element of input is what infinity or not?",
        "Y": "negative",
        "Z": "Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor with what representation if each element of input is close to the corresponding element of other?",
        "Y": "boolean elements",
        "Z": "Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What element represents if each element of input is NaN or not?",
        "Y": "boolean elements",
        "Z": "Alias for torch.ge().   Computesinput>other\\text{input} > \\text{other}input>other element-wise.   Alias for torch.gt().   Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.   Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264other element-wise.   Alias for torch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the infinity of each element of input?",
        "Y": "negative",
        "Z": "Returns a new tensor with boolean elements representing if each element is finite or not.   Tests if each element of input is infinite (positive or negative infinity) or not.   Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor with what elements representing if each element of input is NaN or not?",
        "Y": "boolean",
        "Z": "Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the tensor that returns the smallest element of each row of the input tensor in the given dimension dim",
        "Y": "named tuple",
        "Z": "Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the tensor wherevaluesis the k th smallest element of each row of the input tensor in the",
        "Y": "named tuple",
        "Z": "Tests if each element of input is positive infinity or not.   Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that computesinputother text inputleq textotherin",
        "Y": "Alias for torch.le()",
        "Z": "Tests if each element of input is negative infinity or not.   Returns a new tensor with boolean elements representing if each element of input is NaN or not.   Returns a new tensor with boolean elements representing if each element of input is real-valued or not.   Returns a named tuple(values,indices)wherevaluesis the k th smallest element of each row of the input tensor in the given dimension dim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264other element-wise.   Alias for torch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Which window function computes the Kaiser window with window length window_lengthand shape parameterbeta?",
        "Y": "Hann",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window length window_lengthand shape parameterbeta.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the input to the shape shape?",
        "Y": "Broadcastsinput",
        "Z": "Broadcastsinputto the shape shape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned by Broadcastsinput to the shape shape?",
        "Y": "Do cartesian product of the given sequence of tensors",
        "Z": "Broadcastsinputto the shape shape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does it do to return a copy of input?",
        "Y": "Compute combinations of lengthrrrof the given tensor",
        "Z": "Broadcastsinputto the shape shape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the function that returns the cross product of vectors in dimension dimof inputandother?",
        "Y": "Compute combinations of lengthrrrof the given tensor",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does dimension dimof inputandother return?",
        "Y": "the cross product of vectors",
        "Z": "Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the cumulative maximum of elements of inputin the dimension dim?",
        "Y": "a named tuple",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What returns the cumulative maximum of elements of inputin the dimension dim?",
        "Y": "named tuple",
        "Z": "Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What returns the cumulative minimum of elements of inputin the dimension dim?",
        "Y": "named tuple",
        "Z": "Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned when a named tuple(values,indices) returns the cumulative minimum of elements of inputin the dimension dim",
        "Y": "cumulative product of elements of inputin the dimension dim",
        "Z": "Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Returns the cumulative product of elements of inputin the dimension dim?",
        "Y": "the cumulative sum of elements of inputin the dimension dim",
        "Z": "Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the sum of elements of inputin the dimension dim?",
        "Y": "cumulative product",
        "Z": "Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned when a named tuple(values,indices) returns the cumulative product of elements of inputin the dimension dim",
        "Y": "cumulative sum of elements of inputin the dimension dim",
        "Z": "Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What performs a batch matrix-matrix product of matrices stored in input andmat2?",
        "Y": "batch matrix-matrix product of matrices inbatch1andbatch2.",
        "Z": "Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored in input andmat2.   Returns the matrix product of theNNN2-D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias of torch.outer compute the dot product for?",
        "Y": "1D tensors",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias of torch.outer() compute the dot product for?",
        "Y": "1D tensors",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.linalg.det() call?",
        "Y": "Alias for torch.linalg.inv()",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.linalg.det() calculate of a square matrix or batches of square matrices",
        "Y": "log determinant",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What problems does Alias for torch.linalg.slogdet() solve?",
        "Y": "least squares and least norm problems",
        "Z": "Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What exposes various helper functions for the__torch_function__protocol?",
        "Y": "module",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable]",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "SeeExtending torch for what information about the__torch_function__protocol?",
        "Y": "more detail",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable]",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What cannot be overridden by__torch_function__?",
        "Y": "public functions",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a tuple of functions in the torch API that cannot be overridden by__torch_function__?",
        "Y": "publicly available",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable]",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Why can't a tuple of functions be overridden with __torch_function__?",
        "Y": "none of the arguments of these functions are tensors or tensor-likes",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a set of functions that are overridable via __torch_function__?",
        "Y": "Set[Callable] Examples List functions that are overridable via __torch_function__",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does this module expose for the__torch_function__protocol?",
        "Y": "various helper functions",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable]",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the name of the module that exposes helper functions for the__torch_function__protocol?",
        "Y": "SeeExtending torch",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable]",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Return what that cannot be overridden by__torch_function__?",
        "Y": "public functions",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is publicly available in the torch API but cannot be overridden with__torch_function__?",
        "Y": "tuple of functions",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable]",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Why are functions that are publicly available in the torch API but cannot be overridden by__torch_function__?",
        "Y": "none of the arguments of these functions are tensors or tensor-likes",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What are overridable via __torch_function__?",
        "Y": "Set[Callable] Examples List functions",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Where can a tuple of functions be found?",
        "Y": "torch API",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Why can't a tuple of functions be overridden with torch_function__?",
        "Y": "none of the arguments of these functions are tensors or tensor-likes",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How many of the arguments of torch functions are tensors or tensor-likes?",
        "Y": "none",
        "Z": "A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "List functions that are what via __torch_function__ A dictionary that maps namespaces that contain overridable functions to functions",
        "Y": "overridable",
        "Z": "List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be what?",
        "Y": "overridden",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does Dict[Any, List[Callable]] return containing dummy overrides for all overridable functions?",
        "Y": "dict",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A dictionary that maps namespaces that contain what to functions in that namespace that can be overridden?",
        "Y": "overridable functions",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Dict[Any, List[Callable]] Return a dict containing what for all overridable functions?",
        "Y": "dummy overrides",
        "Z": "Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does the Dict[Any, List[Callable]] return a dict containing for all overridable functions?",
        "Y": "dummy overrides",
        "Z": "List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What are lambda functions useful for testing for a type that defines__torch_function__?",
        "Y": "API coverage",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function unconditionally",
        "Y": "-1",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What are lambda functions useful for for a type that defines__torch_function__?",
        "Y": "testing API coverage",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditional",
        "Y": "-1",
        "Z": "Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Implement a function with what?",
        "Y": "checks for__torch_function__overrides",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does a dict contain for all overridable functions?",
        "Y": "dummy overrides",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Return a dict containing what for all overridable functions?",
        "Y": "dummy overrides",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What implement a function with checks for__torch_function__overrides?",
        "Y": "Dict[Callable, Callable] Examples",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A dictionary that maps overridable functions to lambda functions that have the same signature as the real function and unconditionally return -1?",
        "Y": "PyTorch API",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What implements a function with checks for__torch_function__overrides?",
        "Y": "Dict[Callable, Callable] Examples",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What language implements torch::autograd::handle_torch_function?",
        "Y": "C++",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A dictionary that maps what in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return",
        "Y": "overridable functions",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the equivalent of this function in the C++ implementation?",
        "Y": "handle_torch_function",
        "Z": "See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is an example of a function that implements checks for__torch_function__overrides?",
        "Y": "Dict",
        "Z": "Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "In what language is the equivalent of the torch function implemented?",
        "Y": "C++",
        "Z": "Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What was the function exposed by the public torch API originally called?",
        "Y": "likepublic_api(*args,**kwargs)on",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the name of the iterable of arguments to check for __torch_function__ methods?",
        "Y": "iterable",
        "Z": "Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What example implements a function with checks for__torch_function__overrides?",
        "Y": "Dict",
        "Z": "Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the name of the function exposed by the public torch API?",
        "Y": "public_api(function)",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the iterable of arguments to check for __torch_function__ methods?",
        "Y": "relevant_args(iterable)",
        "Z": "Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What language implements a function with checks for__torch_function__overrides?",
        "Y": "C++",
        "Z": "Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is relevant_args?",
        "Y": "iterable",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is args(tuple)?",
        "Y": "Arbitrary positional arguments originally passed intopublic_api",
        "Z": "Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does a function implement?",
        "Y": "checks for__torch_function__overrides",
        "Z": "Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a function exposed by the public torch API called likepublic_api(*args,**kwargs)on which arguments are",
        "Y": "public_api(function)",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Arbitrary positional arguments originally passed intopublic_api?",
        "Y": "args(tuple)",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What language implements the equivalent of the torch function?",
        "Y": "C++",
        "Z": "See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the name of the Arbitrary keyword arguments originally passed intopublic_api?",
        "Y": "kwargs(tuple)",
        "Z": "Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "In what language is the equivalent of handle_torch_function found?",
        "Y": "C++ implementation",
        "Z": "See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed into",
        "Y": "args",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the acronym for Arbitrary keyword arguments originally passed intopublic_api?",
        "Y": "kwargs",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Where does the result come from?",
        "Y": "callingimplementationor an__torch_function__method",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the name of the object used by the torch API?",
        "Y": "object",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the function exposed by the public torch API called?",
        "Y": "public_api(function)",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a tuple of arbitrary positional arguments originally passed intopublic_api?",
        "Y": "args",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Result from callingimplementationor?",
        "Y": "an__torch_function__method",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What type of object does an__torch_function__method return?",
        "Y": "object",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does object raise if no implementation is found?",
        "Y": "TypeError",
        "Z": "Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the name of the object that raises TypeError if no implementation is found?",
        "Y": "Example",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the name of the arbitrary positional arguments originally passed intopublic_api?",
        "Y": "args",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed into what?",
        "Y": "public_api",
        "Z": "kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Result from callingimplementationor what?",
        "Y": "an__torch_function__method",
        "Z": "Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "object :raises what if no implementation is found?",
        "Y": "TypeError",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What do you check in the elements of an iterable?",
        "Y": "__torch_function__ implementations",
        "Z": "relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is considered non-dispatchable?",
        "Y": "exactTensors andParameters",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Param relevant_args: Iterable or what to check for __torch_function__ methods?",
        "Y": "aguments",
        "Z": "Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":type relevant_args: What type of args is used to check for __torch_function__ methods?",
        "Y": "iterable",
        "Z": "kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What raises if no implementation is found?",
        "Y": "TypeError",
        "Z": ":raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Check for what in the elements of an iterable?",
        "Y": "__torch_function__ implementations",
        "Z": "Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":param relevant_args: Iterable or what to check for __torch_function__ methods?",
        "Y": "aguments",
        "Z": ":raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":type relevant_args: iterable True: if any of the elements of relevant_args have __torch",
        "Y": "if any of the elements of relevant_args have __torch_function__ implementations",
        "Z": ":raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does Checks if something is a Tensor-like, including an exactTensor?",
        "Y": "bool",
        "Z": "See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is used to check for __torch_function__ methods?",
        "Y": "aguments",
        "Z": "Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":type relevant_args: iterable True what?",
        "Y": "if any of the elements of relevant_args have __torch_function__ implementations",
        "Z": "Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What returns true if the passed-in input is a Tensor-like?",
        "Y": "ReturnsTrueif the passed-in input is a Tensor-like",
        "Z": "Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in",
        "Y": "bool",
        "Z": "Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is true if any of the elements of relevant_args have __torch_function__ implementations?",
        "Y": "True if any of the elements of relevant_args have __torch_function__ implementations",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "bool Checks if something is a Tensor-like, including what?",
        "Y": "exactTensor",
        "Z": "bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "When does ReturnsTrueif the passed-in input is a Tensor-like?",
        "Y": "whenever there\u2019s a__torch_function__attribute on the type of the input",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A subclass of tensor is generally what?",
        "Y": "Tensor-like",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What aren't usually Tensor-like?",
        "Y": "Built-in or user types",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How can built-in or user types be made Tensor-like?",
        "Y": "implementing __torch_function__",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does bool do if something is a Tensor-like, including an exactTensor?",
        "Y": "Checks",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "When does ReturnsTrueif the passed-in input is a Tensor-like occur?",
        "Y": "whenever there\u2019s a__torch_function__attribute on the type of the input",
        "Z": "Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Built-in or user types can be made Tensor-like by implementing what?",
        "Y": "__torch_function__",
        "Z": "Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does bool do?",
        "Y": "Checks if something is a Tensor-like",
        "Z": "bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is another name for if something is a Tensor-like?",
        "Y": "Checks",
        "Z": "See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What checks if something is a Tensor-like?",
        "Y": "Checks",
        "Z": "See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does ReturnsTrue if the passed-in input is a Tensor-like?",
        "Y": "ReturnsTrueif the passed-in input is a Tensor-like",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "When does ReturnsTrue if the passed-in input is a Tensor-like?",
        "Y": "whenever there\u2019s a__torch_function__attribute on the type of the input",
        "Z": "ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does an exactTensor do?",
        "Y": "Checks if something is a Tensor-like",
        "Z": "Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Returns True if the passed-in input is what?",
        "Y": "a Tensor-like",
        "Z": "ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the function passed in?",
        "Y": "a handler for a method or property belonging totorch",
        "Z": "ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does the return true if the function passed in is a handler for a method or property belonging totorch.Tens",
        "Y": "Note",
        "Z": "ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What subclass is generally a Tensor-like?",
        "Y": "tensor",
        "Z": "ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the function passed in for a method or property belonging totorch.Tensor?",
        "Y": "a handler",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does Returns True if the function passed in is a handler for a method or property belonging totorch.Tens",
        "Y": "Note",
        "Z": "ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "When does this occur?",
        "Y": "whenever there\u2019s a__torch_function__attribute on the type of the input",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What returns true if the function passed in is a handler for a method or property belonging totorch.Tensor?",
        "Y": "Returns True",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What must be passed in for properties?",
        "Y": "their__get__method",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "When does this happen?",
        "Y": "whenever there\u2019s a__torch_function__attribute on the type of the input",
        "Z": "Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A subclass of tensor is generally a subclass of what?",
        "Y": "Tensor",
        "Z": "A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Returns what if the function passed in is a handler for a method or property belonging totorch.Tensor?",
        "Y": "True",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a subclass of tensor generally called?",
        "Y": "Tensor-like",
        "Z": "A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What sometimes don't contain a__module__slot?",
        "Y": "Methods/properties",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A subclass of tensor is generally a what?",
        "Y": "Tensor-like",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What can built-in or user types be made Tensor-like by implementing?",
        "Y": "__torch_function__",
        "Z": "Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What sometimes don\u2019t contain a__module__slot?",
        "Y": "Methods/properties",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What type of property can be made by implementing __torch_function__?",
        "Y": "Tensor",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What must the first passed-in argument be?",
        "Y": "instance of torch.Tensor",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What type of argument must be passed in for a method to be considered a Tensor?",
        "Y": "Examples",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What can be made Tensor-like by implementing?",
        "Y": "__torch_function__",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Why is the__get__method passed in?",
        "Y": "Methods/properties sometimes don\u2019t contain a__module__slot",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does the first passed-in argument need to be an instance of torch.Tensor?",
        "Y": "Examples",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Methods/properties sometimes don't contain a__module__slot. They require that the first passed-in argument",
        "Y": "an instance of torch.Tensor",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is an example of a function that wraps a given function with?",
        "Y": "Wraps a given function with__torch_function__-related functionality",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Why is the__get__method needed?",
        "Y": "Methods/properties sometimes don\u2019t contain a__module__slot",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does examples wrap a given function with?",
        "Y": "Examples Wraps a given function with__torch_function__-related functionality",
        "Z": "Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a callable that returns an iterable of Tensor-likes passed into the function?",
        "Y": "dispatcher(Callable)",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does the first passed-in argument need to be?",
        "Y": "instance of torch.Tensor",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does Example Wrap a given function with?",
        "Y": "Examples Wraps a given function with__torch_function__-related functionality",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does a method/property require that the first passed-in argument be?",
        "Y": "an instance of torch.Tensor",
        "Z": "Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What wraps a given function with?",
        "Y": "Examples Wraps a given function with__torch_function__-related functionality",
        "Z": "Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What must be the first passed-in argument?",
        "Y": "an instance of torch.Tensor",
        "Z": "They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does example wrap a given function with?",
        "Y": "Wraps a given function with__torch_function__-related functionality",
        "Z": "Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does dispatcher(Callable) do?",
        "Y": "Wraps a given function with__torch_function__-related functionality",
        "Z": "Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does dispatcher(Callable) return?",
        "Y": "Note",
        "Z": "Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Returns a 3-dimensional view of each input tensor with what dimensions?",
        "Y": "zero",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input(Tensororlist of Tensors) \u2013 output (Tensor or tuple of Tensors) Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "X": "What is the second input tensor out(Tensor,optional)?",
        "Y": "output tensor",
        "Z": "Computes the bitwise XOR of inputandother. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input\u2013 the first input tensor other\u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor"
    },
    {
        "X": "Out(Tensor,optional) returns what?",
        "Y": "output tensor",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninput is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "X": "Wheninput is on the CPU, the implementation of torch.sinh may use what library?",
        "Y": "Sleef library",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninput is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "X": "What does the Sleef library do?",
        "Y": "Seeherefor details",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninput is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "X": "shifts is the number of places by which the elements of the tensor are shifted?",
        "Y": "python:ints",
        "Z": "input(Tensor) \u2013 the input tensor. shifts(intortuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims(intortuple of python:ints) \u2013 Axis along which to roll Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "X": "What must be a tuple of the same size if shifts is a tuple?",
        "Y": "dims",
        "Z": "input(Tensor) \u2013 the input tensor. shifts(intortuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims(intortuple of python:ints) \u2013 Axis along which to roll Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "X": "Ifrepsspecifies fewer dimensions thaninputhas, then ones are what?",
        "Y": "prepended torepsuntil all dimensions are specified",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is the shape of a tensor?",
        "Y": "Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive)",
        "Z": "Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.Byte Tensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Ifrepsspecifies how many dimensions thaninputhas, then ones are prepended torepsuntil all dimensions are specified.",
        "Y": "fewer",
        "Z": "Constructs a tensor by repeating the elements of input.\nTherepsargument specifies the number of repetitions\nin each dimension. Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is an example of an input that is treated as (1, 1, 2, 2) if it has fewer dimensions thanrepsspecifies?",
        "Y": "ifinputhas shape",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "When isinput is treated as if it were unsqueezed at dimension zero until it has as many dimensions asrepsspecifies?",
        "Y": "ifinputhas fewer dimensions thanrepsspecifies",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is treated as if it had the shape (1, 1, 4, 2)?",
        "Y": "ifinputhas shape",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is the difference between ifinputhas has fewer dimensions thanrepsspecifies?",
        "Y": "Note",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "Ifrepsspecifies what thaninputhas, then ones are prepended torepsuntil all dimensions are specified?",
        "Y": "fewer dimensions",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "Ifinputhas shape andrepsis are treated as what?",
        "Y": "(1, 1, 2, 2).",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is another example of a condition whereinput is is treated as if it were unsqueezed at dimension zero until it has as many",
        "Y": "ifinputhas fewer dimensions thanrepsspecifies",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "Ifinputhas shape (what is the number of dimensions) andrepsis (3, 3, 2, 2), theninput is treated as",
        "Y": "4, 2)",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is the difference between ifinputhas fewer dimensions thanrepsspecifies and If input is  treated as if it had",
        "Y": "Note",
        "Z": "Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is returned along a given dimension?",
        "Y": "theklargest elements of the giveninput tensor",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "When is the last dimension of the input chosen?",
        "Y": "Ifdimis not given",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What returns theksmallest elements of the given input tensor?",
        "Y": "IflargestisFalsethen",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What are the indices of the named tuple of(values, indices)?",
        "Y": "the indices of the elements in the originalinput tensor",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The boolean optionsortedIf True will make sure that the returnedkelements are themselves sorted?",
        "Y": "input(Tensor)",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What returns the k in \"top-k\"?",
        "Y": "k(int)",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "Which elements of the given input tensor are returned along a given dimension?",
        "Y": "theklargest elements",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What happens when the last dimension of the input is chosen?",
        "Y": "Ifdimis not given",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is returned, where the indicesare the indices of the elements in the originalinput tensor?",
        "Y": "A named tuple of(values, indices)",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The boolean optionsortedIf True will make sure that the returnedkelements are themselves sorted what?",
        "Y": "input(Tensor)",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "If the last dimension of the input is not given, what is the last dimension of theinput?",
        "Y": "Ifdimis",
        "Z": "Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What returns theksmallest elements?",
        "Y": "IflargestisFalsethen",
        "Z": "IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What are the indices in the named tuple of(values, indices)?",
        "Y": "the indices of the elements in the originalinput tensor",
        "Z": "Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is the k in \"top-k\" dim(int,optional)?",
        "Y": "k(int)",
        "Z": "Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is not given, the last dimension of the input is chosen?",
        "Y": "Ifdimis",
        "Z": "Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "Ifdimis is not given, the last dimension of the inputs chosen.",
        "Y": "IflargestisFalsethen theksmallest elements",
        "Z": "Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The boolean optionsortedIf True will make sure that the returnedkelements are themselves sorted.",
        "Y": "input(Tensor)",
        "Z": "Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is the k in \u201ctop-k\u201d dim(int,optional)?",
        "Y": "k(int)",
        "Z": "Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What are the indices in a named tuple of(values, indices)?",
        "Y": "the indices of the elements in the originalinput tensor",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What controls whether to return largest or smallest elements?",
        "Y": "k(int)",
        "Z": "IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What are theksmallest elements returned?",
        "Y": "IflargestisFalsethen",
        "Z": "IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is returned when largestisFalsethen theksmallest elements are returned?",
        "Y": "A named tuple of(values, indices)",
        "Z": "IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What are the indices of a named tuple of(values, indices)?",
        "Y": "the indices of the elements in the originalinput tensor",
        "Z": "A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,",
        "Y": "k(int)",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor?",
        "Y": "boolean optionsortedIf True",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is used in the 2D average-pooling operation?",
        "Y": "ink",
        "Z": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of average-pooling operation does inkHkWkH times kWkHkWregions by step sizes",
        "Y": "2D",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.   Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of average-pooling operation does inkTkHkWkT times kH times",
        "Y": "3D",
        "Z": "Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.   Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of pooling operation does inkHkWkH times kWkHkWregions by step sizes?",
        "Y": "2D average-pooling operation",
        "Z": "Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.   Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of operation does inkHkWkH times kWkHkWregions by step sizes?",
        "Y": "2D average-pooling",
        "Z": "Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.   Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of operation does inkTkHkWkT times kH times kWkT",
        "Y": "3D average-pooling operation",
        "Z": "Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the 3D max pooling over an input signal composed of several input planes compute?",
        "Y": "a partial inverse ofMaxPool1d",
        "Z": "Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does MaxPool2d compute?",
        "Y": "partial inverse",
        "Z": "Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does inkTkHkWkT times kH times kWkTkH",
        "Y": "3D average-pooling operation",
        "Z": "Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of the partial inverse of MaxPool2d?",
        "Y": "Computes a partial inverse ofMaxPool2d",
        "Z": "Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the 3D max pooling do?",
        "Y": "Computes a partial inverse ofMaxPool1d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of Computed a partial inverse of MaxPool3d?",
        "Y": "Computes a partial inverse ofMaxPool3d",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of calculating a partial inverse of MaxPool1d?",
        "Y": "Computes a partial inverse ofMaxPool1d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the function do that computes a partial inverse of?",
        "Y": "Computes a partial inverse ofMaxPool2d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of Computed a partial inverse of MaxPool1d?",
        "Y": "Computes a partial inverse ofMaxPool1d",
        "Z": "Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the inverse of MaxPool3d?",
        "Y": "Computes a partial inverse ofMaxPool3d",
        "Z": "Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the function that computes a partial inverse of MaxPool3d?",
        "Y": "Computes a partial inverse ofMaxPool2d",
        "Z": "Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is another name for MaxPool3d?",
        "Y": "Computes a partial inverse ofMaxPool3d",
        "Z": "Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the HardTanh function apply element-wise?",
        "Y": "In-place version ofrelu()",
        "Z": "In-place version ofrelu().   Applies the HardTanh function element-wise.   In-place version ofhardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "In-place version ofrelu(). Applies what element-wise function?",
        "Y": "HardTanh function",
        "Z": "In-place version ofrelu().   Applies the HardTanh function element-wise.   In-place version ofhardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "In-place version ofhardtanh(). Applies what function element-wise?",
        "Y": "hardswish function",
        "Z": "In-place version ofrelu().   Applies the HardTanh function element-wise.   In-place version ofhardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of function does the HardTanh function apply?",
        "Y": "element-wise",
        "Z": "In-place version ofrelu().   Applies the HardTanh function element-wise.   In-place version ofhardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the hardswish function called?",
        "Y": "In-place version ofelu()",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the hardswish function?",
        "Y": "element-wise",
        "Z": "In-place version ofhardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of function does the hardswish function apply?",
        "Y": "element-wise",
        "Z": "In-place version ofhardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of function is applied element-wise?",
        "Y": "hardswish",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "ELU(x)=max(max(max(0,x) + min(0,(exp(x)",
        "Y": "0,x",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "ELU(x)=max(max(max(0,x)+min(0,(exp(x",
        "Y": "0,x",
        "Z": "Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the function that applies element-wise?",
        "Y": "In-place version ofelu()",
        "Z": "Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "ELU(x)=max(max(x)+min(0,(exp(x)1))",
        "Y": "0,x",
        "Z": "Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Where is the version of ofelu() used?",
        "Y": "In-place",
        "Z": "Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Where is the version ofelu() used?",
        "Y": "In-place",
        "Z": "In-place version ofelu().   Applies element-wise,SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717andscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "In-place version ofelu(). Applies what?",
        "Y": "element-wise",
        "Z": "In-place version ofelu().   Applies element-wise,SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717andscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the value of CELU?",
        "Y": "x",
        "Z": "Applies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version ofleaky_relu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Where is the version ofleaky_relu()?",
        "Y": "In-place",
        "Z": "Applies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version ofleaky_relu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of transformation is applied to the incoming data?",
        "Y": "bilinear",
        "Z": "Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b.   Applies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What returns a LongTensor with zeros everywhere except where the index of last dimension matches the corresponding value of the input tens",
        "Y": "tensor of shape(*,num_classes)",
        "Z": "A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes ofbagsof embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape(*)and returns a tensor of shape(*,num_classes)that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the cosine similarity between?",
        "Y": "x1 and x2",
        "Z": "Seetorch.nn.PairwiseDistancefor details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the function measure Binary Cross Entropy between?",
        "Y": "target and output logits",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is a negative log likelihood loss?",
        "Y": "Poisson",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion that combineslog_softmaxandnll_lossin a single function?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does this criterion combine into a single function?",
        "Y": "log_softmaxandnll_lossin",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of negative log likelihood loss is seen in HingeEmbeddingLossfor details?",
        "Y": "Gaussian",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is this criterion?",
        "Y": "combineslog_softmaxandnll_lossin a single function",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What loss is the Gaussian negative log likelihood loss?",
        "Y": "Connectionist Temporal Classification",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the function that measures Gaussian negative log likelihood loss?",
        "Y": "SeeHingeEmbeddingLoss",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   SeeSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is a single function that measures Binary Cross Entropy between target and output logits?",
        "Y": "combineslog_softmaxandnll_lossin",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the loss that measures the element-wise mean squared error?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does TheKullback-Leibler divergence Loss Function measure?",
        "Y": "MarginRankingLoss",
        "Z": "TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the negative log likelihood loss?",
        "Y": "Poisson",
        "Z": "Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion for the Connectionist Temporal Classification loss?",
        "Y": "combineslog_softmaxandnll_lossin",
        "Z": "This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of negative log likelihood loss is SeeHingeEmbeddingLossfor details?",
        "Y": "Gaussian",
        "Z": "Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does TheKullback-Leibler divergence Loss Function do?",
        "Y": "SeeMarginRankingLossfor details",
        "Z": "TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the MultiLabelMarginLossfor details?",
        "Y": "MultiLabelMarginLossfor details",
        "Z": "Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the loss criterion that combineslog_softmaxandnll_lossin a single function?",
        "Y": "Poisson negative log likelihood loss",
        "Z": "Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the criterion that combineslog_softmaxandnll_lossin a single function?",
        "Y": "SeeCosineEmbeddingLoss",
        "Z": "Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the MultiLabelSoftMarginLossfor details?",
        "Y": "MultiLabelSoftMarginLossfor details",
        "Z": "This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the criterion that combines log_softmaxandnll_lossin a single function?",
        "Y": "SeeCosineEmbeddingLoss",
        "Z": "SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion for combining log_softmaxandnll_lossin?",
        "Y": "combineslog_softmaxandnll_lossin a single function",
        "Z": "SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does TheKullback-Leibler measure?",
        "Y": "the element-wise mean squared error",
        "Z": "SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does MultiLabelSoftMarginLossfor details?",
        "Y": "MultiLabelMarginLossfor details",
        "Z": "This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is another name for MultiLabelMarginLoss?",
        "Y": "MultiLabelMarginLossfor details",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion for log_softmaxandnll_lossin?",
        "Y": "combineslog_softmaxandnll_lossin a single function",
        "Z": "This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion that combines log_softmaxandnll_lossin a single function?",
        "Y": "The Connectionist Temporal Classification loss",
        "Z": "This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is another name for the Gaussian negative log likelihood loss?",
        "Y": "SeeHingeEmbeddingLossfor details",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of TheKullback-Leibler divergence Loss Function?",
        "Y": "SeeMarginRankingLossfor details",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the function that measures the mean element-wise absolute value difference?",
        "Y": "multi_margin_loss",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What loss is Gaussian negative log likelihood loss?",
        "Y": "The Connectionist Temporal Classification loss",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the loss function that takes the mean element-wise absolute value difference?",
        "Y": "SeeHingeEmbeddingLoss",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the loss function that measures the mean element-wise mean squared error?",
        "Y": "MarginRankingLoss",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the term for a negative log likelihood loss?",
        "Y": "Gaussian negative log likelihood loss",
        "Z": "Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the negative log likelihood loss function?",
        "Y": "MultiLabelSoftMarginLoss",
        "Z": "Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is theKullback-Leibler divergence Loss Function?",
        "Y": "SeeHingeEmbeddingLossfor details",
        "Z": "SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the function that measures the element-wise mean squared error?",
        "Y": "Measures the element-wise mean squared error",
        "Z": "Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Measures what?",
        "Y": "the element-wise mean squared error",
        "Z": "Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Rearranges elements in what?",
        "Y": "a tensor of shape",
        "Z": "Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor.   Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.   Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given aninputand a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is thedownscale_factor?",
        "Y": "r",
        "Z": "Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.   Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What tensor is rearranged by rearranging elements in a tensor of shape?",
        "Y": "Pads",
        "Z": "Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor.   Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.   Pads tensor.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Down/up samples the input to what?",
        "Y": "Upsamples the input to either the givensizeor the givenscale_factor Upsamples the input",
        "Z": "Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor.   Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.   Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given aninputand a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "How does Pads tensor sample the input?",
        "Y": "Down/up",
        "Z": "Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given aninputand a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the flow-fieldgrid do?",
        "Y": "computes theoutputusinginputvalues and pixel locations fromgrid",
        "Z": "Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given aninputand a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the tensor that down/up samples the input to?",
        "Y": "Pads tensor",
        "Z": "Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given aninputand a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Given an inputand a what, computes theoutputusinginputvalues and pixel locations fromgrid?",
        "Y": "flow-fieldgrid",
        "Z": "Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor.   Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.   Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given aninputand a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What dot product version of this function does not support anoutparameter?",
        "Y": "1-dimensional",
        "Z": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What is the first tensor to be multiplied other(Tensor)?",
        "Y": "input(Tensor)",
        "Z": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What is another name for Alias for torch.acos?",
        "Y": "Alias for torch.acos",
        "Z": "Alias for torch.acos().",
        "source": "https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos"
    },
    {
        "X": "What does inputalong the givendim do?",
        "Y": "Counts the number of non-zero values in the tensor",
        "Z": "Counts the number of non-zero values in the Tensor inputalong the givendim.\nIf no dim is specified then all non-zeros in the tensor are counted. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints,optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "X": "Intortuple of what is optional?",
        "Y": "python:ints",
        "Z": "Counts the number of non-zero values in the Tensor inputalong the givendim.\nIf no dim is specified then all non-zeros in the tensor are counted. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints,optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "X": "What cannot handle huge sparse matrices?",
        "Y": "thattorch.linalg.svd()",
        "Z": "Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "Why should you use the full-rank SVD implementationtorch.linalg.svd() for dense matrices?",
        "Y": "10-fold higher performance characteristics",
        "Z": "Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What can the low-rank SVD be useful for large sparse matrices?",
        "Y": "thattorch.linalg.svd()",
        "Z": "In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is the input tensor of size(,m,n)(*,m,n)(*,m,n)",
        "Y": "A (Tensor)",
        "Z": "The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is the input of A (Tensor)?",
        "Y": "tensor",
        "Z": "The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "Why should you use full-rank SVD implementationtorch.linalg.svd() for dense matrices?",
        "Y": "10-fold higher performance characteristics",
        "Z": "To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is the tensor of size(,m,n)(*, m, n)(,m,n",
        "Y": "q",
        "Z": "To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What type of matrices can low-rank SVD be useful for?",
        "Y": "huge sparse matrices",
        "Z": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "A (Tensor) is the input tensor of size(,m,n)(*, m, n",
        "Y": "q",
        "Z": "In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "How much higher performance characteristics do full-rank SVD implementations of dense matrices have?",
        "Y": "10-fold",
        "Z": "Return the singular value decomposition(U,S,V)of a matrix,\nbatches of matrices, or a sparse matrixAAAsuch thatA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In caseMMMis given, then\nSVD is computed for the matrixA\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "Why should you use full-rank SVD for dense matrices?",
        "Y": "10-fold higher performance characteristics",
        "Z": "In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is a slightly overestimated rank of A. conduct?",
        "Y": "q",
        "Z": "A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is defined by expanding theiiithinput over dimensions defined by other inputs?",
        "Y": "theiiithgrid",
        "Z": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What can NNNtensors be?",
        "Y": "scalar or 1-dimensional vector",
        "Z": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What is tensors(list of Tensor)?",
        "Y": "list of scalars or 1 dimensional tensors",
        "Z": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What will be treated as tensors of size(1,)(1,)(1,)automatically?",
        "Y": "Scalars",
        "Z": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "How is theiiithgrid defined?",
        "Y": "expanding theiiithinput over dimensions defined by other inputs",
        "Z": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What is a torch.Tensoris?",
        "Y": "multi-dimensional matrix",
        "Z": "a torch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of torch is float64ortorch?",
        "Y": "double torch",
        "Z": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are tensor views?",
        "Y": "tensor views",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are some examples of a torch.Tensor attributes?",
        "Y": "thetorch.dtype,torch.device, and torch.layout attributes",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for a torch.Tensor?",
        "Y": "Note",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the result of torch.FloatTensor.abs()?",
        "Y": "a new tensor",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are some of the attributes of a torch.Tensor?",
        "Y": "thetorch.dtype,torch.device, and torch.layout attributes",
        "Z": "For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does usingto()method on a tensor do?",
        "Y": "Warning",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "For more information on thetorch.dtype,torch.device, and torch.layout attributes of what",
        "Y": "a torch.Tensor",
        "Z": "For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What kind of memory usage could be caused by the current implementation of torch.Tensor?",
        "Y": "unexpectedly high",
        "Z": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does current implementation of torch.Tensor introduce?",
        "Y": "memory overhead",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Returns a new Tensor with dataas the tensor data?",
        "Y": "new_tensor",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor with dataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the new Tensor return with dataas?",
        "Y": "tensor data",
        "Z": "Tensor.new_tensor Returns a new Tensor with dataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Returns a new Tensor with dataas what?",
        "Y": "tensor data",
        "Z": "Returns a new Tensor with dataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the function that is used to log in a system?",
        "Y": "Alias for torch.special.logit()",
        "Z": "Alias for torch.special.logit().",
        "source": "https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit"
    },
    {
        "X": "What is the name of the function used by Alias for torch.special.logit()?",
        "Y": "Alias for torch.special.logit()",
        "Z": "Alias for torch.special.logit().",
        "source": "https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit"
    },
    {
        "X": "What is another name for COO format?",
        "Y": "Coordinate format",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the indices of specified elements collected in COO format?",
        "Y": "inindicestensor of size(ndim,nse)",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the corresponding values collected invaluestensor of size(nse)and with?",
        "Y": "arbitrary integer or floating point number element type",
        "Z": "the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data).",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is added to the memory consumption of a sparse COO tensor from storing other tensor data?",
        "Y": "a constant overhead",
        "Z": "the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data).",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The corresponding values are collected invaluestensor of what?",
        "Y": "size(nse,)",
        "Z": "the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does storing other tensor data add to the memory consumption of a sparse COO tensor?",
        "Y": "a constant overhead",
        "Z": "wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does ndimis indicate about a tensor?",
        "Y": "dimensionality",
        "Z": "wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of tensor is at least(ndim*8+sizeofelementtypeinbytes>)*nse",
        "Y": "a sparse COO tensor",
        "Z": "wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the dimensionality of the tensor andnseis?",
        "Y": "number of specified elements",
        "Z": "wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What tensor is at leastproduct(tensorshape>)*sizeofelementtypeinbytes>?",
        "Y": "strided tensor",
        "Z": "Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of tensor is at leastproduct(tensorshape>)*sizeofelementtypeinbytes",
        "Y": "strided",
        "Z": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How many non-zero 32-bit floating point numbers does a 10 000 x 10 000 tensor have?",
        "Y": "100 000",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero floating point numbers?",
        "Y": "at least(2*8+4)*100000=2000000bytes",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the inputi of a sparse tensor?",
        "Y": "inputiis NOT a list of index tuples",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the inputiis NOT a list of?",
        "Y": "index tuples",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the inputi?",
        "Y": "inputiis NOT a list of index tuples",
        "Z": "Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The inputiis NOT a list of what?",
        "Y": "index tuples",
        "Z": "Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What extends the sparse COO tensor by allowing thevaluestensor to be a multi-dimensional tensor",
        "Y": "PyTorch",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What element does PyTorch hybrid COO tensor have?",
        "Y": "typetorch.int64",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The indices of specified elements are collected inindicestensor of what?",
        "Y": "size",
        "Z": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the tensor values collected with?",
        "Y": "arbitrary integer or floating point number element type",
        "Z": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The corresponding tensor values are collected invaluestensor of size(sparse_dims,nse)and",
        "Y": "nse,dense_dims",
        "Z": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the indices of specified elements collected?",
        "Y": "inindicestensor",
        "Z": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are tensor values collected invaluestensor of?",
        "Y": "size(nse,dense_dims)",
        "Z": "the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the corresponding tensor values collected invaluestensor of size?",
        "Y": "nse,dense_dims",
        "Z": "the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What would we write to create a 2 + 1-dimensional tensor?",
        "Y": "ifsis a sparse COO tensor",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What would we write if we want to create a sparse tensor?",
        "Y": "ifsis a sparse COO tensor",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the location of the 2 + 1-dimensional tensor?",
        "Y": "entry [3, 4] at location (0, 2)",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What would we write to create a 2 + 1-dimensional tensor with the entry [3, 4 at location (0, 2), entry [",
        "Y": "ifsis a sparse COO tensor",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the function that stores sparse indices?",
        "Y": "s.indices()",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the COO tensor?",
        "Y": "sparse",
        "Z": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What do we have the following invariants?",
        "Y": "ifsis a sparse COO tensor",
        "Z": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the meaning of strided tensors in a sparse COO tensor?",
        "Y": "Note",
        "Z": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the function where sparse indices are stored explicitly?",
        "Y": "s.indices()",
        "Z": "M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the values of a hybrid tensor?",
        "Y": "K-dimensional tensors",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "s.values().layout==torch.strided- values are stored as what?",
        "Y": "strided tensors",
        "Z": "s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are s.values().layout==torch.strided- values stored as?",
        "Y": "strided tensors",
        "Z": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Dense dimensions always follow what dimensions?",
        "Y": "sparse",
        "Z": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What permits uncoalescedsparse tensors?",
        "Y": "PyTorch sparse COO tensor format",
        "Z": "Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors where there",
        "Y": "duplicate coordinates",
        "Z": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of uncoalesced tensor is created when multiple values are specified for the same index1?",
        "Y": "1-D",
        "Z": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the value at an index in a PyTorch sparse COO tensor?",
        "Y": "the sum of all duplicate value entries",
        "Z": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.Tensor.is_coalesced() return true?",
        "Y": "Note",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.is_coalesced()returnsTrue?",
        "Y": "Note",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Most operations will work identically given what two types of tensors?",
        "Y": "coalesced or uncoalesced sparse tensor",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Most operations will work identically given what?",
        "Y": "coalesced or uncoalesced sparse tensor",
        "Z": "In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Are the indices of specified tensor elements unique or unique?",
        "Y": "unique",
        "Z": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Which indices are sorted in lexicographical order?",
        "Y": "torch",
        "Z": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns True?",
        "Y": "torch.Tensor.is_coalesced()",
        "Z": "torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How is the addition of sparse COO tensors implemented?",
        "Y": "concatenating the indices and values tensors",
        "Z": "For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What dimension can be acquired using methodstorch.Tensor.sparse_dim() and torch.Tens",
        "Y": "dense",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of tensor can be acquired using methodstorch.Tensor.indices() and torch.Tens",
        "Y": "sparse COO tensor",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "For acquiring the COO format data of an what type of tensor, usetorch.Tensor._values",
        "Y": "uncoalesced",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices():",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can one construct of a sparse COO tensor using thetorch.Tensor.coalesce()",
        "Y": "coalesced copy",
        "Z": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "In PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be what?",
        "Y": "zero",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are there operations that may interpret the fill value differently?",
        "Y": "operations that may interpret the fill value differently",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "A CSR sparse tensor consists of three what?",
        "Y": "1-D tensors",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What consists of compressed row indices?",
        "Y": "Thecrow_indicestensor",
        "Z": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the CSR sparse tensor?",
        "Y": "1-D tensor of sizesize[0]+1",
        "Z": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the index invaluesandcol_indices depend on?",
        "Y": "where the given row starts",
        "Z": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What consists of three 1-D tensors?",
        "Y": "CSR sparse tensor",
        "Z": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the size of a CSR sparse tensor?",
        "Y": "sizesize[0]+1",
        "Z": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the tensor encode the index invaluesandcol_indicesdepending on?",
        "Y": "where the given row starts",
        "Z": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Thecrow_indicestensor consist of?",
        "Y": "compressed row indices",
        "Z": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is Thecrow_indicestensor?",
        "Y": "1-D tensor of sizesize[0]+1",
        "Z": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the last element of Thecrow_indicestensor?",
        "Y": "the number of non-zeros",
        "Z": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thecrow_indicestensor encodes the index invaluesandcol_indicesdepending on what?",
        "Y": "where the given row starts",
        "Z": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Thecol_indicestensor contain?",
        "Y": "column indices of each value",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is Thecol_indicestensor?",
        "Y": "1-D tensor of sizennz",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does thecol_indicestensor contain?",
        "Y": "column indices",
        "Z": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the value of the CSR tensor?",
        "Y": "1-D tensor of sizennz",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What should the index tensorscrow_indicesandcol_indices have?",
        "Y": "element type eithertorch.int64(default) ortorch.int32",
        "Z": "Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the default linking of pytorch being with MKL LP64?",
        "Y": "usetorch.int32",
        "Z": "Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thecol_indicestensor contains the column indices of each value. This is a what?",
        "Y": "1-D tensor",
        "Z": "Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What should the index tensorscrow_indicesandcol_indices have element type?",
        "Y": "eithertorch.int64(default) ortorch.int32",
        "Z": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If you want to use matrix operations, usetorch.int32. This is as a result of the default linking of py",
        "Y": "MKL",
        "Z": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thevaluestensor contains the values of the CSR tensor of sizennz.",
        "Y": "1-D",
        "Z": "Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What should be used if you want to use MKL-enabled matrix operations?",
        "Y": "usetorch.int32",
        "Z": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If you want to use matrix operations, usetorch.int32.",
        "Y": "MKL",
        "Z": "The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is optional and will be deduced from thecrow_indicesandcol_indicesif it is not present?",
        "Y": "Thesizeargument",
        "Z": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is optional and will be deduced from thecrow_indicesandcol_indices if it is not present?",
        "Y": "Thesizeargument",
        "Z": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the only math operation supported on CSR tensors?",
        "Y": "thetensor.matmul()method",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thesizeargument is optional and will be deduced from what if it is not present?",
        "Y": "thecrow_indicesandcol_indices",
        "Z": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be directly constructed by using thetorch._sparse_csr_tensor()method?",
        "Y": "Sparse CSR matrices",
        "Z": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[layout]denote?",
        "Y": "matrix",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of scalar does fdenote?",
        "Y": "float or 0-D PyTorch tensor",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is an example of a PyTorch operation?",
        "Y": "Sparse grad?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the layout signature torch.mv() no?",
        "Y": "M[sparse_coo]@V[strided]->V[strided]",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "HereT[layout]denotes what with a given layout?",
        "Y": "tensor",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a scalar?",
        "Y": "float or 0-D PyTorch tensor",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the layout signature torch.mv()?",
        "Y": "no M[sparse_coo]@V[strided]->V[strided]",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the default value for M[sparse_coo]@V[strided]->V[strided",
        "Y": "no",
        "Z": "no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the grad?",
        "Y": "Sparse",
        "Z": "Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a Sparse grad?",
        "Y": "Layout signature",
        "Z": "Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does mv() no?",
        "Y": "M[sparse_coo]@V[strided]->V[strided] torch",
        "Z": "Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.mv() no M[sparse_coo]@V[strided]->V[stri",
        "Y": "Layout signature",
        "Z": "Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the value of the M[sparse_coo]@V[strided]->V[strided",
        "Y": "no",
        "Z": "torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "M[sparse_coo]@V[strided]->V[strided] torch.mv()",
        "Y": "no",
        "Z": "torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is M[sparse_coo]@V[strided]->V[strided] torch?",
        "Y": "no",
        "Z": "no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo]@V[strided]->V[strided] torch do?",
        "Y": "M[sparse_coo]@V[strided]->V[strided] torch",
        "Z": "M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.mv() do?",
        "Y": "no",
        "Z": "torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the value of torch.mv()?",
        "Y": "no",
        "Z": "torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the command that does not exist?",
        "Y": "M[sparse_csr]@V[strided]->V[strided] torch",
        "Z": "no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the torch that is not sparse_csr?",
        "Y": "M[sparse_csr]@V[strided]->V[strided] torch",
        "Z": "no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the command?",
        "Y": "M[sparse_coo]@M[strided]->M[strided] torch",
        "Z": "M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_csr]@V[strided]->V[strided] torch do",
        "Y": "M[sparse_csr]@V[strided]->V[strided] torch",
        "Z": "M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does matmul() not do?",
        "Y": "M[sparse_csr]@M[strided]->M[strided] torch",
        "Z": "torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the M[sparse_coo]@M[strided]->M[strided",
        "Y": "M[sparse_coo]@M[strided]->M[strided] torch",
        "Z": "no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does matmul() no?",
        "Y": "M[sparse_csr]@M[strided]->M[strided] torch",
        "Z": "M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the default value of M[sparse_csr]@M[strided]->M[stride",
        "Y": "no",
        "Z": "torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Is M[sparse_csr]@M[strided]->M[strided] torch?",
        "Y": "no",
        "Z": "no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_csr]@M[strided]->M[strided] torch?",
        "Y": "M[sparse_csr]@M[strided]->M[strided] torch",
        "Z": "no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where is M[sparse_csr] located?",
        "Y": "M[strided]->M[strided] torch",
        "Z": "M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_csr]@M[strided]->M[strided] torch do",
        "Y": "M[sparse_csr]@M[strided]->M[strided] torch",
        "Z": "M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.mm() not do?",
        "Y": "M[sparse_coo]@M[strided]->M[strided] torch",
        "Z": "torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the component that is not present at the start of a mission?",
        "Y": "M[sparse_coo]",
        "Z": "no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the sparse_coo at M[strided]->M[strided] torch",
        "Y": "M[sparse_coo]@M[strided]->M[strided] torch",
        "Z": "no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo]@M[strided]->M[strided] torch do?",
        "Y": "M[sparse_coo]@M[strided]->M[strided] torch",
        "Z": "M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does smm() no?",
        "Y": "M[sparse_coo]@M[strided]->M[sparse_coo] torch",
        "Z": "torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the command that is used to send a message to a target?",
        "Y": "M[sparse_coo]",
        "Z": "yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the component that is at the end of a mission?",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo]@M[strided]->M[sparse_coo]",
        "Y": "no",
        "Z": "no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does hspmm() no M[sparse_coo]@M[strided]->M[h",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo]@M[strided]->M[hybridsparse",
        "Y": "no",
        "Z": "no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the component that is at the end of a stride?",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does ->M[hybridsparse_coo] torch.bmm() no T[sparse_",
        "Y": "M[sparse_coo]@M[strided]",
        "Z": "M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does svd_lowrank() use?",
        "Y": "SVD",
        "Z": "f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does f*M[sparse_coo]+f*(M[sparse_coo]@M",
        "Y": "no",
        "Z": "no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "All PyTorch operations, excepttorch.smm(), support backward with respect to what?",
        "Y": "strided matrix arguments",
        "Z": "M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the PyTorch operation do to support backward with respect to strided matrix arguments?",
        "Y": "Note",
        "Z": "M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the \u201cSparse grad?\u201d column indicate?",
        "Y": "if the PyTorch operation supports backward with respect to sparse matrix argument",
        "Z": "no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "All PyTorch operations, excepttorch.smm(), support backward with respect to what argument?",
        "Y": "strided matrix",
        "Z": "no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What do all PyTorch operations, excepttorch.smm(), support backward with respect to strided matrix arguments?",
        "Y": "Note",
        "Z": "no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the \"Sparse grad?\" column indicate?",
        "Y": "if the PyTorch operation supports backward with respect to sparse matrix argument",
        "Z": "f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the Tensor use?",
        "Y": "sparse storage layout",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the number of dense dimensions in a sparse tensorself?",
        "Y": "Tensor.dense_dim",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the number of sparse dimensions in a sparse tensorself?",
        "Y": "Tensor.sparse_dim",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The following Tensor methods are related to what tensors?",
        "Y": "sparse",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns the number of dense dimensions in a sparse tensorself?",
        "Y": "Tensor.dense_dim",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_dim return in a sparse tensorself?",
        "Y": "sparse dimensions",
        "Z": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.is_sparse IsTrueif the Tensor uses?",
        "Y": "sparse storage layout",
        "Z": "Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_dim return the number of sparse dimensions in?",
        "Y": "a sparse tensorself",
        "Z": "Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "IsTrue if the Tensor uses sparse storage layout?",
        "Y": "False",
        "Z": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "IsTrue if the Tensor uses what?",
        "Y": "sparse storage layout",
        "Z": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "IsTrueif the Tensor uses sparse storage layout?",
        "Y": "False",
        "Z": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does a tensor use to convert a tensor to compressed row storage format?",
        "Y": "Tensor.indices",
        "Z": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What return the number of sparse dimensions in a sparse tensorself?",
        "Y": "sparse",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What tensor with values from a strided tensorselffiltered by the indices of the sparse ten",
        "Y": "newsparse tensor",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Return the number of dense dimensions in what?",
        "Y": "a sparse tensorself",
        "Z": "Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Return the number of what in a sparse tensorself?",
        "Y": "sparse dimensions",
        "Z": "Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns the number of sparse dimensions in a sparse tensorself?",
        "Y": "Tensor.sparse_dim",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What tensor with values from a strided tensorself filtered by the indices of the sparse",
        "Y": "newsparse tensor",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Return the number of sparse dimensions in what?",
        "Y": "a sparse tensorself",
        "Z": "Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns a strided tensorselffiltered by the indices of the sparse tensormask?",
        "Y": "newsparse tensor",
        "Z": "Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of tensor does Tensor.coalesce return?",
        "Y": "coalesced copy",
        "Z": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_resize_ do?",
        "Y": "Resizesselfsparse tensorto",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What _ Resizes selfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "Y": "Tensor.sparse_resize",
        "Z": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of copy of the tensor is returned?",
        "Y": "sparse",
        "Z": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_resize do?",
        "Y": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions",
        "Z": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the Tensor method that returns the desired size and the number of sparse and dense dimensions?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What _ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "Y": "Tensor.sparse_resize",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the tensor method that resizes itselfsparse tensorto?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned as a coalesced copy of an uncoalesced tensor?",
        "Y": "selfifselfis",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is another name for sparse tensorto?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the desired size and the number of sparse and dense dimensions?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is anuncoalesced tensor?",
        "Y": "selfifselfis",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What do a sparse COO tensor methods return?",
        "Y": "Return the values tensor",
        "Z": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What removes all specified elements from a sparse tensorselfand resizes itselfto the desired size and the number of spar",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does tensor.sparse_resize_and_clear_ Resizesselfto?",
        "Y": "the desired size and the number of sparse and dense dimensions",
        "Z": "Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What removes all specified elements from a sparse tensorselfand resizesselfto the desired size and number of spars",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of copy of self does Tensor.coalesce return?",
        "Y": "coalesced",
        "Z": "Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns true if selfis a sparse COO tensorthat is coalesced?",
        "Y": "coalesced",
        "Z": "Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What resizes selfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "Y": "Tensor.sparse_resize",
        "Z": "Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.is_coalesced return?",
        "Y": "Trueifselfis",
        "Z": "Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of copy of self is returned if selfis anuncoalesced tensor?",
        "Y": "coalesced",
        "Z": "Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned if selfis anuncoalesced tensor?",
        "Y": "Trueifselfis a sparse COO tensorthat",
        "Z": "Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of spar",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "Y": "Tensor.sparse_resize",
        "Z": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns a sparse COO tensorthat is coalesced?",
        "Y": "Trueifselfis",
        "Z": "Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What method is specific tosparse CSR tensors?",
        "Y": "Tensor.crow_indices",
        "Z": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does it do to the desired size and the number of sparse and dense dimensions?",
        "Y": "Resizesselfsparse tensorto",
        "Z": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_resize_and_clear_ resizesselfto?",
        "Y": "the desired size and the number of sparse and dense dimensions",
        "Z": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the method that returns true if selfis a sparse COO tensorthat is coalesced?",
        "Y": "coalesced",
        "Z": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the following methods?",
        "Y": "specific tosparse CSR tensors",
        "Z": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does tensor.sparse_resize_and_clear_ resize selfto?",
        "Y": "the desired size and the number of sparse and dense dimensions",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned when selfis a sparse CSR tensor of layoutsparse_csr?",
        "Y": "tensor containing the compressed row indices of theselftensor",
        "Z": "Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does tensor.crow_indices return when selfis a sparse CSR tensor of layoutspar",
        "Y": "tensor containing the compressed row indices of theselftensor",
        "Z": "Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the tensor return when selfis a sparse CSR tensor of layoutsparse_cs",
        "Y": "column indices of theselftensor",
        "Z": "Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned when a COO tensorthat is coalesced?",
        "Y": "Trueifselfis",
        "Z": "Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "When selfis a sparse CSR tensor of layoutsparse_csr, what is it?",
        "Y": "a sparse CSR tensor",
        "Z": "ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "ReturnsTrueifselfis a sparse COO tensorthat is what?",
        "Y": "coalesced",
        "Z": "ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR",
        "Y": "Tensor.crow_indices",
        "Z": "Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What indices of theselftensor is returned when selfis a sparse CSR tensor of layoutspars",
        "Y": "column",
        "Z": "Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the tensor return whenselfis a sparse CSR tensor of layoutsparse_cs",
        "Y": "column indices of theselftensor",
        "Z": "Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Creates what type of self?",
        "Y": "strided copy",
        "Z": "Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the Tensor.col_indices return whenselfis a sparse CSR tensor of layoutspars",
        "Y": "column indices of theselftensor",
        "Z": "Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of indices does theselftensor return when selfis a sparse CSR tensor of layoutspar",
        "Y": "column",
        "Z": "The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.crow_indices return when selfis a sparse CSR tensor of layoutspars",
        "Y": "the tensor containing the compressed row indices of theselftensor",
        "Z": "Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What indices does theselftensor return when selfis a sparse CSR tensor of layoutsparse",
        "Y": "column",
        "Z": "Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.col_indices return whenselfis a sparse CSR tensor of layoutsparse",
        "Y": "tensor containing the column indices of theselftensor",
        "Z": "Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layouts",
        "Y": "the tensor",
        "Z": "Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the selftensor return when selfis a sparse CSR tensor of layoutsparse_cs",
        "Y": "the tensor containing the column indices",
        "Z": "Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "In what format is the a sparse tensor constructed?",
        "Y": "CSR",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse Tensor inputto the result.   Performs a matrix multiplication of a sparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.sum return?",
        "Y": "sum of each row of the sparse Tensor inputin the given dimensionsdim",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse Tensor inputto the result.   Performs a matrix multiplication of a sparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the sparse.addmm function support for sparse matrixmat1?",
        "Y": "backward",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse Tensor inputto the result.   Performs a matrix multiplication of a sparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the function that supports backward for sparse matrixmat1?",
        "Y": "sparse.mm",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where are specified values in the a sparse tensor?",
        "Y": "givencrow_indicesandcol_indices",
        "Z": "Constructs a sparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The function sparse.addmm supports which direction for sparse matrixmat1?",
        "Y": "backward",
        "Z": "Constructs a sparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.mm perform of the sparse matrixmat1 and the (sparse or strided) matrixmat2",
        "Y": "matrix multiplication",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse Tensor inputto the result.   Performs a matrix multiplication of a sparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the sum of each row of the sparse Tensor inputin the given dimensionsdim?",
        "Y": "sparse",
        "Z": "sparse.sum Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse Tensor inputto the result.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the return value of the sparse Tensor input?",
        "Y": "sum of each row of the sparse Tensor inputin the given dimensionsdim",
        "Z": "Returns the sum of each row of the sparse Tensor inputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse Tensor inputto the result.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What do the followingtorchfunctions support?",
        "Y": "sparse tensors",
        "Z": "The followingtorchfunctions support sparse tensors: cat()dstack()empty()empty_like()hstack()index_select()is_complex()is_floating_point()is_nonzero()is_same_size()is_signed()is_tensor()lobpcg()mm()native_norm()pca_lowrank()select()stack()svd_lowrank()unsqueeze()vstack()zeros()zeros_like()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the equivalent of totorch.full_like?",
        "Y": "totorch.full",
        "Z": "Returns a tensor with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size of inputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "Returns what with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)",
        "Y": "a tensor",
        "Z": "Returns a tensor with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size of inputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What is dtype(torch.dtype, optional)?",
        "Y": "the desired data type of returned Tensor",
        "Z": "Returns a tensor with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size of inputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What does ifNone do?",
        "Y": "defaults to the dtype of input",
        "Z": "Returns a tensor with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size of inputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "Default: ifNone, defaults to what?",
        "Y": "the device of input",
        "Z": "fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype of input. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout of input. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What is the latest version of STFT?",
        "Y": "1.8.0",
        "Z": "Short-time Fourier transform (STFT). Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "When will return_complex=Trueas be used?",
        "Y": "pytorch release",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What version of pytorch does return_complex need to be given explicitly for real inputs?",
        "Y": "1.8.0",
        "Z": "Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What cantorch.view_as_real() be used for?",
        "Y": "to recover a real tensor with an extra last dimension",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What version of pytorch does return_complex always have to be given explicitly for real inputs?",
        "Y": "1.8.0",
        "Z": "Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does return_complex=Trueas do in a future pytorch release?",
        "Y": "return complex tensors",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "In what version of Python must return_complex always be given explicitly for real inputs?",
        "Y": "1.8.0",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What function can be used to recover a real tensor with an extra last dimension for real and imaginary components?",
        "Y": "thattorch.view_as_real()",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "From what version of pytorch did return_complex always be given explicitly for real inputs?",
        "Y": "1.8.0",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What can Torch.view_as_real() be used to recover?",
        "Y": "real tensor",
        "Z": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression:",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does this method compute?",
        "Y": "wheremmmis the index of the sliding window",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4).",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Input must be either a 1-D time sequence or a 2-D batch of time sequences?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is treated as equal tofloor(n_fft/4)?",
        "Y": "Ifhop_lengthisNone",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is wheremmmis?",
        "Y": "the index of the sliding window",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for a time sequence?",
        "Y": "Ifhop_lengthisNone",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4).",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifhop_lengthisNone(default), it is treated as equal what?",
        "Y": "tofloor(n_fft/4)",
        "Z": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthisNone(default), it is treated as equal what?",
        "Y": "ton_fft",
        "Z": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What type of input must be used?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifhop_lengthisNone(default), it is treated as equal to what?",
        "Y": "floor(n_fft/4)",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is treated as equal ton_fft?",
        "Y": "Ifwin_lengthisNone",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "IfwindowisNone(default), what is it treated as?",
        "Y": "if having111everywhere in the window",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthn_ffttextwin_lengthn_fft,windowwill be padded on",
        "Y": "lengthn_fft",
        "Z": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "IfwindowisNone(default), it is treated as if what?",
        "Y": "having111everywhere in the window",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the window padded on both sides to before being applied?",
        "Y": "lengthn_fft",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for a window to be treated as equal tofloor(n_fft/4)?",
        "Y": "Ifhop_lengthisNone",
        "Z": "Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What can a window be of sizewin_length?",
        "Y": "1-D tensor",
        "Z": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthisNone(default), it is treated as what?",
        "Y": "equal ton_fft",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the tensor of sizewin_length?",
        "Y": "1-D",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthisNone(default), what is it treated as?",
        "Y": "equal ton_fft",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthisNone(default), windowwill be padded on both sides to what before being applied?",
        "Y": "lengthn_fft",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What tensor of sizewin_length can a window be?",
        "Y": "1-D",
        "Z": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is treated as if having111everywhere in the window?",
        "Y": "IfwindowisNone",
        "Z": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthn_ffttextwin_lengthn_fftwin_lengthn_ff",
        "Y": "lengthn_fft",
        "Z": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value of a window?",
        "Y": "IfwindowisNone",
        "Z": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Input will be padded on both sides so that thettt-th frame is centered at what?",
        "Y": "IfcenterisTrue",
        "Z": "IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What determines the padding method used oninputwhencenterisTrue?",
        "Y": "pad_mode",
        "Z": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the name of the padding method used oninputwhencenterisTrue?",
        "Y": "Seetorch.nn.functional.pad()",
        "Z": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value of the padding method used oninputwhencenterisTrue?",
        "Y": "reflect",
        "Z": "Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the name of the method that determines the padding method used oninputwhencenterisTrue?",
        "Y": "Seetorch.nn.functional.pad()",
        "Z": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default padding method used oninputwhencenterisTrue?",
        "Y": "reflect",
        "Z": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What makes onesidedoutput not possible?",
        "Y": "if the input or window tensors are complex",
        "Z": "IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does IfnormalizedisTrue return?",
        "Y": "normalized STFT results",
        "Z": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the return value of Ifreturn_complexisTrue?",
        "Y": "ainput.dim()+1dimensional complex tensor",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the output of IfFalse?",
        "Y": "ainput.dim()+2dimensional real tensor",
        "Z": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What function returns the normalized STFT results?",
        "Y": "IfnormalizedisTrue",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifreturn_complexisTrue(default if input is complex) the return is what?",
        "Y": "ainput.dim()+1dimensional complex tensor",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If return_complexisTrue, what is the return?",
        "Y": "ainput.dim()+1dimensional complex tensor",
        "Z": "Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What returns ainput.dim()+1dimensional complex tensor?",
        "Y": "Ifreturn_complexisTrue",
        "Z": "Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default return value for ainput.dim()+2dimensional real tensor?",
        "Y": "IfFalse",
        "Z": "Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If return_complexisTrue(default if input is complex) what is the return?",
        "Y": "ainput.dim()+1dimensional complex tensor",
        "Z": "Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default if input is complex?",
        "Y": "Ifreturn_complexisTrue",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does return if return_complexis true?",
        "Y": "a complex tensor of size",
        "Z": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Returns either a complex tensor of size(NT)(* times N times T",
        "Y": "a real tensor of size",
        "Z": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does TTT represent?",
        "Y": "the total number of frames",
        "Z": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Calling with the previous signature may cause error or return what?",
        "Y": "incorrect result",
        "Z": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional)",
        "Y": "ton_fft",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is window(Tensor,optional)?",
        "Y": "optional window function",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default:None(treated as window of what number) \u2013 the optional window function?",
        "Y": "all111s",
        "Z": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default window of all111s?",
        "Y": "Default:None",
        "Z": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value of the window function?",
        "Y": "True",
        "Z": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is used to center thetttt-th frame?",
        "Y": "padinputon",
        "Z": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default:True pad_mode(string,optional) \u2013 controls the padding method used what?",
        "Y": "whencenterisTrue",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\"",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default name of the padding method used whencenterisTrue?",
        "Y": "\"reflect\"",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\"",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default window function of all111s?",
        "Y": "Default:None",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What controls the padding method used?",
        "Y": "whencenterisTrue",
        "Z": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default whencenterisTrue?",
        "Y": "Default:\"reflect\"",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\"",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the padding method used?",
        "Y": "whencenterisTrue",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Window(Tensor) is what type of window function?",
        "Y": "optional",
        "Z": "window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return what?",
        "Y": "normalized STFT results",
        "Z": "center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is center(bool,optional)?",
        "Y": "whether to padinputon both sides",
        "Z": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default pad_mode?",
        "Y": "whencenterisTrue",
        "Z": "center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What controls the padding method used whencenterisTrue?",
        "Y": "pad_mode",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default to return the normalized STFT results?",
        "Y": "Default:False onesided",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default:True for what?",
        "Y": "realinputandwindow",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is a tensor with an extra last dimension for the real and imaginary components?",
        "Y": "return_complex",
        "Z": "pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default:False onesided(bool,optional) \u2013 controls whether to return how many results to avoid redundancy for real",
        "Y": "half",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for the return of half of results?",
        "Y": "Truefor realinputandwindow",
        "Z": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "A tensor containing the STFT result with what described above Tensor?",
        "Y": "shape",
        "Z": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does false onesided avoid for real inputs?",
        "Y": "redundancy",
        "Z": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What controls whether to return half of results to avoid redundancy for real inputs?",
        "Y": "Default:Truefor realinputandwindow,Falseotherwise",
        "Z": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is dim(int,list of python:int,optional)?",
        "Y": "the dimension or dimensions to approximate the gradient over",
        "Z": "This function is analogous to NumPy\u2019s gradient function. {input}\u2013 spacing(scalar,list of scalar,list of Tensor,optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at(the) \u2013 dim(int,list of python:int,optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order(int,optional) \u2013 unsupported (must be equal to its default value which is 1.) Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "X": "What does spacing(scalar,list of scalar,list of Tensor,optional) represent?",
        "Y": "implicitly or explicitly represents coordinates",
        "Z": "This function is analogous to NumPy\u2019s gradient function. {input}\u2013 spacing(scalar,list of scalar,list of Tensor,optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at(the) \u2013 dim(int,list of python:int,optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order(int,optional) \u2013 unsupported (must be equal to its default value which is 1.) Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "X": "If the number of bins is at leastminlength and If input is  empty, then the result is tensor of sizeminlength",
        "Y": "zeros",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Ifnis the value at what position, out[n]+=weights[i]ifweightsis specified elseout[n]+",
        "Y": "positioni",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=",
        "Y": "Note",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "The number of bins is one larger than the largest value in input unlessinput is empty, in which case the result is a ten",
        "Y": "size 1",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Ifminlengthis specified, the number of bins is at leastminlengthand what?",
        "Y": "If input is  empty",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Ifnis the value at what?",
        "Y": "positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is the size of the bins?",
        "Y": "size 1",
        "Z": "The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Ifminlengthis specified, the number of bins is at leastminlength and If input is  empty, what is the result?",
        "Y": "tensor of sizeminlengthfilled with zeros",
        "Z": "The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Ifnis the value at what position?",
        "Y": "positioni",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What device may produce nondeterministic gradients when given tensors?",
        "Y": "CUDA device",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Where can you find more information about this operation?",
        "Y": "SeeReproducibility",
        "Z": "The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "If the number of bins is at leastminlength and If input is  empty, then the result is a tensor of size",
        "Y": "Ifminlengthis specified",
        "Z": "The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Ifnis the value at what positioni,out[n]+=weights[i]ifweightsis specified elseout[n]",
        "Y": "positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "This operation may produce what when given tensors on a CUDA device?",
        "Y": "nondeterministic gradients",
        "Z": "The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is the minimum number of bins?",
        "Y": "minlength(int)",
        "Z": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What should minlength(int) be?",
        "Y": "non-negative",
        "Z": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is the weight for each value in the input tensor?",
        "Y": "optional",
        "Z": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is a tensor of shapeSize([max(input)+1])?",
        "Y": "If input is  non-empty",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is the name of the file that is used by Alias?",
        "Y": "Alias of torch.vstack()",
        "Z": "Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack"
    },
    {
        "X": "What is the name of the function that is used to create a vstack?",
        "Y": "Alias of torch.vstack()",
        "Z": "Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack"
    },
    {
        "X": "Fillsselftensor with elements samples from the normal distribution parameterized what?",
        "Y": "bymeanandstd",
        "Z": "Fillsselftensor with elements samples from the normal distribution\nparameterized bymeanandstd.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "X": "What is filled with elements samples from the normal distribution parameterized bymeanandstd?",
        "Y": "Fillsselftensor",
        "Z": "Fillsselftensor with elements samples from the normal distribution\nparameterized bymeanandstd.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "X": "What always copiesdata?",
        "Y": "torch.tensor()",
        "Z": "torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.tensor() use if you want to avoid a copy of data?",
        "Y": "NumPyndarray",
        "Z": "Constructs a tensor with data. Warning torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor()always copiesdata?",
        "Y": "Tensordata",
        "Z": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor do you want to avoid a copy of?",
        "Y": "NumPyndarray",
        "Z": "Constructs a tensor with data. Warning torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor use if you want to avoid a copy?",
        "Y": "NumPyndarray",
        "Z": "Warning torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor.as_tensor() use if you want to avoid a copy?",
        "Y": "NumPyndarray",
        "Z": "torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor do if you have a NumPyndarray and want to avoid a copy?",
        "Y": "Warning",
        "Z": "torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What do you need to do to avoid a copy of a Tensordata?",
        "Y": "change itsrequires_gradflag, userequires_grad_()ordetach()",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.as_tensor() use if you want to avoid a copy?",
        "Y": "NumPyndarray",
        "Z": "torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor.as_tensor() do?",
        "Y": "Warning",
        "Z": "torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does Torch.tensor() construct when data is a tensorx?",
        "Y": "leaf variable",
        "Z": "When data is a tensorx,torch.tensor()reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Thereforetorch.tensor(x)is equivalent tox.clone().detach()and torch.tensor(x,requires_grad=True)is equivalent tox.clone().detach().requires_grad_(True).\nThe equivalents usingclone()anddetach()are recommended. data(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What is the equivalent totorch.tensor(x)?",
        "Y": "tox.clone().detach()",
        "Z": "When data is a tensorx,torch.tensor()reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Thereforetorch.tensor(x)is equivalent tox.clone().detach()and torch.tensor(x,requires_grad=True)is equivalent tox.clone().detach().requires_grad_(True).\nThe equivalents usingclone()anddetach()are recommended. data(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What are the equivalents of clone() and detach()?",
        "Y": "usingclone()anddetach()",
        "Z": "When data is a tensorx,torch.tensor()reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Thereforetorch.tensor(x)is equivalent tox.clone().detach()and torch.tensor(x,requires_grad=True)is equivalent tox.clone().detach().requires_grad_(True).\nThe equivalents usingclone()anddetach()are recommended. data(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What can be a list, tuple, NumPyndarray, NumPyndarray, and other types",
        "Y": "scalar",
        "Z": "When data is a tensorx,torch.tensor()reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Thereforetorch.tensor(x)is equivalent tox.clone().detach()and torch.tensor(x,requires_grad=True)is equivalent tox.clone().detach().requires_grad_(True).\nThe equivalents usingclone()anddetach()are recommended. data(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What infers data type fromdata?",
        "Y": "ifNone",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, infers data type fromdata. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What default infers data type fromdata?",
        "Y": "ifNone",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, infers data type fromdata. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What is the name of the pinned memory that is allocated to the returned tensor?",
        "Y": "pin_memory",
        "Z": "requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. pin_memory(bool,optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default:False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does the second row of the arowbycolmatrix contain?",
        "Y": "column coordinates",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "Returns the indices of the upper triangular part of arowbycolmatrix in what format?",
        "Y": "2-by-N Tensor",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the upper triangular part of arowbycolmatrix defined as?",
        "Y": "the elements on and above the diagonal",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "When running on what platform must row*col be less than259259259 to prevent overflow during calculation?",
        "Y": "CUDA",
        "Z": "When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the name of the number of rows in the 2-D matrix?",
        "Y": "row(int)",
        "Z": "row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the default value for offset(int) if not provided?",
        "Y": "Default",
        "Z": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is ifNone,torch.long?",
        "Y": "Default",
        "Z": "row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the default value of offset(int)?",
        "Y": "if not provided",
        "Z": "offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the default if not provided?",
        "Y": "Default",
        "Z": "row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the default value of offset(int) if not provided?",
        "Y": "Default",
        "Z": "offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the layout of a tensor?",
        "Y": "currently only supporttorch.strided",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is an example of a tensor layout?",
        "Y": "Example:",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What has the same sign as the divisorother?",
        "Y": "The remainder",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisorother. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What is used for how division by zero is handled?",
        "Y": "Seetorch.fmod()",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisorother. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What is the input for the dividend other(TensororScalar)?",
        "Y": "Tensor",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisorother. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "Input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(T",
        "Y": "the output tensor",
        "Z": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What computes the element-wise remainder of division equivalently to the C library functionfmod()?",
        "Y": "torch.fmod()",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library functionfmod().",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "The input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(T",
        "Y": "the output tensor",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library functionfmod().",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What is another name forfor torch.div?",
        "Y": "Alias for torch.div()",
        "Z": "Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide"
    },
    {
        "X": "What is the name of the number of rows in a 2-D tensor?",
        "Y": "n(int)",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "Returns a 2-D tensor with what else?",
        "Y": "zeros",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor",
        "Y": "n",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What does n(int) mean?",
        "Y": "the number of rows m(int,optional)",
        "Z": "n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor?",
        "Y": "m",
        "Z": "m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the default beingn out(Tensor,optional)?",
        "Y": "output tensor",
        "Z": "m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the name of Alias for torch.special.erfinv?",
        "Y": "Alias for torch.special.erfinv()",
        "Z": "Alias for torch.special.erfinv().",
        "source": "https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv"
    },
    {
        "X": "What is another name forfor torch.special.erfinv()?",
        "Y": "Alias for torch.special.erfinv()",
        "Z": "Alias for torch.special.erfinv().",
        "source": "https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv"
    },
    {
        "X": "What is the name of the site?",
        "Y": "Alias for torch.special.expm1",
        "Z": "Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1"
    },
    {
        "X": "What does Alias for torch.special.expm1() do?",
        "Y": "Alias for torch.special.expm1()",
        "Z": "Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1"
    },
    {
        "X": "What does seetorch.i0() do?",
        "Y": "Let I_0 be the zeroth order modified Bessel function of the first kind",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)?",
        "Y": "Callingtorch.kaiser_window",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is intended as a helpful shorthand to produce a periodic window as input to functions liketorch.stft()?",
        "Y": "Theperiodicargument",
        "Z": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does theperiodicargument do to produce a periodic window as input to functions liketorch.stft()?",
        "Y": "Note",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is equivalent to callingtorch.kaiser_window?",
        "Y": "Callingtorch.kaiser_window",
        "Z": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "If the returned window is a single element tensor containing a one, what is the result?",
        "Y": "Ifwindow_lengthis one",
        "Z": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is equivalent to callingtorch.kaiser_window(L,B,periodic=True)?",
        "Y": "callingtorch.kaiser_window",
        "Z": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is intended as a useful shorthand to produce a periodic window as input to functions liketorch.stft()?",
        "Y": "Theperiodicargument",
        "Z": "Computes the Kaiser window with window length window_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is window_length(int)?",
        "Y": "length of the window",
        "Z": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the returned window ifwindow_lengthis one?",
        "Y": "a single element tensor containing a one",
        "Z": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does window_length(int) mean?",
        "Y": "length of the window",
        "Z": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does beta(float,optional) provide for the window?",
        "Y": "shape parameter",
        "Z": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does window_length(int) represent?",
        "Y": "length of the window",
        "Z": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Beta(float,optional) \u2013 what parameter for the window?",
        "Y": "shape parameter",
        "Z": "periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the returned window if window_lengthis one?",
        "Y": "a single element tensor containing a one",
        "Z": "Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Ifwindow_lengthis one, then the returned window is what?",
        "Y": "a single element tensor containing a one",
        "Z": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What returns a periodic window suitable for use in spectral analysis?",
        "Y": "If True",
        "Z": "periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "If False, returns a window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window.",
        "Y": "symmetric",
        "Z": "Computes the Kaiser window with window length window_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the term for a dense layout?",
        "Y": "Onlytorch.strided",
        "Z": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is supported for a dense layout?",
        "Y": "Onlytorch.strided",
        "Z": "beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What parameter does beta(float,optional) provide for the window?",
        "Y": "shape parameter",
        "Z": "beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the default value of the window tensor?",
        "Y": "ifNone",
        "Z": "beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What type of tensor does ifNone use?",
        "Y": "Default",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the function that computes the then-th power of a square matrix for an integern?",
        "Y": "Computes then-th power of a square matrix for an integern",
        "Z": "Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "How does reordering the multiplications work?",
        "Y": "multiplies two or more matrices",
        "Z": "Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "Computes what power of a square matrix for an integern?",
        "Y": "then-th power",
        "Z": "Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "How does this function work?",
        "Y": "Efficiently multiplies two or more matrices",
        "Z": "Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "What is the result of the multiplicative inverse of of torch.tensordot()?",
        "Y": "the solutionXto the systemtorch.tensordot(A, X) = B",
        "Z": "Computes the multiplicative inverse of torch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "Computes the inverse of torch.tensordot()?",
        "Y": "multiplicative",
        "Z": "Computes the multiplicative inverse of torch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "Computes the solutionXto what?",
        "Y": "systemtorch.tensordot",
        "Z": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "What type of tensor is constructed in COO(rdinate) format?",
        "Y": "a sparse tensor",
        "Z": "Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices. Note This function returns anuncoalesced tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "Constructs a sparse tensor in COO(rdinate) format with what?",
        "Y": "specified values at the given indices",
        "Z": "Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Can be a list, NumPyndarray, scalar, and what other types?",
        "Y": "tuple, NumPyndarray, scalar, and other types",
        "Z": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to a torch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What will the indices be cast to internally?",
        "Y": "a torch.LongTensor",
        "Z": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to a torch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is the second dimension of the indices?",
        "Y": "the number of non-zero values",
        "Z": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to a torch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "indices(array_like) \u2013 Initial data for the tensor. Can be what?",
        "Y": "a list, tuple, NumPyndarray, scalar, and other types",
        "Z": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to a torch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "Where will the indices be cast?",
        "Y": "a torch.LongTensorinternally",
        "Z": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to a torch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What are initial values for the tensor?",
        "Y": "values",
        "Z": "values(array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What are some types of values?",
        "Y": "a list, tuple, NumPyndarray, scalar",
        "Z": "values(array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "Default: if what, infers data type fromvalues?",
        "Y": "None",
        "Z": "size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What infers data type fromvalues?",
        "Y": "if None",
        "Z": "size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "Default: if what, infers data type fromvalues. device(torch.device, optional) \u2013 the desired",
        "Y": "None",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is the tensor to compute OR with out(Tensor,optional) \u2013 the output tensor?",
        "Y": "Example",
        "Z": "Computes the element-wise logical OR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute OR with out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "X": "What is the solution to the system of linear equations represented byAX=BAX = BAX=Band?",
        "Y": "LU factorization of A",
        "Z": "This function returns the solution to the system of linear\nequations represented byAX=BAX = BAX=Band the LU factorization of\nA, in order as a named tuplesolution, LU. LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band?",
        "Y": "LUcontainsLandUfactors",
        "Z": "This function returns the solution to the system of linear\nequations represented byAX=BAX = BAX=Band the LU factorization of\nA, in order as a named tuplesolution, LU. LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What types of inputs does torch.solve(B, A)support?",
        "Y": "real-valued and complex-valued inputs",
        "Z": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does this function return the solution to?",
        "Y": "the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A",
        "Z": "This function returns the solution to the system of linear\nequations represented byAX=BAX = BAX=Band the LU factorization of\nA, in order as a named tuplesolution, LU. LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does LUcontainsLandUfactors do for LU factorization ofA?",
        "Y": "LUcontainsLandUfactors",
        "Z": "This function returns the solution to the system of linear\nequations represented byAX=BAX = BAX=Band the LU factorization of\nA, in order as a named tuplesolution, LU. LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is LUcontainsLandUfactors for?",
        "Y": "LU factorization ofA",
        "Z": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does LUcontainsLandUfactors for LU factorization ofA?",
        "Y": "LUcontainsLandUfactors",
        "Z": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does LUcontainsLandUfactors for?",
        "Y": "Warning",
        "Z": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does torch.solve(B, A)notify when inputs are batches of 2D matrices?",
        "Y": "Warning",
        "Z": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does torch.linalg.solve() use to get the LU factorization?",
        "Y": "seetorch.lu()",
        "Z": "torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What should be replaced with X=torch.solve(B,A)?",
        "Y": "Note",
        "Z": "torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What should be replaced with X=torch.solve(B,A).solution?",
        "Y": "Note",
        "Z": "X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the name of the strides that will be transposed?",
        "Y": "A.contiguous().transpose(-1, -2)",
        "Z": "X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does input(Tensor) represent?",
        "Y": "zero or more batch dimensions",
        "Z": "X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "A(Tensor) \u2013 input square matrix of size(,m,m)(*, m, m)(",
        "Y": "zero or more batch dimensions",
        "Z": "X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does the cublasHandle_t pointer to current cuBLAS handle return?",
        "Y": "the currently selectedStreamfor a given device",
        "Z": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is the defaultStream for a given device?",
        "Y": "defaultStreamfor a given device",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is returned when the index of a currently selected device is returned?",
        "Y": "currently selectedStreamfor a given device",
        "Z": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does CUDA semantics do?",
        "Y": "Checks if peer access between two devices is possible",
        "Z": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns the what?",
        "Y": "currently selectedStreamfor a given device",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns the currently selectedStreamfor a given device.",
        "Y": "defaultStreamfor a given device",
        "Z": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does cublasHandle_t return?",
        "Y": "the currently selectedStreamfor a given device",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does cublasHandle_t pointer to return?",
        "Y": "defaultStreamfor a given device",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What stream does cublasHandle_t return for a given device?",
        "Y": "defaultStream",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does the index of a currently selected device return?",
        "Y": "currently selectedStreamfor a given device",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns the index of a currently selected device. Returns the what?",
        "Y": "currently selectedStreamfor a given device",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns the currently selectedStreamfor a given device. Returns what?",
        "Y": "defaultStreamfor a given device",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does the currentStream for a given device return?",
        "Y": "the currently selectedStreamfor a given device",
        "Z": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns the index of a currently selected device.",
        "Y": "currently selectedStreamfor a given device",
        "Z": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What gencode flags does this library return?",
        "Y": "NVCC",
        "Z": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Releases what currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-s",
        "Y": "all unoccupied cached memory",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What type of memory allocator statistics does this return a dictionary of?",
        "Y": "CUDA",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does it do to all unoccupied cached memory currently held by the caching allocator?",
        "Y": "Releases all unoccupied cached memory currently held by the caching allocator",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does seememory_reserved do?",
        "Y": "Set memory fraction for a process",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is seememory_reserved()?",
        "Y": "Deprecated",
        "Z": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What function is deprecated?",
        "Y": "seemax_memory_reserved()",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Set memory fraction for a process. Deprecated; what?",
        "Y": "seememory_reserved()",
        "Z": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is the name of the function that returns the maximum GPU memory occupied by tensors in bytes for a given device?",
        "Y": "seemax_memory_reserved()",
        "Z": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is seememory_reserved?",
        "Y": "Deprecated",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does seememory_reserved do for a process?",
        "Y": "Set memory fraction",
        "Z": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does this function do for a process?",
        "Y": "Set memory fraction",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is the name of the function that sets memory fraction for a process?",
        "Y": "seememory_reserved()",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is the name of the function that returns the maximum GPU memory managed by the caching allocator for a given device?",
        "Y": "seemax_memory_reserved()",
        "Z": "Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What function provides greater precision than the exponential of the elements minus 1 of input?",
        "Y": "exp(x) - 1",
        "Z": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the function that computes the exponential of the elements minus 1 of input?",
        "Y": "base two exponential function of input",
        "Z": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "input is clamped to what when eps is not None?",
        "Y": "eps",
        "Z": "Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "If increasing is true, the order of the columns is reversedx0,x1,...,x(N1),x(N2)",
        "Y": "True",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What type of input tensor is x(Tensor)?",
        "Y": "1-D",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is generated by the output matrix?",
        "Y": "Vandermonde matrix",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What are the powers of the input vectorx(N1),x(N2),...,x0x(N-1)",
        "Y": "elementwise",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is True, the order of the columns of the output matrix is what?",
        "Y": "reversed",
        "Z": "The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "For whom is a Vandermonde matrix named?",
        "Y": "Alexandre-Theophile Vandermonde",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is x(Tensor)?",
        "Y": "1-D input tensor",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is true, the order of the columns is reversedx0,x1,...,x(N1)x0, x",
        "Y": "True",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is the name of the matrix with a geometric progression in each row?",
        "Y": "Alexandre-Theophile Vandermonde",
        "Z": "The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What does N(int,optional) mean?",
        "Y": "Number of columns in the output",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is increasing(bool,optional)?",
        "Y": "Order of the powers of the columns",
        "Z": "increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is N(int,optional)?",
        "Y": "Number of columns in the output",
        "Z": "N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is the square array returned if N is not specified?",
        "Y": "N=len(x)",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is true, the columns arex0,x1,...,x(N-1)x0,x1,...,x(N-",
        "Y": "True",
        "Z": "increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is the name of the matrix where the powers increase from left to right?",
        "Y": "Vandermonde matrix",
        "Z": "increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is what, the first column isx(N1)x(N-1)x(N1)?",
        "Y": "False",
        "Z": "increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is an example of a Vandermonde matrix?",
        "Y": "Tensor",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What does it do to a mantissa and exponent tensor?",
        "Y": "Decomposesinputinto mantissa and exponent tensors",
        "Z": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "What type of inputs does mantissa support?",
        "Y": "float inputs",
        "Z": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "What is the input tensor out(tuple,optional)?",
        "Y": "output tensors",
        "Z": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "What does inputinto mantissa and exponent tensors do?",
        "Y": "Decomposes",
        "Z": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "What type of inputs does the mantissa support?",
        "Y": "float inputs",
        "Z": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "What is the name of the function that performs the element-wise division of often s or 1 byte n s or 2?",
        "Y": "Warning",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2,\nmultiply the result by the scalar valueand add it toinput. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What does the element-wise division of often s or 1 byte n s or 2 do?",
        "Y": "Warning",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2,\nmultiply the result by the scalar valueand add it toinput. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "The input,tensor1, andtensor2must bebroadcastable.",
        "Y": "shapes",
        "Z": "The shapes of input,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "For inputs of typeFloatTensororDoubleTensor,valuemust be a what?",
        "Y": "real number",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2,\nmultiply the result by the scalar valueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What is the tensor to be added tensor1(Tensor)?",
        "Y": "input(Tensor)",
        "Z": "The shapes of input,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What encapsulates an asynchronous execution and a set of utility functions to simplify operations on Futureobjects?",
        "Y": "aFuturetype",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is theFuturetype primarily used by?",
        "Y": "theDistributed RPC Framework",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What wrapper encapsulates an asynchronous execution of a callable?",
        "Y": "a torch._C.Future",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the package expose a set of APIs for?",
        "Y": "add callback functions and set results",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the Futuretype primarily used by?",
        "Y": "theDistributed RPC Framework",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a torch._C.Future expose to add callback functions and set results?",
        "Y": "APIs",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the wrapper expose a set of APIs for?",
        "Y": "add callback functions and set results",
        "Z": "Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a torch._C.Future expose?",
        "Y": "APIs",
        "Z": "Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will be run when theFutureis completed?",
        "Y": "Append the given callback function to thisFuture",
        "Z": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What can be added to the sameFuture, but the order in which they will be executed cannot be guaranteed?",
        "Y": "Multiple callbacks",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If thisFutureis already completed, the given callback will be run what?",
        "Y": "inline",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What cannot be guaranteed when multiple callbacks are added to the sameFuture?",
        "Y": "the order in which they will be executed",
        "Z": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "The callback must take one argument, which is the reference to what?",
        "Y": "thisFuture",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will the callback function be run when theFutureis completed?",
        "Y": "thisFuture",
        "Z": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If thisFutureis already completed, the given callback will be run what way?",
        "Y": "immediately inline",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does this method behave in the same way asthen()?",
        "Y": "GPU tensors",
        "Z": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the reference to the Future?",
        "Y": "aCallable",
        "Z": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does aCallable that takes in one argument, is the reference to this Future?",
        "Y": "Note",
        "Z": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is aCallable that takes in one argument, is the reference to this Future?",
        "Y": "callback(Future)",
        "Z": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does callback(Future) take in one argument, is the reference to this Future?",
        "Y": "Note",
        "Z": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What must be carefully taken care of if the callback function throws an error?",
        "Y": "callback function throws",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does Future.done() return if it has a result or an exception?",
        "Y": "ReturnTrueif thisFutureis done",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What if it has a result or an exception?",
        "Y": "AFutureis done",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is done if it has a result or an exception?",
        "Y": "AFuture",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven what?",
        "Y": "if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device",
        "Z": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs,Future.done() will returnTrueeven what?",
        "Y": "if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device",
        "Z": "If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will mark thisFutureas completed with an error and trigger all attached callbacks?",
        "Y": "thisFuture",
        "Z": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When calling wait()/value() on thisFuture, the exception set here will be raised what way?",
        "Y": "inline",
        "Z": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the exception for thisFuture?",
        "Y": "aFuturecannot be marked completed twice",
        "Z": "result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will mark thisFutureas completed and trigger all attached callbacks?",
        "Y": "Set the result for thisFuture",
        "Z": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does thisFuture not mark completed twice?",
        "Y": "aFuturecannot be marked completed twice",
        "Z": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How many times can aFuture not be marked completed?",
        "Y": "twice",
        "Z": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Is aFuture able to be marked twice?",
        "Y": "aFuturecannot be marked completed twice",
        "Z": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the result object of thisFuture?",
        "Y": "result(object)",
        "Z": "result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What takes thisFutureas the only argument?",
        "Y": "aCallable",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is aCallable that takes thisFutureas the only argument?",
        "Y": "callback",
        "Z": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "A newFutureobject that holds what will be marked as completed when the givencallbackfinishes?",
        "Y": "return value",
        "Z": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does aCallable that takes thisFutureas the only argument do?",
        "Y": "Note",
        "Z": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What argument does callback(Callable) take as the only argument?",
        "Y": "thisFuture",
        "Z": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "A newFutureobject that holds the return value of thecallback will be marked as what when the givencallbackfinishes?",
        "Y": "completed",
        "Z": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What do callback(Callable) and callback(Callable) take?",
        "Y": "Note",
        "Z": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When will a newFutureobject that holds the return value of thecallbackand be marked as completed?",
        "Y": "when the givencallbackfinishes",
        "Z": "A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What do you need to do to mark a newFutureobject as completed when the givencallbackfinishes?",
        "Y": "Note",
        "Z": "A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens if the callback function throws?",
        "Y": "the future returned bythenwill be marked appropriately with the encountered error",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens if a callback function throws an error?",
        "Y": "the future returned bythenwill be marked appropriately",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens in other cases?",
        "Y": "thisFuturemay not yet hold a value and callingvalue()could fail",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains what that reside on GPUs, this method willnotperform any additional synchronization?",
        "Y": "tensors",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Where is the synchronization of callbacks already taken care of?",
        "Y": "bythen()",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could cause callingvalue() to fail?",
        "Y": "thisFuturemay not yet hold a value",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, this method will not perform what?",
        "Y": "synchronization",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is already being taken care of within callbacks?",
        "Y": "bythen()",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens when callingvalue() fails?",
        "Y": "thisFuturemay not yet hold a value and callingvalue()could fail",
        "Z": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How is synchronization taken care of within callbacks?",
        "Y": "bythen()",
        "Z": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Where should this method be called after a call towait() has completed?",
        "Y": "inside a callback function passed tothen()",
        "Z": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, then this method willnot perform any additional synchronization. This should be done",
        "Y": "separately",
        "Z": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the value contain that reside on GPUs?",
        "Y": "tensors",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the callback for which towait() is already taken care of?",
        "Y": "bythen()",
        "Z": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What should be done until the value of thisFuture is ready?",
        "Y": "Block",
        "Z": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the method that performs the synchronization of tensors that reside on GPUs?",
        "Y": "towait()",
        "Z": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When does thisvalue() method block?",
        "Y": "until the value of thisFutureis ready",
        "Z": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What method throws an error if the function creating the value has thrown an error?",
        "Y": "thisvalue()method",
        "Z": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How long until the value of thisFutureis ready?",
        "Y": "Block",
        "Z": "Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How long does it take to block the value of thisFuture?",
        "Y": "until the value of thisFutureis ready",
        "Z": "Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Collects the providedFutureobjects into what kind of Future?",
        "Y": "combined",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a list of Futureobjects?",
        "Y": "futures(list)",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Returns aFutureobject to a list of the passed in what?",
        "Y": "Futures",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What holds the value held by?",
        "Y": "thisFuture",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does thiswaitmethod do when all of the sub-futures are completed?",
        "Y": "Collects the providedFutureobjects into a single combinedFuture",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Collects the providedFutureobjects into a single what?",
        "Y": "combinedFuture",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does futures(list) return to a list of the passed in Futures?",
        "Y": "aFutureobject",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is futures(list)?",
        "Y": "a list ofFutureobjects",
        "Z": "futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When is a combinedFuture completed?",
        "Y": "when all of the sub-futures are completed",
        "Z": "Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does futures(list) do?",
        "Y": "Waits for all provided futures to be complete",
        "Z": "futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If any futures encounter an error, the method will do what?",
        "Y": "exit early",
        "Z": "Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does futures(list) return?",
        "Y": "a list ofFutureobject",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "The method will throw an error ifwaiton what?",
        "Y": "anyFuturethrows",
        "Z": "futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What can the dividend and divisor contain?",
        "Y": "both for integer and floating point numbers",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What does the dividend and divisor support?",
        "Y": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "When the divisor is what, returnsNaN for floating point dtypes on both CPU and GPU?",
        "Y": "zero",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What is the dividend other(TensororScalar)?",
        "Y": "input(Tensor)",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What happens when the divisor is zero on CPU?",
        "Y": "raisesRuntimeError",
        "Z": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What is the dividend other(TensororScalar) \u2013 the divisor?",
        "Y": "Tensor",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What is the divisor out(Tensor,optional) a part of?",
        "Y": "the output tensor",
        "Z": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What does p(float,optional) return?",
        "Y": "the norm to be computed",
        "Z": "Returns the p-norm of (input-other) The shapes of inputandothermust bebroadcastable. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the Right-hand-side input tensor p(float,optional) \u2013 the norm to be computed Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"
    },
    {
        "X": "What is another name for Alias for torch.mul?",
        "Y": "Alias for torch.mul",
        "Z": "Alias for torch.mul().",
        "source": "https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"
    },
    {
        "X": "What does Alias for torch.mul() do?",
        "Y": "Alias for torch.mul()",
        "Z": "Alias for torch.mul().",
        "source": "https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"
    },
    {
        "X": "What is the function that calculates pointwiselog(ex+ey)logleft(ex + ey",
        "Y": "Logarithm of the sum of exponentiations of the inputs",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What function performs a reduction on a single tensor?",
        "Y": "with torch.logsumexp()",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What is an example of a logarithm of the sum of exponentiations of the inputs?",
        "Y": "Example",
        "Z": "Logarithm of the sum of exponentiations of the inputs. Calculates pointwiselog\u2061(ex+ey)\\log\\left(e^x + e^y\\right)log(ex+ey). This function is useful\nin statistics where the calculated probabilities of events may be so small as to\nexceed the range of normal floating point numbers. In such cases the logarithm\nof the calculated probability is stored. This function allows adding\nprobabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp()which performs a\nreduction on a single tensor. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "What is the only way NaN's are considered equal to each other?",
        "Y": "ifequal_nanisTrue",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  what type of valued, they are considered close if both their real and imaginary components are considered close according to the definition above",
        "Y": "complex",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is considered close If actual and expected are  real-valued and finite?",
        "Y": "Asserts",
        "Z": "Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  valued, they are considered close if both their real and imaginary components are considered close according to the definition above.",
        "Y": "complex",
        "Z": "Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are actual and expected considered close if their structure matches and all their elements are considered close according to the above definition?",
        "Y": "be Sequence\u2019s or Mapping\u2019s",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does if check_device is True stand for?",
        "Y": "same device",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  complex-valued, they are considered close if both their real and imaginary components are considered close according to the definition above.",
        "Y": "If actual and expected are  complex-valued",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What can be Sequence's or Mapping's?",
        "Y": "be Sequence\u2019s or Mapping\u2019s",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Actual(Any) \u2013 what?",
        "Y": "Actual input",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If what must also be specified, default values based on the dtype are selected with the below table?",
        "Y": "specifiedatol",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are selected with the below table if specifiedatol is omitted?",
        "Y": "default values based on the dtype",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What must also be specified if actual and expected can  be Tensor's are considered close if both their real and imaginary components are considered",
        "Y": "specifiedatol",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are selected with the below table if specified atol must also be specified?",
        "Y": "default values based on the dtype",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If atol(Optional[float])must also be specified, what must also be specified?",
        "Y": "specified rtol",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are selected with the below table?",
        "Y": "default values based on the dtype",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What else can actual and expected be?",
        "Y": "be Sequence\u2019s or Mapping\u2019s",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What must also be specified if rtol(Optional[float]) is omitted?",
        "Y": "specifiedatol",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is selected with the below table?",
        "Y": "default values based on the dtype",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does union[bool,str] stand for?",
        "Y": "equal_nan",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If \"relaxed\" complex values are considered as NaN if what component is NaN?",
        "Y": "real or imaginary component",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if specified atol must also be specified?",
        "Y": "default values based on the dtype are selected with the below table",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is selected with the below table if specified rtol must also be specified?",
        "Y": "default values based on the dtype",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If check_device(bool) is disabled, what are moved to the CPU before being compared?",
        "Y": "tensors on differentdevice\u2019s",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If True(default) asserts that corresponding tensors have what?",
        "Y": "same dtype",
        "Z": "actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If check_dtype(bool) is disabled, which tensors are promoted to a common dtype?",
        "Y": "tensors with different type\u2019s",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "rtol(Optional[float]) \u2013 What is rtol?",
        "Y": "Relative tolerance",
        "Z": "expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does rtol(Optional[float]) do?",
        "Y": "specified atol must also be specified",
        "Z": "expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If check_device(bool) is disabled, what happens to tensors on differentdevice's before being compared?",
        "Y": "tensors on differentdevice\u2019s are moved to the CPU",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does check_dtype(bool) assert?",
        "Y": "corresponding tensors have the same dtype",
        "Z": "expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Check_dtype(bool) \u2013 If True(default) asserts that corresponding tensors have what?",
        "Y": "same dtype",
        "Z": "rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are selected with the below table if specified rtol must also be specified?",
        "Y": "default values based on the dtype",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "When are complex values considered as NaN ?",
        "Y": "if either the real or imaginary component is NaN",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If True(default) asserts that what?",
        "Y": "corresponding tensors have the same dtype",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are promoted to a common dtype if check_dtype(bool) is disabled?",
        "Y": "tensors with different type\u2019s",
        "Z": "atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "msg(Optional[Union[str,Callable[[Tensor,Tensor,Diagno",
        "Y": "Optional error message",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What do you need to know about the mismatching tensors?",
        "Y": "details",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What can't be constructed from an array-or-scalar-like?",
        "Y": "a torch.Tensor",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If any tensor is what?",
        "Y": "quantized or sparse",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are the inputs if their set of keys do not match?",
        "Y": "Mapping\u2019s",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If corresponding tensors are not on the same device, what is an example of an Assertion Error?",
        "Y": "If Check_device",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if the inputs are Mapping\u2019s, but their set of keys do not match?",
        "Y": "corresponding tensors do not have the same shape",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the name of the corresponding tensor that does not have the same dtype?",
        "Y": "If Check_dtype",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Which tensor does not have the same stride?",
        "Y": "If Check_stride",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is a temporary restriction and will be relaxed in the future?",
        "Y": "a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like",
        "Z": "UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the UsageError if any tensor is quantized or sparse?",
        "Y": "temporary restriction",
        "Z": "msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is an example of an Assertion Error if corresponding tensors are not on the same device?",
        "Y": "If Check_device",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes:",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the default rtol and atol for different type's?",
        "Y": "dtype rtol atol float16",
        "Z": "UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if the inputs are Sequence\u2019s, but their length does not match?",
        "Y": "If the inputs are Sequence\u2019s, but their length does not match",
        "Z": "UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does the following table display for different type\u2019s?",
        "Y": "default rtol and atol",
        "Z": "Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes:",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the value of the default rtol and atol?",
        "Y": "0.0 0.0",
        "Z": "Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If the inputs are Mapping's, but what do they do not match?",
        "Y": "their set of keys do not match",
        "Z": "Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "In what case does the dtype refer to the promoted type?",
        "Y": "case actual and expected do not have the same dtype",
        "Z": "The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs. max_rel_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest relative difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "The namespace of diagnostic information that will be passed to msg if its a callable has what attribute?",
        "Y": "number_of_elements",
        "Z": "Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the namespace of diagnostic information that will be passed to msg if its a callable has the following attributes?",
        "Y": "total_mismatches",
        "Z": "Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the name for total mismatches divided by number of elements?",
        "Y": "mismatch_ratio",
        "Z": "Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does max_abs_diff_idx represent?",
        "Y": "Index of greatest absolute difference",
        "Z": "Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What if the values of corresponding tensors are not close?",
        "Y": "Assertion Error",
        "Z": "Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does max_abs_diff_idx(Union[int, Tuple[int,...]) represent?",
        "Y": "Index of greatest absolute difference",
        "Z": "Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does max_rel_diff(Union[int, float]) represent?",
        "Y": "Greatest relative difference of the inputs",
        "Z": "Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does max_abs_diff_idx(Union[int, Tuple[int,...]]) represent?",
        "Y": "Index of greatest absolute difference",
        "Z": "The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs. max_rel_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest relative difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does max_rel_diff(Union[int, float]) represent of the inputs?",
        "Y": "Greatest relative difference",
        "Z": "The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs. max_rel_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest relative difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does max_rel_diff_idx(Union[int, Tuple[int,...]]) represent",
        "Y": "Index of greatest relative difference",
        "Z": "The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared. total_mismatches(int): Total number of mismatches. mismatch_ratio(float): Total mismatches divided by number of elements. max_abs_diff(Union[int, float]): Greatest absolute difference of the inputs. max_abs_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest absolute difference. max_rel_diff(Union[int, float]): Greatest relative difference of the inputs. max_rel_diff_idx(Union[int, Tuple[int, \u2026]]): Index of greatest relative difference.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If input is  a matrix (2-D tensor), then returns a what tensor with the elements of input?",
        "Y": "1-D",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What is the main diagonal?",
        "Y": "If diagonal > 0, it is above the main diagonal",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "If diagonal 0, it is what?",
        "Y": "below the main diagonal",
        "Z": "If input is  a vector (1-D tensor), then returns a 2-D square tensor\nwith the elements of inputas the diagonal. If input is  a matrix (2-D tensor), then returns a 1-D tensor with\nthe diagonal elements of input. The argument diagonal controls which diagonal to consider: If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal. input(Tensor) \u2013 the input tensor. diagonal(int,optional) \u2013 the diagonal to consider out(Tensor,optional) \u2013 the output tensor. See also torch.diagonal()always returns the diagonal of its input. torch.diagflat()always constructs a tensor with diagonal elements\nspecified by the input. Examples: Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix:",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "What does the torch package return if obj is a PyTorch tensor?",
        "Y": "PyTorch storage object",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what if obj is a PyTorch tensor?",
        "Y": "True",
        "Z": "Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns True if the inputs a single element tensor which is not equal to zero after type conversions?",
        "Y": "if the inputs a single element tensor",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Returns the default torch.Tensortype return?",
        "Y": "total number of elements in the input tensor",
        "Z": "Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Constructs a sparse tensor in COO(rdinate) formatwith what?",
        "Y": "specified values at the given indices",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the default torch.Tensortype?",
        "Y": "current default floating point torch.dtype",
        "Z": "Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned by the default torch.Tensortype to floating point tensor typet?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the a sparse tensor in COO(rdinate) format contain?",
        "Y": "specified values at the given indices",
        "Z": "Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What tensor of size end start step left l ceil fractextend - text",
        "Y": "1-D",
        "Z": "Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the Tensor input that creates a view of an existing torch.Tensor input",
        "Y": "specified size,stride and storage_offset",
        "Z": "Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the tensor of size end start step left l ceil fractextend -",
        "Y": "1-D",
        "Z": "Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of tensor does a numpy.ndarray create?",
        "Y": "one-dimensional",
        "Z": "Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is filled with the scalar value 0?",
        "Y": "a tensor",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a what size tensor of size end start step left l ceil fractextend",
        "Y": "1-D",
        "Z": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element in input .",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What happens to the given sequence of seq tensor in the given dimension?",
        "Y": "Concatenates",
        "Z": "Concatenates the given sequence of seq tensor in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Splits input, a tensor with three or more dimensions, into what according toindices_or_sections?",
        "Y": "multiple tensors depth wise",
        "Z": "Concatenates the given sequence of seq tensor in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Splits input, a tensor with one or more dimensions, into what according toindices_or_sections?",
        "Y": "multiple tensors horizontally",
        "Z": "Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What function moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination?",
        "Y": "Alias for torch.movedim()",
        "Z": "Concatenates the given sequence of seq tensor in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that moves the dimension(s) of in out at the  position(s) in source to the position(s)",
        "Y": "Alias for torch.movedim()",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What do tensors do in sequence depth wise?",
        "Y": "Stack tensors",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to?",
        "Y": "indices_or_sections",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Gathers values along what specified by dim?",
        "Y": "axis",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Which version of torch.Tensor.scatter_add_() Splits the tensor into chunks?",
        "Y": "Out-of-place version of torch.Tensor.scatter_()",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What happens when a tensor is returned with all the dimensions of inputof size1removed?",
        "Y": "Concatenates a sequence of tensors along a new dimension",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What function returns a tensor with all the dimensions of inputof size1removed?",
        "Y": "Alias for torch.transpose()",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.transpose() call?",
        "Y": "Alias for torch.transpose()",
        "Z": "Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What dimensions does Alias for torch.transpose() transpose?",
        "Y": "0 and 1",
        "Z": "Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.movedim() do?",
        "Y": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination",
        "Z": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what with all the dimensions of inputof size1removed?",
        "Y": "a tensor",
        "Z": "Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor with the elements of in out atwhat?",
        "Y": "given indices",
        "Z": "Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that returns a tensor with all the dimensions of input of size1removed?",
        "Y": "Alias for torch.transpose()",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor with the elements of in out at the  given what?",
        "Y": "indices",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Returns a tensor that is a transposed version of input do?",
        "Y": "Removes a tensor dimension",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a tensor filled with random integers generated uniformly between low(inclusive) and what (exclusive)?",
        "Y": "high",
        "Z": "Returns the random number generator state as a torch.Byte Tensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Each element sampled from what distribution with rate parameter given by the corresponding element in input i.e., Returns a tens",
        "Y": "Poisson",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned when a tensor is filled with random integers generated uniformly between low(inclusive) and high(exclusive)?",
        "Y": "tensor with the same shape",
        "Z": "Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive",
        "Y": "a tensor",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Alias for torch.acos(). Returns a new tensor with what cosine of the elements of",
        "Y": "inverse hyperbolic",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor with the inverse hyperbolic sine of the elements of input. Alias fortorch",
        "Y": "arctangent",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the element-wise division performed by Alias for torch.acosh()?",
        "Y": "often s or 1 byte n s or 2",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Alias for torch.asinh(). Returns a new tensor with the inverse hyperbolic sine",
        "Y": "arctangent",
        "Z": "Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the element-wise multiplication performed by Alias for torch.acos?",
        "Y": "often s or 1 byte n s or 2",
        "Z": "Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that adds the scalar other to each element of the input input?",
        "Y": "Alias for torch.acosh()",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the bitwise OR of inputandother. Computes the bitwise XOR of inputandother.",
        "Y": "AND",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the bitwise what of inputandother?",
        "Y": "OR",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Computes the bitwise OR of inputandother?",
        "Y": "Computes the bitwise OR of inputandother",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.clamp() clamp?",
        "Y": "all elements in input into the range[min,max]",
        "Z": "Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that clamps all elements in input into the range[min,max]?",
        "Y": "Alias for torch.clamp()",
        "Z": "Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the bitwise NOT of the given input tensor. Computes the bitwise OR of inputandother?",
        "Y": "Computes the bitwise OR of inputandother",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What computes the element-wise conjugate of the given input tensor?",
        "Y": "Alias for torch.clamp()",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.clamp() compute?",
        "Y": "the element-wise conjugate of the giveninput tensor",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What computes the element-wise conjugate of the giveninput tensor?",
        "Y": "Alias for torch.clamp()",
        "Z": "Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.clamp() create a new floating-point tensor with?",
        "Y": "the magnitude of inputand the sign of other",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Clamps which elements in input into the range[min,max]?",
        "Y": "all elements in input into the range[min,max].",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What tangent does Alias for torch.atan() return a new tensor with?",
        "Y": "inverse hyperbolic tangent",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.clamp() do?",
        "Y": "Clamps all elements in input into the range[min,max].",
        "Z": "Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Returns ignoreNaN values?",
        "Y": "the median of the values in input",
        "Z": "Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Tests if all elements input evaluate to what?",
        "Y": "True",
        "Z": "Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what value, ignoringNaN values?",
        "Y": "the median of the values in input",
        "Z": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the variant of torch.quantile() do?",
        "Y": "ignores",
        "Z": "Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "The variant of torch.quantile()ignores what ifNaN values in input did not exist?",
        "Y": "quantilesqas",
        "Z": "Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does return a copy of input?",
        "Y": "Compute combinations of lengthrrrof the given tensor",
        "Z": "Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What returns the cumulative product of elements of inputin the dimension dim?",
        "Y": "Returns the cumulative product of elements of inputin the dimension dim",
        "Z": "Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "If input is  a vector (1-D tensor), then returns what?",
        "Y": "a 2-D square tensor",
        "Z": "Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned when a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the",
        "Y": "the cumulative product of elements of inputin the dimension dim",
        "Z": "Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim",
        "Y": "the cumulative product of elements of inputin the dimension dim",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned in dimension dimof inputandother?",
        "Y": "the cross product of vectors",
        "Z": "Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what of inputin the dimension dim?",
        "Y": "the cumulative product of elements",
        "Z": "Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what of elements of inputin the dimension dim?",
        "Y": "cumulative sum",
        "Z": "Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of inputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements of inputin the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "If input is  a vector, then returns a 2-D square tensor Creates a tensor whose diagonals",
        "Y": "1-D tensor",
        "Z": "Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does the batch matrix-matrix product of matrices stored in input andmat2 return?",
        "Y": "matrix product of theNNN2-D tensors",
        "Z": "Performs a batch matrix-matrix product of matrices stored in input andmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What can't be overridden by __torch_function__?",
        "Y": "public functions",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable]",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Why are public functions that are publicly available in the torch API but cannot be overridden by__torch_function__?",
        "Y": "none of the arguments of these functions are tensors or tensor-likes",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What are functions that are via __torch_function__?",
        "Y": "overridable",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real",
        "Y": "Example",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the number of functions that are publicly available in the torch API but cannot be overridden by__torch_function__?",
        "Y": "tuple",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How many functions are publicly available in the torch API but cannot be overridden with__torch_function__?",
        "Y": "tuple",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What are lambda functions useful for?",
        "Y": "testing API coverage",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Where is the equivalent of the torch::autograd::handle_torch_function?",
        "Y": "C++ implementation",
        "Z": "A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden?",
        "Y": "__torch_function__",
        "Z": "List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Arbitrary positional arguments originally passed intopublic_api. kwargs (tuple) \u2013 Arbitrary keyword arguments originally passed into",
        "Y": "args",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are",
        "Y": "public_api(function)",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What are Arbitrary positional arguments originally passed intopublic_api?",
        "Y": "args",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is checked in the elements of an iterable?",
        "Y": "__torch_function__ implementations in the elements of an iterable",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Relevant_args: Iterable or what to check for __torch_function__ methods?",
        "Y": "aguments",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What check if something is a Tensor-like, including an exactTensor?",
        "Y": "bool",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What happens if any of the elements of relevant_args have __torch_function__ implementations?",
        "Y": "if any of the elements of relevant_args have __torch_function__ implementations",
        "Z": "relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is checked if something is a Tensor-like, including an exactTensor?",
        "Y": "bool",
        "Z": "Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What type of arguments are used to check for a__torch_function__attribute on the type of the input?",
        "Y": "Examples",
        "Z": "relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is another name for Arbitrary keyword arguments originally passed intopublic_api?",
        "Y": "kwargs",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What type of methods does iterable or aguments check for?",
        "Y": "__torch_function__",
        "Z": ":raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":type relevant_args: iterable True what if any of the elements of relevant_args have __torch",
        "Y": "if any of the elements of relevant_args have __torch_function__ implementations",
        "Z": "Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method,",
        "Y": "kwargs(tuple)",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does object :raise if no implementation is found?",
        "Y": "TypeError",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is TypeError?",
        "Y": "if no implementation is found",
        "Z": ":raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Check for what in the elements of an iterable. Considers exactTensors andParameters non-dispatchable.",
        "Y": "__torch_function__ implementations",
        "Z": "Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "If the function passed in is a handler for a method or property belonging totorch.Tensor, as passed into_",
        "Y": "True",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What may be needed to pass in a property's__get__method?",
        "Y": "following reasons",
        "Z": "Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "For what reasons is a property's__get__method required?",
        "Y": "following reasons",
        "Z": "Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does bool check if something is a Tensor-like?",
        "Y": "Checks if something is a Tensor-like, including an exactTensor",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What Returns True if the passed-in input is a Tensor-like?",
        "Y": "ReturnsTrueif the passed-in input is a Tensor-like",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Why is a__get__method passed in?",
        "Y": "Methods/properties sometimes don\u2019t contain a__module__slot",
        "Z": "True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does Wrap a given function with?",
        "Y": "Wraps a given function with__torch_function__-related functionality",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Why is a__get__method needed?",
        "Y": "Methods/properties sometimes don\u2019t contain a__module__slot",
        "Z": "bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What checks if something is a Tensor-like, including an exactTensor?",
        "Y": "Checks",
        "Z": "See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A subclass of what is generally a Tensor-like?",
        "Y": "tensor",
        "Z": "See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is generally a Tensor-like?",
        "Y": "A subclass of tensor",
        "Z": "Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does this wrap a given function with?",
        "Y": "Wraps a given function with__torch_function__-related functionality",
        "Z": "Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Examples Wraps what?",
        "Y": "a given function with__torch_function__-related functionality",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does this decorator do to your code?",
        "Y": "reduce the performance of your code",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does it usually suffice to express your code as?",
        "Y": "a series of functions",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "If you need a function to work for Tensor-likes, then this function is available.",
        "Y": "rare situation where this is not the case",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "If you\u2019re wrapping a low-level library and you need it to work for Tensor-likes, what is available?",
        "Y": "this function is available",
        "Z": "But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What should you express your code as?",
        "Y": "a series of functions that, themselves, support __torch_function__",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is an example of a situation where a function that supports __torch_function__ is available?",
        "Y": "rare situation where this is not the case",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is an example of a situation where this function is not available?",
        "Y": "if you\u2019re wrapping a low-level library and you also need it to work for Tensor-likes",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What type of function is available if you're wrapping a low-level library and you need it to work for Tensor-likes",
        "Y": "Examples",
        "Z": "Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Ifrepsspecifies how many dimensions than inputhas, then ones are prepended torepsuntil all dimensions are specified.",
        "Y": "fewer",
        "Z": "Constructs a tensor by repeating the elements of input.\nTherepsargument specifies the number of repetitions\nin each dimension. Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input(Tensor) \u2013 the tensor whose elements to repeat. reps(tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "Ifinputhas shape, andrepsis treated as what?",
        "Y": "1, 1, 2, 2).",
        "Z": "Constructs a tensor by repeating the elements of input.\nTherepsargument specifies the number of repetitions\nin each dimension. Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input(Tensor) \u2013 the tensor whose elements to repeat. reps(tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is an example of a tensor that is treated as if it were unsqueezed at dimension zero until it has as many",
        "Y": "ifinputhas fewer dimensions thanrepsspecifies",
        "Z": "Constructs a tensor by repeating the elements of input.\nTherepsargument specifies the number of repetitions\nin each dimension. Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input(Tensor) \u2013 the tensor whose elements to repeat. reps(tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is an example of a tensor that has fewer dimensions thanrepsspecifies?",
        "Y": "ifinputhas shape",
        "Z": "Constructs a tensor by repeating the elements of input.\nTherepsargument specifies the number of repetitions\nin each dimension. Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input(Tensor) \u2013 the tensor whose elements to repeat. reps(tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is the number of repetitions per dimension?",
        "Y": "reps",
        "Z": "Constructs a tensor by repeating the elements of input.\nTherepsargument specifies the number of repetitions\nin each dimension. Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input(Tensor) \u2013 the tensor whose elements to repeat. reps(tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is an example of a tuple function?",
        "Y": "Example",
        "Z": "Constructs a tensor by repeating the elements of input.\nTherepsargument specifies the number of repetitions\nin each dimension. Ifrepsspecifies fewer dimensions thaninputhas, then\nones are prepended torepsuntil all dimensions are specified.\nFor example, ifinputhas shape (8, 6, 4, 2) andrepsis (2, 2), thenrepsis treated as (1, 1, 2, 2). Analogously, ifinputhas fewer dimensions thanrepsspecifies, theninput is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions asrepsspecifies.\nFor example, ifinputhas shape (4, 2) andrepsis (3, 3, 2, 2), theninput is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input(Tensor) \u2013 the tensor whose elements to repeat. reps(tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What element of the given input tensor is returned along a given dimension?",
        "Y": "theklargest elements",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is returned, where the indicesare the indices of the elements in the original input tensor?",
        "Y": "A named tuple of(values, indices)",
        "Z": "Returns theklargest elements of the giveninput tensor along\na given dimension. Ifdimis not given, the last dimension of the inputs chosen. IflargestisFalsethen theksmallest elements are returned. A named tuple of(values, indices)is returned, where theindicesare the indices\nof the elements in the originalinput tensor. The boolean optionsortedIf True, will make sure that the returnedkelements are themselves sorted input(Tensor) \u2013 the input tensor. k(int) \u2013 the k in \u201ctop-k\u201d dim(int,optional) \u2013 the dimension to sort along largest(bool,optional) \u2013 controls whether to return largest or\nsmallest elements sorted(bool,optional) \u2013 controls whether to return the elements\nin sorted order out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What type of convolution over an input signal composed of several input planes?",
        "Y": "1D",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 3D max pooling over an input signal comprised of several input planes do?",
        "Y": "Computes a partial inverse ofMaxPool2d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies the element-wise functionReLU6(x)=min(max(0,x),6)textReLU6",
        "Y": "hardswish function",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().   Applies element-wise,SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717andscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "In-place version ofelu()?",
        "Y": "In-place version ofelu()",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version ofelu().   Applies element-wise,SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717andscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the value of CELU(max(0,x)+min(0,(exp(x/)",
        "Y": "x",
        "Z": "Applies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is in-place version ofrrelu()?",
        "Y": "Randomized leaky ReLU",
        "Z": "In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the functionSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretizes.   Applies a softmax followed by a logarithm.   Applies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b   Applies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b   Applies the element-wise function   Applies the Sigmoid Linear Unit (SiLU) function, element-wise.   Applies the Mish function, element-wise.   Applies Batch Normalization for each channel across a batch of data.   Applies Group Normalization for last certain number of dimensions.   Applies Instance Normalization for each channel in each data sample in a batch.   Applies Layer Normalization for last certain number of dimensions.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Randomized leaky ReLU.",
        "Y": "In-place version ofrrelu()",
        "Z": "Applies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version ofrrelu()?",
        "Y": "gated linear unit",
        "Z": "In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the functionSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretizes.   Applies a softmax followed by a logarithm.   Applies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b   Applies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b   Applies the element-wise function   Applies the Sigmoid Linear Unit (SiLU) function, element-wise.   Applies the Mish function, element-wise.   Applies Batch Normalization for each channel across a batch of data.   Applies Group Normalization for last certain number of dimensions.   Applies Instance Normalization for each channel in each data sample in a batch.   Applies Layer Normalization for last certain number of dimensions.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What function applies element-wise?",
        "Y": "hard shrinkage",
        "Z": "Applies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the gated linear unit?",
        "Y": "leaky_relu()",
        "Z": "In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the functionSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretizes.   Applies a softmax followed by a logarithm.   Applies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b   Applies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b   Applies the element-wise function   Applies the Sigmoid Linear Unit (SiLU) function, element-wise.   Applies the Mish function, element-wise.   Applies Batch Normalization for each channel across a batch of data.   Applies Group Normalization for last certain number of dimensions.   Applies Instance Normalization for each channel in each data sample in a batch.   Applies Layer Normalization for last certain number of dimensions.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is a learnable parameter?",
        "Y": "weight",
        "Z": "In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the functionSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretizes.   Applies a softmax followed by a logarithm.   Applies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b   Applies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b   Applies the element-wise function   Applies the Sigmoid Linear Unit (SiLU) function, element-wise.   Applies the Mish function, element-wise.   Applies Batch Normalization for each channel across a batch of data.   Applies Group Normalization for last certain number of dimensions.   Applies Instance Normalization for each channel in each data sample in a batch.   Applies Layer Normalization for last certain number of dimensions.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Randomized leaky ReLU. In-place version of what?",
        "Y": "rrelu()",
        "Z": "In-place version ofleaky_relu().   Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version ofrrelu().   The gated linear unit.   Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the functionSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretizes.   Applies a softmax followed by a logarithm.   Applies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b   Applies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b   Applies the element-wise function   Applies the Sigmoid Linear Unit (SiLU) function, element-wise.   Applies the Mish function, element-wise.   Applies Batch Normalization for each channel across a batch of data.   Applies Group Normalization for last certain number of dimensions.   Applies Instance Normalization for each channel in each data sample in a batch.   Applies Layer Normalization for last certain number of dimensions.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the term for the loss of the Gaussian negative log likelihood?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does TheKullback-Leibler divergence Loss measure?",
        "Y": "the element-wise mean squared error",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   SeeSoftMarginLossfor details.   SeeTripletMarginLossfor details   SeeTripletMarginWithDistanceLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion that measures Binary Cross Entropy between target and output logits?",
        "Y": "combineslog_softmaxandnll_lossin a single function",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   SeeSoftMarginLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does rearranging elements in a tensor of shape(,C,Hr,Wr)(*,",
        "Y": "Reverses thePixelShuffleoperation",
        "Z": "Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor.   Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.   Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given aninputand a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is given a batch of in a 2D or 3D flow field?",
        "Y": "affine matricestheta",
        "Z": "Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor.   Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.   Pads tensor.   Down/up samples the input to either the givensizeor the givenscale_factor   Upsamples the input to either the givensizeor the givenscale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given aninputand a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "If the first argument is 2-dimensional and the second argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched",
        "Y": "1",
        "Z": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What type of dimensions are broadcastable?",
        "Y": "non-matrix",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What must the non-matrix dimensions be?",
        "Y": "broadcastable",
        "Z": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "Inputs are valid for broadcasting even though what are different?",
        "Y": "final two dimensions",
        "Z": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "a torch.Tensoris is a what?",
        "Y": "multi-dimensional matrix",
        "Z": "a torch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of torch does Torch define?",
        "Y": "LongTensor Boolean torch",
        "Z": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the float torch?",
        "Y": "float32ortorch",
        "Z": "GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How many bits of floating point torch does float32ortorch.float torch.FloatTensor torch.cuda.",
        "Y": "32",
        "Z": "32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is 1 torch?",
        "Y": "16-bit floating point",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a torch called?",
        "Y": "float16ortorch.half torch",
        "Z": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "ByteTensor is sometimes referred to as what?",
        "Y": "binary16",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What operator supports Brain Floating Point?",
        "Y": "EmbeddingBag operator",
        "Z": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor).",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is torch.Tensoris an alias for?",
        "Y": "default tensor type",
        "Z": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How can a tensor be constructed from a Pythonlistor sequence?",
        "Y": "thetorch.tensor()",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is used to avoid a copy of a tensordata?",
        "Y": "userequires_grad_()ordetach()",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does thattorch.autogradrecords operations do on a tensor?",
        "Y": "automatic differentiation",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of storage does the tensor class provide?",
        "Y": "multi-dimensional,stridedview",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What suffix mark methods which mutate a tensor?",
        "Y": "underscore",
        "Z": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the term for a tensor'storch.deviceand/ortorch.dtype?",
        "Y": "Warning",
        "Z": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a tensor'storch.deviceand/ortorch.dtype?",
        "Y": "Warning",
        "Z": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are there a few main ways to create?",
        "Y": "tensor",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor with dataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is one way to create a tensor with specific size?",
        "Y": "*tensor creation ops",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor with dataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.new_full return a Tensor of sizesizefilled?",
        "Y": "withfill_value",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor with dataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is added to a scalar or tensor toselftensor?",
        "Y": "Add a scalar or tensor toselftensor",
        "Z": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). Tensor.bfloat16",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor",
        "Y": "addcdiv",
        "Z": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor?",
        "Y": "addbmm",
        "Z": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.ndim Alias fordim() Tensor.real Return a new tensor containing?",
        "Y": "real values of theselftensor",
        "Z": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is added to a scalar or tensor?",
        "Y": "Add a scalar or tensor toselftensor",
        "Z": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Seetorch.addbmm() Tensor?",
        "Y": "addbmm",
        "Z": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does add do to a scalar or tensor?",
        "Y": "Add a scalar or tensor toselftensor",
        "Z": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the scalar or tensor toselftensor?",
        "Y": "Tensor",
        "Z": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does add a scalar or tensor toselftensor do?",
        "Y": "Add a scalar or tensor toselftensor",
        "Z": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Seetorch.addbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.",
        "Y": "addbmm",
        "Z": "Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What do you add to a tensor?",
        "Y": "scalar or tensor toselftensor",
        "Z": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is replaced by each element in the tensor?",
        "Y": "the value returned bycallable",
        "Z": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What represent a multi-dimensional array containing elements of a single data type?",
        "Y": "PyTorch",
        "Z": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are stored contiguously in memory?",
        "Y": "array elements",
        "Z": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is another name for multi-dimensional arrays where the contiguous memory storage of array elements turns out to be suboptimal?",
        "Y": "sparse arrays",
        "Z": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a sparse array format?",
        "Y": "Note",
        "Z": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is at leastproduct(tensorshape>)*sizeofelementtypeinbytes>?",
        "Y": "strided tensor",
        "Z": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the size of a strided tensor?",
        "Y": "10 000 x 10 000",
        "Z": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the inputinot a list of?",
        "Y": "index tuples",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing thevaluestensor to",
        "Y": "multi-dimensional tensor",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where is the entry [5, 6] in a sparse COO tensor?",
        "Y": "(1, 0",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What would we do to create a sparse COO tensor?",
        "Y": "write",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The indices of specified elements are collected what?",
        "Y": "inindicestensor",
        "Z": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the values of a sparse tensor stored as?",
        "Y": "strided tensors",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors, where",
        "Y": "duplicate coordinates",
        "Z": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What will the coalescing process accumulate into a single value using summation?",
        "Y": "multi-valued elements",
        "Z": "M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be found in the indices of PyTorch sparse COO tensors?",
        "Y": "duplicate coordinates",
        "Z": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns true?",
        "Y": "torch.Tensor.is_coalesced()",
        "Z": "torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of COO tensor is a torch.Tensorinstance?",
        "Y": "sparse",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How can the scalar multiplication on an uncoalesced sparse tensor be implemented?",
        "Y": "multiplying all the uncoalesced values with the scalar",
        "Z": "but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What nonlinear operation cannot be implemented by applying the operation to uncoalesced data?",
        "Y": "a square root",
        "Z": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The CSR sparse tensor encodes the index invaluesandcol_indicesdepending on what?",
        "Y": "where the given row starts",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thecrow_indicestensor is a 1-D tensor of what size?",
        "Y": "sizesize[0]+1",
        "Z": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What do you want to use if you want to usetorch.int32?",
        "Y": "MKL-enabled matrix operations",
        "Z": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the default linking of pytorch with?",
        "Y": "MKL LP64",
        "Z": "Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thecrow_indicestensor consists of what?",
        "Y": "compressed row indices",
        "Z": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If you want to use what -enabled matrix operations, usetorch.int32.",
        "Y": "MKL",
        "Z": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does usetorch.int32 do?",
        "Y": "MKL-enabled matrix operations",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the user have to do to construct a sparse CSR matrices?",
        "Y": "The user must supply the row and column indices and values tensors separately",
        "Z": "Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present?",
        "Y": "Thesizeargument",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is currently the only math operation supported on CSR tensors?",
        "Y": "thetensor.matmul()method",
        "Z": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the sparse matrix-vector multiplication?",
        "Y": "currently the only math operation supported on CSR tensors",
        "Z": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch, operation Sparse grad?",
        "Y": "PyTorch",
        "Z": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Which PyTorch operations support backward with respect to sparse matrix argument?",
        "Y": "All PyTorch operations, excepttorch.smm(), support",
        "Z": "torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "All PyTorch operations, excepttorch.smm(), support backward with respect to sparse matrix argument. <sep>",
        "Y": "All PyTorch operations, excepttorch.smm(), support backward with respect to",
        "Z": "M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does PyTorch not support with the layout signatureM[strided]@M[sparse_coo]?",
        "Y": "PyTorch does not support matrix multiplication",
        "Z": "torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Applications can still compute this using what?",
        "Y": "matrix relationD@S==(S.t()@D.t()).t()",
        "Z": "torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_dim Return in a sparse tensorself?",
        "Y": "sparse dimensions",
        "Z": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the tensor method that resizes itselfsparse tensorto the desired size and the number",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What removes all specified elements from a sparse tensorself?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns a coalesced copy ofselfifselfis anuncoalesced tensor?",
        "Y": "Tensor.coalesce",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.is_coalesced return if a sparse COO tensorthat is coalesced?",
        "Y": "Trueifselfis",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns trueifselfis a sparse COO tensorthat is coalesced?",
        "Y": "Tensor.to_dense",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The following methods are specific tosparse what tensors?",
        "Y": "CSR",
        "Z": "Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the indices of theselftensor whenselfis a sparse CSR tensor of layoutspars",
        "Y": "Tensor.col_indices",
        "Z": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Return what tensor of a sparse COO tensor?",
        "Y": "indices",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the tensor containing return when selfis a sparse CSR tensor of layoutsparse_",
        "Y": "column indices",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse C",
        "Y": "Tensor.crow_indices",
        "Z": "Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned when a sparse COO tensorthat is coalesced?",
        "Y": "Trueifselfis",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "When selfis a sparse CSR tensor of layoutsparse_csr, what type of tens",
        "Y": "a sparse CSR tensor",
        "Z": "Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns a tensor with the same size as inputfilled withfill_value?",
        "Y": "a tensor with the same size as inputfilled withfill_value",
        "Z": "Returns a tensor with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size of inputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype of input. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout of input. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device of input. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What cantorch.view_as_real() be used to recover?",
        "Y": "real tensor",
        "Z": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the interface of the STFT modeled after?",
        "Y": "thelibrosastft function",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does wheremmmis?",
        "Y": "the index of the sliding window",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "In a future pytorch release, this function will only what?",
        "Y": "return complex tensors",
        "Z": "Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "In a future pytorch release, this function will only do what?",
        "Y": "return complex tensors",
        "Z": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "The input must be either a what?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Inputmust be either a what?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for a sliding window?",
        "Y": "Ifhop_lengthisNone",
        "Z": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthisNone(default), what will the window be padded on both sides to before being applied?",
        "Y": "lengthn_fft",
        "Z": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default for input to be padded on both sides?",
        "Y": "IfcenterisTrue",
        "Z": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What begins at timethop_lengtht times texthop_lengththop_length?",
        "Y": "thettt-th frame",
        "Z": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for n_fft/4?",
        "Y": "Ifhop_lengthisNone",
        "Z": "inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Inputwill be padded on both sides so that thettt-th frame is centered at what?",
        "Y": "IfcenterisTrue",
        "Z": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for input when centerisTrue?",
        "Y": "reflect",
        "Z": "inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value that is treated as equal tofloor(n_fft/4)?",
        "Y": "Ifhop_lengthisNone",
        "Z": "Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Windowcan be a what?",
        "Y": "1-D tensor of sizewin_length",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthisNone(default),windowwill be padded on both sides to what before being applied?",
        "Y": "lengthn_fft",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifwin_lengthisNone(default) is treated as what?",
        "Y": "equal ton_fft",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for a window?",
        "Y": "IfwindowisNone",
        "Z": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting for windows?",
        "Y": "reflect",
        "Z": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What returns the normalized STFT results?",
        "Y": "IfnormalizedisTrue",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If true, the output is what?",
        "Y": "ainput.dim()+2dimensional real tensor",
        "Z": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the optional batch size of input?",
        "Y": "the number of frequencies where STFT is applied",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the input tensor n_fft(int) \u2013 size of Fourier transform?",
        "Y": "input(Tensor)",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default:None(treated as window of what number) \u2013 the optional window function.",
        "Y": "all111s",
        "Z": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int",
        "Y": "input(Tensor)",
        "Z": "Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for window of all111s?",
        "Y": "Default:None",
        "Z": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does false onesided(bool,optional) avoid for real inputs?",
        "Y": "redundancy",
        "Z": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the name of the function that returns a complex tensor?",
        "Y": "return_complex",
        "Z": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the name of the tensor function that returns a complex tensor?",
        "Y": "return_complex",
        "Z": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If the number of bins is at leastminlength, then the result is a tensor of sizeminlengthfilled with zeros.",
        "Y": "Ifminlengthis specified",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 what?",
        "Y": "optional",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What should the input tensor weights(Tensor) be of?",
        "Y": "same size",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is the optional, minimum number of bins?",
        "Y": "minlength(int)",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Minlength(int) \u2013 optional, minimum number of bins. Should be what?",
        "Y": "non-negative",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unlessinput is empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand If input is  empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])If input is  non-empty, elseSize(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "When running on what platform,row*colmust be less than259259259 to prevent overflow during calculation?",
        "Y": "CUDA",
        "Z": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is layout(torch.layout, optional)?",
        "Y": "currently only supporttorch.strided",
        "Z": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is an example of a CUDA tensor type?",
        "Y": "Example:",
        "Z": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the default setting for a 2-D tensor with ones on the diagonal and zeros elsewhere?",
        "Y": "Default:torch.strided",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)?",
        "Y": "callingtorch.kaiser_window(L+1,B,periodic=False)",
        "Z": "Computes the Kaiser window with window length window_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Ifwindow_lengthis what, then the returned window is a single element tensor containing a one?",
        "Y": "one",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Let I_0 be the modified Bessel function of the first kind?",
        "Y": "zeroth order",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does reordering the multiplications do?",
        "Y": "multiplies two or more matrices",
        "Z": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "Computes the inverse of a square matrix what?",
        "Y": "if it exists",
        "Z": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "What should X=torch.solve(B,A)solution be replaced with?",
        "Y": "Note Irrespective of the original strides",
        "Z": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the name of the strides that are used to transpose the returned matrices?",
        "Y": "A.contiguous().transpose(-1, -2)",
        "Z": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is input matrixBBBof size(,m,k)(*, m, k)(,m,k)",
        "Y": "zero or more batch dimensions",
        "Z": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "To get the LU factorization of the input, what may be used with torch.lu_solve() and torch.",
        "Y": "seetorch.lu()",
        "Z": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does the input matrixBBBof size(,m,k)(*, m, k)(,m,k",
        "Y": "zero or more batch dimensions",
        "Z": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is an input square matrix of size(,m,m)(*, m, m)(,m,m)",
        "Y": "A(Tensor)",
        "Z": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor of torch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used with torch.lu_solve()and torch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is returned when a library is compiled for CUDA?",
        "Y": "Gets the cuda capability of a device",
        "Z": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Sets what if PyTorch's CUDA state has been initialized?",
        "Y": "current device",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns what stream for a given device?",
        "Y": "currently selectedStreamfor a given device",
        "Z": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is returned when a device is selected?",
        "Y": "Gets the cuda capability of a device",
        "Z": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is returned when a device is compiled?",
        "Y": "Gets the cuda capability of a device",
        "Z": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Sets the current stream.This is a wrapper API to what?",
        "Y": "set the stream",
        "Z": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does the wrapper API do?",
        "Y": "Waits for all kernels in all streams on a CUDA device to complete",
        "Z": "Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does seememory_reserved() do?",
        "Y": "Set memory fraction",
        "Z": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is the name of the function that returns the memory fraction for a process?",
        "Y": "seemax_memory_reserved()",
        "Z": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is the name of the function that computes the inverse error function of input?",
        "Y": "Computes the inverse error function of input",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does a Vandermonde matrix do?",
        "Y": "Generates a Vandermonde matrix",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is a Vandermonde matrix named for?",
        "Y": "Alexandre-Theophile Vandermonde",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is the name of the order of the powers of the columns in a Vandermonde matrix?",
        "Y": "increasing(bool,optional)",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is True, what happens to the powers of the columns in a Vandermonde matrix?",
        "Y": "the powers increase from left to right",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is the name of the matrix with a geometric progression in each row named for Alexandre-Theophile Vandermonde?",
        "Y": "Vandermonde matrix",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is True, the columns of the Vandermonde matrix are reversed if what?",
        "Y": "False",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is False, the columns arex0,x1,...,x(N1)x0, x1,",
        "Y": "True",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is the element-wise division performed by addcdiv?",
        "Y": "often s or 1 byte n s or 2",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2,\nmultiply the result by the scalar valueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What of input,tensor1, andtensor2must bebroadcastable?",
        "Y": "shapes",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2,\nmultiply the result by the scalar valueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What is the tensor to be added tensor1(Tensor) \u2013 the numerator tensor",
        "Y": "input(Tensor)",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2,\nmultiply the result by the scalar valueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What encapsulates an asynchronous execution and a set of utility functions to simplify operations onFutureobjects?",
        "Y": "aFuturetype",
        "Z": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around a torch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does thisFuture return if it has a result or an exception?",
        "Y": "ReturnTrueif thisFutureis done",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains what that reside on GPUs, Future.done()will returnTrueeven if the asynchronous kernels that are",
        "Y": "tensors",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does Future.done() return if thisFutureis done?",
        "Y": "ReturnTrue",
        "Z": "is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When calling wait()/value() on thisFuture, the exception set here will be raised what?",
        "Y": "inline",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the exception for thisFuture mean?",
        "Y": "aFuturecannot be marked completed twice",
        "Z": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "This method will record events on all the relevant current streams and use them to ensure proper scheduling for all the consumers of what?",
        "Y": "thisFuture",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does thisFuture not do?",
        "Y": "aFuturecannot be marked completed twice",
        "Z": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the callback function do to thisFuture?",
        "Y": "Append",
        "Z": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What can be used to enforce a certain order?",
        "Y": "chaining:fut.then(cb1).then(cb2)",
        "Z": "result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If theFuture's value contains what that reside on GPUs, the callback might be invoked while the async kernels",
        "Y": "tensors",
        "Z": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "The callback will be invoked with dedicated streams set as what?",
        "Y": "current",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is similar to the non-blocking behavior ofwait()?",
        "Y": "non-blocking behavior ofwait()",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does callback(Callable) take as the only argument?",
        "Y": "thisFuture",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When will a newFutureobject that holds the return value of thecallback be marked as completed?",
        "Y": "when the givencallbackfinishes",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a method that can be called after a call towait() has been completed?",
        "Y": "Obtain the value of an already-completed future",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, this method willnotperform any additional synchronization. This should be done",
        "Y": "separately",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What may not yet hold a value and callingvalue()could fail?",
        "Y": "thisFuture",
        "Z": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How long should thisFuture be blocked?",
        "Y": "until the value of thisFutureis ready",
        "Z": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When does block occur?",
        "Y": "until the value of thisFutureis ready",
        "Z": "Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains what that reside on GPUs, an additional synchronization is performed with the kernels (executing on the device) which may be",
        "Y": "tensors",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does non-blocking do to ensure that further operations enqueued on those streams will be properly scheduled after the async",
        "Y": "wait()will insert the necessary instructions in the current streams",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will throw an error if the function (callback or RPC) creating the value has thrown an error?",
        "Y": "thiswaitmethod",
        "Z": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens if the value contains tensors that reside on GPUs?",
        "Y": "wait()will insert the necessary instructions in the current streams",
        "Z": "Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the function (callback or RPC) creating the value has thrown an error, what will also throw an error?",
        "Y": "thiswaitmethod",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does non-blocking mean?",
        "Y": "wait()will insert the necessary instructions in the current streams",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When is the combinedFuture completed?",
        "Y": "when all of the sub-futures are completed",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When may the available functions change?",
        "Y": "PyTorch releases",
        "Z": "Warning This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What are we actively looking for for UI/UX improvements or missing functionalities?",
        "Y": "feedback",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Asserts  that actual and expected are close?",
        "Y": "that actual and expected are close",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  close, they are considered close if and they have the same device(if check_device is True), same",
        "Y": "real-valued and finite",
        "Z": "Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "NaN\u2019s are only considered equal to each other what?",
        "Y": "ifequal_nanisTrue",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  close, they are considered close if both their real and imaginary components are considered close according to the definition above.",
        "Y": "complex-valued",
        "Z": "Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "How can be Tensor\u2019s be constructed?",
        "Y": "with torch.as_tensor()",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What can be considered close if their structure matches and all their elements are considered close according to the above definition?",
        "Y": "be Sequence\u2019s or Mapping\u2019s",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "rtol(Optional[float]) \u2013 what?",
        "Y": "Relative tolerance",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does union[bool,str] \u2013 If True, two NaN values will be considered equal?",
        "Y": "equal_nan",
        "Z": "Warning This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If \"relaxed\", complex values are considered as NaN  what?",
        "Y": "if either the real or imaginary component is NaN",
        "Z": "Warning This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does check_device(bool) stand for?",
        "Y": "If True(default)",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "When may the available functions change in this module?",
        "Y": "PyTorch releases",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If actual and expected are  what, they are considered close if both their real and imaginary components are considered close according to the definition above?",
        "Y": "complex-valued",
        "Z": "and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "How can be Tensor\u2019s constructed?",
        "Y": "with torch.as_tensor()",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If what must also be specified, default values based on the dtype are selected with the below table. atol(Optional[float]",
        "Y": "specifiedatol",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the name of the flag that indicates that two NaN values will be considered equal?",
        "Y": "equal_nan",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What types of values are considered close if they have the same device(if check_device is True), same dtype(ifcheck",
        "Y": "real-valued and finite",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What values are only considered close if and only if they are equal?",
        "Y": "Non-finite values",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What type of values are considered close if both their real and imaginary components are considered close according to the definition above?",
        "Y": "complex-valued",
        "Z": "This module is in a PROTOTYPE state. New functions are still being added, and the available functions may change in\nfuture PyTorch releases. We are actively looking for feedback for UI/UX improvements or missing functionalities. Asserts  that actual and expected are close. If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Actual input. expected(Any) \u2013 what?",
        "Y": "Expected input",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "rtol(Optional[float]) \u2013 Relative tolerance. If what is omitted, default values based on the",
        "Y": "specified atol must also be specified",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If omitted, what are selected with the below table?",
        "Y": "default values based on the dtype are selected with the below table",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal",
        "Y": "equal",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "When are complex values considered as NaN if either the real or imaginary component is NaN?",
        "Y": "\"relaxed\"",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If check_device(bool) is disabled, what happens to tensors on differentdevice\u2019s?",
        "Y": "tensors on differentdevice\u2019s are moved to the CPU before being compared",
        "Z": "If actual and expected are  real-valued and finite, they are considered close if and they have the same device(if check_device is True), same dtype(if check_dtype is True), and the same stride (if check_stride is True). Non-finite values\n(-infandinf) are only considered close if and only if they are equal.NaN\u2019s are only considered equal\nto each other ifequal_nanisTrue. If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What can actual and expected do?",
        "Y": "be Tensor\u2019s or any array-or-scalar-like of the same type",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What can be Sequence\u2019s or Mapping\u2019s in which case they are considered close if their structure matches and all their elements are",
        "Y": "be Sequence\u2019s or Mapping\u2019s",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "rtol(Optional[float]) \u2013 Relative tolerance. If what?",
        "Y": "specified atol must also be specified",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Absolute tolerance must also be specified if what must also be specified?",
        "Y": "specified rtol",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Which tensors are on the same device?",
        "Y": "corresponding tensors",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Check_dtype(bool) \u2013 If True(default) \u2013 asserts that corresponding tensors have what?",
        "Y": "same dtype",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If check_dtype(bool) is disabled, tensors with different type's are what?",
        "Y": "promoted to a common dtype",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Check_stride(bool) \u2013 If True(default) asserts that what?",
        "Y": "corresponding tensors have the same stride",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is optional[Union[str,Callable[[Tensor,Tensor,Dial,Tens",
        "Y": "msg",
        "Z": "If actual and expected are  complex-valued, they are considered close if both their real and\nimaginary components are considered close according to the definition above. actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "How can actual and expected be Tensor\u2019s or any array-or-scalar-like of the same type be constructed?",
        "Y": "with torch.as_tensor()",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Absolute tolerance. If what?",
        "Y": "specified rtol must also be specified",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Check_device(bool) \u2013 If True(default) \u2013 If True(default), asserts that what?",
        "Y": "corresponding tensors are on the same device",
        "Z": "actual and expected can  be Tensor\u2019s or any array-or-scalar-like of the same type,\nfrom whichtorch.Tensor\u2019s can be constructed with torch.as_tensor(). In addition,actual and expected can  be Sequence\u2019s or Mapping\u2019s in which case\nthey are considered close if their structure matches and all their elements are considered close according to the\nabove definition. actual(Any) \u2013 Actual input. expected(Any) \u2013 Expected input. rtol(Optional[float]) \u2013 Relative tolerance. If specified atol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. atol(Optional[float]) \u2013 Absolute tolerance. If specified rtol must also be specified. If omitted,\ndefault values based on the dtype are selected with the below table. equal_nan(Union[bool,str]) \u2013 If True, two NaN values will be considered equal. If\"relaxed\",\ncomplex values are considered as NaN if either the real or imaginary component is NaN. check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Check_device(bool) \u2013 If True(default) \u2013 asserts that what are on the same device?",
        "Y": "corresponding tensors",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Check_dtype(bool) \u2013 If True(default) \u2013 If True(default), asserts that corresponding ten",
        "Y": "same dtype",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If check_dtype is disabled, tensors with different type\u2019s are promoted to what?",
        "Y": "common dtype",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Check_stride(bool) \u2013 If True(default) asserts that corresponding tensors have what?",
        "Y": "same stride",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Can be passed as callable in which case it will be called with the what?",
        "Y": "mismatching tensors",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is the name of the error message that can be used if the values of corresponding tensors mismatch?",
        "Y": "See below for details",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What is a UsageError?",
        "Y": "a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If any tensor is quantized or sparse, this is a what?",
        "Y": "temporary restriction",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What does UsageError occur if?",
        "Y": "only rtol or atol is specified",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If the inputs are Sequence\u2019s, but their length does not match?",
        "Y": "If the inputs are Sequence\u2019s, but their length does not match",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If corresponding tensors are not on the same device, what is the cause of the error?",
        "Y": "If Check_device",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. As",
        "Y": "If Check_dtype",
        "Z": "check_device(bool) \u2013 If True(default), asserts that corresponding tensors are on the same device. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared. check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If check_dtype is disabled, tensors with different type\u2019s are what?",
        "Y": "promoted to a common dtype",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What should you do if the values of corresponding tensors mismatch?",
        "Y": "See below for details",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "If corresponding tensors are not on the same device, what is the cause of Assertion Error?",
        "Y": "If Check_device",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What error occurs if corresponding tensors do not have the same dtype?",
        "Y": "If Check_dtype",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "What happens if the values of corresponding tensors have the same stride?",
        "Y": "Assertion Error",
        "Z": "check_dtype(bool) \u2013 If True(default), asserts that corresponding tensors have the same dtype. If this\ncheck is disabled, tensors with different type\u2019s are promoted  to a common dtype(according totorch.promote_types()) before being compared. check_stride(bool) \u2013 If True(default), asserts that corresponding tensors have the same stride. msg(Optional[Union[str,Callable[[Tensor,Tensor,DiagnosticInfo],str]]]) \u2013 Optional error message to use if\nthe values of corresponding tensors mismatch. Can be passed as callable in which case it will be called\nwith the mismatching tensors and a namespace of diagnostic info about the mismatches. See below for details. UsageError\u2013 If a torch.Tensorcan\u2019t be constructed from an array-or-scalar-like. UsageError\u2013 If any tensor is quantized or sparse. This is a temporary restriction and will be relaxed in the\n    future. UsageError\u2013 If only rtol or atol is specified. Assertion Error\u2013 If corresponding array-likes have different types. Assertion Error\u2013 If the inputs are Sequence\u2019s, but their length does not match. Assertion Error\u2013 If the inputs are Mapping\u2019s, but their set of keys do not match. Assertion Error\u2013 If corresponding tensors do not have the same shape. Assertion Error\u2013 If Check_device, but corresponding tensors are not on the same device. Assertion Error\u2013 If Check_dtype, but corresponding tensors do not have the same dtype. Assertion Error\u2013 If Check_stride, but corresponding tensors do not have the same stride. Assertion Error\u2013 If the values of corresponding tensors are not close. The following table displays the default rtol and atolfor different type\u2019s. Note that the dtype refersto the promoted type in case actual and expected do not have the same dtype. dtype rtol atol float16 1e-3 1e-5 bfloat16 1.6e-2 1e-5 float32 1.3e-6 1e-5 float64 1e-7 1e-7 complex32 1e-3 1e-5 complex64 1.3e-6 1e-5 complex128 1e-7 1e-7 other 0.0 0.0 The namespace of diagnostic information that will be passed to msg if its a callable has the following\nattributes: number_of_elements(int): Number of elements in each tensor being compared.",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "Returns what if the inputs a single element tensor?",
        "Y": "True if the inputs a single element tensor",
        "Z": "Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Random sampling creation ops are listed under Random sampling and include:torch.rand()torch.rand()torch.",
        "Y": "Random sampling creation ops",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size. Returns",
        "Y": "Returns a tensor",
        "Z": "Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, and torch.bfloat16.   Returns True if the inputs a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a what type of tensor of size end start step left l ceil fractextend",
        "Y": "1-D tensor",
        "Z": "Sets the default floating point dtype tod.   Get the current default floating point torch.dtype.   Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a tensor filled with what value 1?",
        "Y": "scalar",
        "Z": "Sets the default torch.Tensortype to floating point tensor typet.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is created of sizestepswhose values are evenly spaced from start to end inclusive?",
        "Y": "one-dimensional tensor",
        "Z": "Random sampling creation ops are listed under Random sampling and\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is constructed in COO(rdinate) format with specified values at the given indices?",
        "Y": "a sparse tensor",
        "Z": "Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of tensor of sizestepswhose values are evenly spaced from start to end, inclusive?",
        "Y": "one-dimensional",
        "Z": "Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) formatwith specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of tensor of sizestepswhose values are evenly spaced from start to end inclusive?",
        "Y": "one-dimensional tensor",
        "Z": "Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.",
        "Y": "Con",
        "Z": "Create a view of an existing torch.Tensor input with specified size,stride and storage_offset.   Creates aTensorfrom a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values from start to endwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Splits input, a tensor with three or more dimensions, into what?",
        "Y": "multiple tensors depth wise",
        "Z": "Concatenates the given sequence of seq tensor in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depth wise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Splits input, a tensor with one or more dimensions, into what?",
        "Y": "multiple tensors",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the index that returns a new tensor which indexes the input tensor along dimension dimusing",
        "Y": "a Long Tensor",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the boolean mask mask that returns a new 1-D tensor which indexes the input tensor",
        "Y": "a Bool Tensor",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.transpose() stand for?",
        "Y": "Alias for torch.transpose()",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What happens along an axis specified by dim?",
        "Y": "Gathers values",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.movedim() return a new tensor that is?",
        "Y": "a narrowed version of input tensor",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What are the elements of in out at the  given?",
        "Y": "indices",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Selects values fromin out atwhat?",
        "Y": "1-dimensional indices",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Splits input into what horizontally according toindices_or_sections?",
        "Y": "multiple tensors",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the index which indexes the input tensor along dimension dimusing the entries in index?",
        "Y": "a Long Tensor",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what with the elements of in out at the  given indices?",
        "Y": "a new tensor",
        "Z": "Stack tensors in sequence depth wise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dimusing the entries in indexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a Bool Tensor.   Moves the dimension(s) of in out at the  position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of inputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of in out at the  given indices.   Selects values fromin out at the  1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splits input, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row",
        "Y": "a tensor",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive",
        "Y": "a tensor",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What returns a random permutation of integers from 0 to n -1?",
        "Y": "random permutation of integers from 0 to n -1",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that returns the initial seed for generating random numbers as a Python long?",
        "Y": "Sets the seed for generating random numbers",
        "Z": "Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.Byte Tensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Sets the seed for generating random numbers. Returns the random number generator state as a torch.Byte Tensor.",
        "Y": "random number generator state",
        "Z": "Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.Byte Tensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive)?",
        "Y": "a tensor",
        "Z": "Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.Byte Tensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive)",
        "Y": "a tensor",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Each element sampled from a what distribution with rate parameter given by the corresponding element in input ?",
        "Y": "Poisson",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of ten",
        "Y": "a tensor",
        "Z": "Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasi random.SobolEngine",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a tensor with what shape as Tensor input?",
        "Y": "same shape",
        "Z": "Returns a tensor where each row contains num_sample indices sampled from the multinomial probability distribution located in the corresponding row of Tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasi random.SobolEngine",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned if a tensor is filled with random integers generated uniformly between low(inclusive) and high(exclusive)",
        "Y": "a tensor",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensor inputfilled with random integers generated uniformly between low(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n -1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version of torch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version of torch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasi random.SobolEngine Thetorch.quasi random.SobolEngineis an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the what value of each element in input ?",
        "Y": "absolute value",
        "Z": "Computes the absolute value of each element in input .   Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Alias for torch.abs() Computes the what cosine of each element in input ?",
        "Y": "inverse",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the giveninput tensor's what?",
        "Y": "element-wise angle",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.asinh() return a new tensor with?",
        "Y": "arctangent",
        "Z": "Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Who returns a new tensor with the inverse hyperbolic tangent of the elements of input?",
        "Y": "Alias for torch.atanh()",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the what of inputandother?",
        "Y": "bitwise XOR",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.acosh() add?",
        "Y": "scalar other to each element of the inputinput",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Clamps all elements in input into what range?",
        "Y": "range[min,max]",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.abs() do?",
        "Y": "Alias for torch.",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the what cosine of each element in input ?",
        "Y": "inverse",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the inverse cosine of each element in input . Computes the smallest integer greater than or equal to each element.",
        "Y": "Alias for torch.clamp()",
        "Z": "Computes the inverse cosine of each element in input .   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the giveninput tensor. Returns a new tensor with the arcsine of the elements ofin",
        "Y": "element-wise angle",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Who returns a new tensor with the inverse hyperbolic sine of the elements of input?",
        "Y": "Alias for torch.asin()",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Clamps all elements in input into what?",
        "Y": "range[min,max]",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What function computes the element-wise conjugate of the giveninput tensor?",
        "Y": "Alias for torch.clamp()",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What type of tensor is created with the magnitude of input?",
        "Y": "floating-point tensor",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What function adds the scalar other to each element of the inputinputand returns a new resulting tensor?",
        "Y": "Alias for torch.acosh()",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Create a new what with the magnitude of inputand the sign of other, elementwise?",
        "Y": "floating-point tensor",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Alias for torch.asinh(). Returns a new tensor with the what of the elements ofin",
        "Y": "arctangent",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Clamps which elements in input into the range[min,max].?",
        "Y": "all elements in input into the range[min,max].",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Alias for torch.clamp(). Computes the what of the giveninput tensor?",
        "Y": "element-wise conjugate",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Create a new what tensor with the magnitude of inputand the sign of other, elementwise?",
        "Y": "floating-point tensor",
        "Z": "Performs the element-wise division often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Multiply the result by the scalar value and add it toinput. Computes the element-wise angle (in radi",
        "Y": "often s or 1 byte n s or 2",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes what of the giveninput tensor?",
        "Y": "the element-wise angle",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Who returns a new tensor with the arctangent of the elements of input?",
        "Y": "Alias for torch.asinh()",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes the element-wise what of the giveninput tensor?",
        "Y": "conjugate",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is returned with each of the elements of inputconverted from angles in degrees to radians?",
        "Y": "a new tensor",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does divide each element of the inputinput by the corresponding element of other?",
        "Y": "Divides each element of the inputinputby the corresponding element of other",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Which Alia performs the element-wise multiplication often s or 1 byte n s or 2 multiply the result by the scalar value?",
        "Y": "Alia",
        "Z": "Performs the element-wise multiplication often s or 1 byte n s or 2, multiply the result by the scalar valueand add it toinput.   Computes the element-wise angle (in radians) of the giveninput tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What are the elements of inputconverted from?",
        "Y": "angles in degrees to radians",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What happens to each element of the inputinput by the corresponding element of other?",
        "Y": "Divides each element of the inputinputby the corresponding element of other",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.special.erf() do?",
        "Y": "Alias for torch.special.erfc()",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is another name for Alias for torch.special.erf()?",
        "Y": "Alias for torch.special.erfc()",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that returns a new tensor with the exponential of the elements of input?",
        "Y": "Alias for torch.special.erfinv()",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What element of the elements of input does Alias for torch.asinh() return a new tensor with",
        "Y": "arctangent",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor with the inverse hyperbolic tangent of the elements of input. Alias fort",
        "Y": "atan()",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "The arctangent of inputi/otheri is considered with consideration of what?",
        "Y": "quadrant",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What function returns a new tensor with the exponential of the elements of the input Tensor input?",
        "Y": "Alias for torch.special.erfinv()",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns a new tensor with what of the elements of the input Tensor input?",
        "Y": "exponential",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the name of the function that returns a new tensor with the exponential of the elements of the input Tensor input",
        "Y": "Alias for torch.special.exp2()",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is Alias for torch.special.exp2()?",
        "Y": "Alia",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of inputandother.   Computes the bitwise OR of inputandother.   Computes the bitwise XOR of inputandother.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range[min,max].   Alias for torch.clamp().   Computes the element-wise conjugate of the giveninput tensor.   Create a new floating-point tensor with the magnitude of inputand the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of inputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input Tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data in input fake quantized usingscale,zero_point,quant_minandquant_max.   Alias for torch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Tests if all elements input evaluate what?",
        "Y": "toTrue",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the Tensor inputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim. Returns the mean",
        "Y": "p-norm of (input-other) Returns the log of summed exponentials",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the Tensor inputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns the median of the values in input , doing what?",
        "Y": "ignoringNaN values",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the Tensor inputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes what quantiles of each row of the input tensor along the dimension dim?",
        "Y": "q-th",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the Tensor inputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the result of Computes the q-th quantiles of each row of the input tensor along the dimension dim",
        "Y": "variant of torch.quantile()that \u201cignores\u201dNaN values",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements input evaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input .   Returns the median of the values in input , ignoringNaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile()that \u201cignores\u201dNaN values, computing the quantilesqas ifNaN values in input did not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the Tensor inputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Compute combinations of lengthrrrof the given tensor. Returns a copy of input. Returns a copy of",
        "Y": "Compute combinations of lengthrrrof the given tensor",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shape shape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Returns what wherevaluesis the cumulative minimum of elements of inputin the dimension dim?",
        "Y": "a named tuple(values,indices)",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Compute combinations of lengthrrrof the given tensor. Returns the cross product of vectors in dimension dimof input",
        "Y": "Compute combinations of lengthrrrof the given tensor",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shape shape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of inputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Compute combinations of lengthrrof the given tensor. Returns the cross product of vectors in dimension dimof inputand",
        "Y": "Compute combinations of lengthrrrof the given tensor",
        "Z": "Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of inputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements of inputin the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Computes what product, denoted by otimes, of inputandother?",
        "Y": "Kronecker",
        "Z": "Returns a copy of input.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimension dimof inputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements of inputin the dimension dim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements of inputin the dimension dim.   Returns the cumulative product of elements of inputin the dimension dim.   Returns the cumulative sum of elements of inputin the dimension dim.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1anddim2) are filled byinput.    If input is  a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of inputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of inputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of inputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of inputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements of inputin the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Performs what of matrices stored inbatch1andbatch2 with a reduced add step (all matrix multiplications get",
        "Y": "a batch matrix-matrix product",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored in input andmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Performs what of matrices stored in input andmat2?",
        "Y": "a batch matrix-matrix product",
        "Z": "Performs a batch matrix-matrix product of matrices stored in input andmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What does Alias for torch.linalg.slogdet() do?",
        "Y": "Alias",
        "Z": "Performs a batch matrix-matrix product of matrices stored in input andmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Alias for torch.linalg.matrix_power() Returns what of a 2-D tensor?",
        "Y": "numerical rank",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a named tuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "What is the householder_product of Alias for torch.linalg?",
        "Y": "householder_product",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a named tuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "Alias for torch.linalg.matrix_power() Returns the what of a 2-D tensor",
        "Y": "numerical rank",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of inputandvec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a named tuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a named tuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "This module exposes what for the__torch_function__protocol?",
        "Y": "various helper functions",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "For more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__tor",
        "Y": "SeeExtending torch",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does a dictionary that maps overridable functions in the PyTorch API unconditionally return?",
        "Y": "-1",
        "Z": "Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "These lambda functions are useful for what?",
        "Y": "testing API coverage",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are",
        "Y": "public_api(function)",
        "Z": "See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What \u2013 Iterable of arguments to check for __torch_function__ methods?",
        "Y": "relevant_args(iterable)",
        "Z": "Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "args(tuple) \u2013 what?",
        "Y": "Arbitrary positional arguments originally passed intopublic_api",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Arbitrary keyword arguments originally passed intopublic_api. what (tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api?",
        "Y": "kwargs",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Result from what?",
        "Y": "callingimplementationor an__torch_function__method",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "When does object :raises TypeError :",
        "Y": "if no implementation is found",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Check for what in the elements of the elements of the elements of the elements of the elements of the elements of the elements of the elements of the elements of",
        "Y": "__torch_function__ implementations",
        "Z": "This module exposes various helper functions for the__torch_function__protocol. SeeExtending torchfor more detail on the__torch_function__protocol. Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is iterable of arguments to check for __torch_function__ methods?",
        "Y": "relevant_args(iterable)",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Arbitrary keyword arguments originally passed intopublic_api. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_",
        "Y": "kwargs",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What are non-dispatchable?",
        "Y": "exactTensors andParameters",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":param what : Iterable or aguments to check for aguments to check for?",
        "Y": "relevant_args",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What \u2013 Arbitrary keyword arguments originally passed intopublic_api?",
        "Y": "kwargs(tuple)",
        "Z": "Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Example Check for what?",
        "Y": "__torch_function__ implementations in the elements of an iterable",
        "Z": "See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Checks if something is a Tensor-like, including an exactTensor-like, including an exactTensor",
        "Y": "bool",
        "Z": "Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Dict[Any, List[Callable]] Return a dict containing what?",
        "Y": "dummy overrides",
        "Z": "Return public functions that cannot be overridden by__torch_function__. A tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes. Set[Callable] Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What Examples Implement a function with checks for__torch_function__overrides?",
        "Y": "Dict[Callable, Callable]",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does the example check for?",
        "Y": "__torch_function__ implementations in the elements of an iterable",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Checks if something is a Tensor-like, including an exactTensor. ReturnsTrue if any of the",
        "Y": "bool",
        "Z": "Examples List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueifsTrueif",
        "Y": "bool",
        "Z": "List functions that are overridable via __torch_function__ A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A dictionary that maps namespaces that contain what?",
        "Y": "overridable functions",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "A dictionary that maps overridable functions in the PyTorch API to lambda functions unconditionally return what?",
        "Y": "-1",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the current state of a Tensor-like?",
        "Y": "Currently",
        "Z": "A dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden. Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does Dict[Callable, Callable] Examples Implement?",
        "Y": "a function with checks for__torch_function__overrides",
        "Z": "Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":param relevant_args: Iterable or aguments to check for what?",
        "Y": "__torch_function__ methods",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What ReturnsTrueif the passed-in input is a Tensor-like?",
        "Y": "ReturnsTrueif the passed-in input is a Tensor-like",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a_",
        "Y": "whenever there\u2019s a__torch_function__attribute on the type of the input",
        "Z": "Dict[Any, List[Callable]] Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Return a dict containing what?",
        "Y": "dummy overrides",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is generally a subclass of?",
        "Y": "tensor",
        "Z": "Return a dict containing dummy overrides for all overridable functions A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Where are overridable functions found?",
        "Y": "PyTorch API",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is the term for a function with checks for__torch_function__overrides?",
        "Y": "Dict",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What aren't usually used in a Tensor-like class?",
        "Y": "Built-in or user types",
        "Z": "A dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__. Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons:",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Result from what, as appropriate?",
        "Y": "callingimplementationor an__torch_function__method",
        "Z": "Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "if no implementation is found, object :raises what?",
        "Y": "TypeError",
        "Z": "Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":type relevant_args: iterable <sep> True if any of the elements of relevant_args have __torch",
        "Y": "True if any of the elements of relevant_args have __torch_function__ implementations",
        "Z": "Dict[Callable, Callable] Examples Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Built-in or user types can be made Tensor-like by what?",
        "Y": "implementing __torch_function__",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Returns True if the function passed in is a handler for a method or property belonging totorch.Tensor,",
        "Y": "Note",
        "Z": "Implement a function with checks for__torch_function__overrides. See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "If no implementation is found, what happens to TypeError?",
        "Y": "if no implementation is found",
        "Z": "See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "If properties, their__get__method must be passed in, what may be needed?",
        "Y": "may be needed",
        "Z": "See torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation. public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "If no implementation is found, object :raises what?",
        "Y": "TypeError",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "Relevant_args: Iterable or aguments to check for what?",
        "Y": "__torch_function__ methods",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What type of argument is used to check if something is a Tensor-like, including an exactTensor?",
        "Y": "bool",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What aren\u2019t usually Tensor-like?",
        "Y": "Built-in or user types",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What sometimes don\u2019t contain a__module___?",
        "Y": "Methods/properties",
        "Z": "public_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked. relevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What does a kwargs(tuple) result from?",
        "Y": "callingimplementationor an__torch_function__method",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "When does TypeError occur?",
        "Y": "if no implementation is found",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": ":type relevant_args: iterable True <sep> if any of the elements of relevant_args have __torch",
        "Y": "if any of the elements of relevant_args have __torch_function__ implementations",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "For properties, what must be passed in?",
        "Y": "their__get__method must be passed in",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What is a callable that returns an iterable of Tensor-likes?",
        "Y": "dispatcher(Callable)",
        "Z": "args(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api. kwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api. Result from callingimplementationor an__torch_function__method, as appropriate. object :raises TypeError : if no implementation is found.: Example Check for __torch_function__ implementations in the elements of an iterable.\nConsiders exactTensors andParameters non-dispatchable.\n:param relevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable True if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise. bool See also Checks if something is a Tensor-like, including an exactTensor. ReturnsTrueif the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. Built-in or user types aren\u2019t usually Tensor-like. But, they can be made Tensor-like by implementing __torch_function__. Returns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__. Note For properties, their__get__method must be passed in. This may be needed, in particular, for the following reasons: Methods/properties sometimes don\u2019t contain a__module__slot. They require that the first passed-in argument is an instance\nof torch.Tensor. Examples Wraps a given function with__torch_function__-related functionality. dispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function. Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available. Examples",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "What operation does inkTkHkWkT times kWkTkHkWregions by step sizes",
        "Y": "3D average-pooling",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.   Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version ofthreshold().   Applies the rectified linear unit function element-wise.   In-place version ofrelu().   Applies the HardTanh function element-wise.   In-place version ofhardtanh().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does 3D fractional max pooling over an input signal composed of several input planes do?",
        "Y": "Thre",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.   Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse ofMaxPool1d.   Computes a partial inverse ofMaxPool2d.   Computes a partial inverse ofMaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version ofthreshold().   Applies the rectified linear unit function element-wise.   In-place version ofrelu().   Applies the HardTanh function element-wise.   In-place version ofhardtanh().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion for combining log_softmaxandnll_lossin a single function?",
        "Y": "combineslog_softmaxandnll_lossin a single function",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   SeeSoftMarginLossfor details.   SeeTripletMarginLossfor details   SeeTripletMarginWithDistanceLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the function that measures the Gaussian negative log likelihood loss?",
        "Y": "SeeHingeEmbeddingLoss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   SeeSoftMarginLossfor details.   SeeTripletMarginLossfor details   SeeTripletMarginWithDistanceLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Function that uses what if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise?",
        "Y": "squared term",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   SeeSoftMarginLossfor details.   SeeTripletMarginLossfor details   SeeTripletMarginWithDistanceLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the function that uses a squared term if the absolute element-wise error falls below delta and an L1 term otherwise",
        "Y": "SeeSoftMarginLoss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   SeeCosineEmbeddingLossfor details.   This criterion combineslog_softmaxandnll_lossin a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   SeeHingeEmbeddingLossfor details.   TheKullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   SeeMarginRankingLossfor details.   SeeMultiLabelMarginLossfor details.   SeeMultiLabelSoftMarginLossfor details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   SeeSoftMarginLossfor details.   SeeTripletMarginLossfor details   SeeTripletMarginWithDistanceLossfor details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What product is returned if both tensors are 1-dimensional?",
        "Y": "Matrix product of two tensors",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "The non-matrix dimensions are Broadcasted and therefore must be what?",
        "Y": "broadcastable",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions?",
        "Y": "the broadcasting logic",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "If both tensors are what, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix",
        "Y": "1-dimensional",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "The matrix-matrix product is returned if the first argument is what dimension?",
        "Y": "1-dimensional",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "If both arguments are at least 1-dimensional and at least one argument is N-dimensional, what is returned?",
        "Y": "batched matrix multiply",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, If input is  a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, If input is  a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What is a GPU tensor?",
        "Y": "CPU tensor",
        "Z": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of torch?",
        "Y": "64-bit floating point",
        "Z": "64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor().",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is sometimes referred to as binary16?",
        "Y": "Sometimes referred",
        "Z": "64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor().",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is Brain Float sometimes referred to as?",
        "Y": "Brain Float",
        "Z": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor().",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How many bits floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.",
        "Y": "16",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Brain Floating Point uses 1 sign, 8 exponent, and 7 significand bits. Useful when precision is important at the expense of range",
        "Y": "Use",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How many bits are floating point1?",
        "Y": "16",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Useful when precision is important at the expense of range, since it has the same number of exponent bits asfloat32 quantized 4-bit integer is",
        "Y": "range is important",
        "Z": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the current state of the torch?",
        "Y": "Currently it",
        "Z": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Where is EmbeddingBag only supported?",
        "Y": "EmbeddingBag operator",
        "Z": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a float32 quantized 4-bit integer stored as?",
        "Y": "8-bit signed integer",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is an EmbeddingBag operator?",
        "Y": "torch.Tensoris",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a quantized 4-bit integer stored as?",
        "Y": "8-bit signed integer",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "A tensor can be constructed from what?",
        "Y": "Pythonlistor",
        "Z": "torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "A tensor can be constructed from what sequence?",
        "Y": "Pythonlistor",
        "Z": "torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What do you have if you want a tensor to be constructed from a Pythonlistor sequence?",
        "Y": "Tensordata",
        "Z": "torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If you have a Tensordataand just want to change what?",
        "Y": "itsrequires_gradflag, userequires_grad_()ordetach",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of integer (signed) is torch?",
        "Y": "16-bit",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is used to avoid a copy of a Tensordata?",
        "Y": "userequires_grad_()ordetach()",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a way to avoid a copy of a numpy array?",
        "Y": "usetorch.as_tensor()",
        "Z": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "A tensor of what can be a tensor of?",
        "Y": "specific data type",
        "Z": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What sequence can a tensor be constructed from?",
        "Y": "Pythonlistor",
        "Z": "torch.Byte Tensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If you have a numpy array and want to avoid a copy, what is the best way to avoid a copy?",
        "Y": "usetorch.as_tensor()",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.Byte Tensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.Byte Tensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is used to avoid a copy of a numpy array?",
        "Y": "usetorch.as_tensor()",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "A tensor of what can be constructed by passing a torch.dtypeand/or a torch.device to a construct",
        "Y": "specific data type",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are Tensor Views?",
        "Y": "tensor views",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are some examples of a torch.Tensor Attributes?",
        "Y": "thetorch.dtype,torch.device, and torch.layout attributes",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing a torch.dtypeand/or a torch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, and torch.layout attributes of a torch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation of torch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.argmax stand for?",
        "Y": "Tensor.argmax",
        "Z": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). Tensor.bfloat16",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Add a scalar or tensor toselftensor do?",
        "Y": "Add a scalar or tensor toselftensor",
        "Z": "Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the element that adds a scalar or tensor toselftensor?",
        "Y": "addbmm",
        "Z": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). Tensor.bfloat16",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Alias forabs() Tensor.absolute stand for?",
        "Y": "Alias forabs() Tensor.absolute",
        "Z": "Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p).",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What can be constructed by providing the two tensors of indices and values, as well as the size of the sparse",
        "Y": "A sparse COO tensor",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What dimensionality of a tensor is the sum of the number of sparse and dense dimensions?",
        "Y": "M+K==len(s.shape)==s.ndim",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What permitsuncoalescedsparse tensors?",
        "Y": "PyTorch sparse COO tensor format",
        "Z": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is an example of a sparse COO tensor format that permitsuncoalescedsparse tens",
        "Y": "For example, one",
        "Z": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What format permitsuncoalescedsparse tensors?",
        "Y": "PyTorch sparse COO tensor",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the definition of a dimensional tensor?",
        "Y": "(2 + 1)",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the location of a 2 + 1)-dimensional tensor?",
        "Y": "(1, 0",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the sum of the number of sparse and dense dimensions in a tensor?",
        "Y": "M+K==len(s.shape)==s.ndim",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What sparse COO tensor format permitsuncoalescedsparse tensors?",
        "Y": "PyTorch",
        "Z": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Suppose we want to create a 2 + 1-dimensional tensor with the entry [3, 4] at location (0, 2), entry",
        "Y": "Note For the most part",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What kind of uncoalesced tensor can one specify multiple values,3and4, for the same index1 that leads to?",
        "Y": "1-D",
        "Z": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What do most operations work identically given a coalesced or uncoalesced sparse tensor?",
        "Y": "coalesced or not",
        "Z": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Addition of what is implemented by simply concatenating the indices and values tensors?",
        "Y": "sparse COO tensors",
        "Z": "s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the lexico of sparse tensors?",
        "Y": "lexico",
        "Z": "s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is an example of a sparse COO tensor?",
        "Y": "sparse COO tens",
        "Z": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Most operations will work identically given what two types of sparse tensors?",
        "Y": "coalesced or uncoalesced sparse tensor",
        "Z": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be acquired using methodstorch.Tensor.indices() and torch.Tensor.indices()?",
        "Y": "COO format data",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the output of torch.Tensor.coalesce()method?",
        "Y": "a sparse tensor",
        "Z": "In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be acquired using methodstorch.Tensor.indices() and torch.Tensor.values()?",
        "Y": "COO format data",
        "Z": "Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can one acquire only when a sparse COO tensor is acquired?",
        "Y": "COO format data",
        "Z": "In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Which tensor is coalesced or not?",
        "Y": "sparse tensor",
        "Z": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be acquired using methodstorch.Tensor.indices() and usetorch.Tensor.values()",
        "Y": "COO format data",
        "Z": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is used to acquire the COO format data of an uncoalesced tensor?",
        "Y": "usetorch",
        "Z": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.Tensor.is_coalesced()returnsTrue?",
        "Y": "Note",
        "Z": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What represents a multi-dimensional array containing elements of a single data type?",
        "Y": "PyTorch providestorch.Tensorto",
        "Z": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What term is used to denote unspecified elements in sparse arrays?",
        "Y": "fill value",
        "Z": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What ordering of indices can be advantageous for implementing algorithms that involve many element selection operations?",
        "Y": "lexicographical",
        "Z": "For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products?",
        "Y": "lexicographical",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of nonlinear tensor can be implemented by multiplying all the uncoalesced values with the scalar because",
        "Y": "nonline",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What cannot be implemented by applying the operation to uncoalesced data becausesqrt(a+b)==sqrt",
        "Y": "nonlinear operation",
        "Z": "Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()and torch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()and torch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch sparse COO tensor format permits what in the indices?",
        "Y": "duplicate coordinates",
        "Z": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thecrow_indicestensor consists of compressed row indices. This is what?",
        "Y": "1-D tensor of sizesize[0]+1",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the CSR sparse tensor encode the index invaluesandcol_indicesdepending on?",
        "Y": "where the given row starts",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thecol_indicestensor contains the column indices of each value. This is a 1-D tensor of what?",
        "Y": "sizennz",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Thevaluestensor contains the values of the CSR tensor. This is a what?",
        "Y": "1-D tensor of sizennz",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What must the user supply?",
        "Y": "row and column indices and values tensors separately",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Return the number of dense dimensions in a sparse tensorself?",
        "Y": "Tensor.dense_dim",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Return the number of sparse dimensions in a sparse tensorself?",
        "Y": "Tensor.sparse_dim",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned by the sparse tensormask?",
        "Y": "newsparse tensor",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What _ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Tensor.is_coalesced Returns what?",
        "Y": "Trueifselfis a sparse COO tensorthat is coalesced",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What creates a strided copy ofself?",
        "Y": "Tensor.to_dense",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns the tensor containing the compressed row indices?",
        "Y": "Tensor.crow_indices",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis a sparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()is NaN()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "For what components can a real tensor have an extra last dimension?",
        "Y": "real and imaginary components",
        "Z": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What expression does the STFT compute?",
        "Y": "wheremmmis the index of the sliding window",
        "Z": "Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "inputmust be either a what?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Windowwill be padded on both sides to what before being applied?",
        "Y": "lengthn_fft",
        "Z": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting that allows input to be padded on both sides so that thettt-th frame is centered?",
        "Y": "IfcenterisTrue",
        "Z": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Inputwill be padded on both sides so that thettt-th frame is centered at timethop_lengtht",
        "Y": "IfcenterisTrue",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Seetorch.nn.functional.pad() for what?",
        "Y": "all available options",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting for the padding method used oninputwhencenterisTrue?",
        "Y": "\"reflect\"",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "For what type of input is IfonesidedisTrue default?",
        "Y": "real input",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If the input or window tensors are complex, thenonesidedoutputisTrue(default for real input), only values for",
        "Y": "if the input or window tensors are complex",
        "Z": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Ifreturn_complexisTrue(default if input is complex), the return is what?",
        "Y": "ainput.dim()+1dimensional complex tensor",
        "Z": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the output of ainput.dim()+2dimensional real tensor?",
        "Y": "ainput.dim()+2dimensional real tensor",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Where*is what?",
        "Y": "the optional batch size of input",
        "Z": "IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\"",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "input(Tensor) \u2013 the input tensor n_fft(int) \u2013 the input ten",
        "Y": "Fourier transform",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the option to padinputon both sides so that thettt-th frame is centered at timethop_lengtht",
        "Y": "center(bool,optional)",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "IfnormalizedisTrue(default isFalse) returns the normalized STFT results?",
        "Y": "Default",
        "Z": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size of input,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Returns the indices of the upper triangular part of arowbycolmatrix in what?",
        "Y": "2-by-N Tensor",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "The upper triangular part of the matrix is defined as what?",
        "Y": "elements on and above the diagonal",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "Ifoffset=0, all elements on and above the main diagonal are retained?",
        "Y": "Ifoffset= 0, all elements on and above the main diagonal are retained",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "The set of indices(i,i)lbrace (i, i) rbrace(i,",
        "Y": "main diagonal",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "Why does row*colmust be less than259259259?",
        "Y": "to prevent overflow during calculation",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What are the default data types of returned tensors?",
        "Y": "ifNone,torch.long",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is currently only supporttorch supported?",
        "Y": "layout",
        "Z": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What does Sets the seed for generating random numbers to a random number for the current GPU?",
        "Y": "Sets the seed for generating random numbers to a random number for the current GPU",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns the currently selectedStreamfor a given device. Returns what for a given device?",
        "Y": "defaultStream",
        "Z": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Gets the cuda capability of a device. Gets the name of a device. Gets the properties of a device. Return",
        "Y": "Gets the name of a device",
        "Z": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does comm.broadcast broadcast to specified GPU devices?",
        "Y": "comm.broadcast Broadcasts a tensor",
        "Z": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "comm.broadcast Broadcasts a tensor to specified GPU devices?",
        "Y": "comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPU",
        "Z": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Which entity selects a given stream?",
        "Y": "Context-manager",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does comm.reduce_add Sums tens tens?",
        "Y": "comm.reduce_add Sums tens",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "comm.reduce_add Sums tensors from multiple GPUs?",
        "Y": "comm.s",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns what this library was compiled for?",
        "Y": "list CUDA architectures",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Gets the capability of a device. Gets the name of a device. Gets the properties of a device. Gets the properties",
        "Y": "cuda",
        "Z": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What Gathers tensors from multiple GPUs?",
        "Y": "comm.gather",
        "Z": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Releases what currently held by the caching allocator?",
        "Y": "all unoccupied cached memory",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is the name of the function that releases all unoccupied cached memory currently held by the caching allocator?",
        "Y": "seemax_memory_reserved()",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Computes the error function of input. The error function is defined as follows: input(Tensor) \u2013 the input ten",
        "Y": "output tensor",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the name of the function that Computes the inverse error function of input?",
        "Y": "Computes the inverse error function of input",
        "Z": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is an example of a Computes the exponential of the elements minus 1 of input?",
        "Y": "Computes the exponential of the elements minus 1 of input",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the elements minus 1 of input. Note This function provides greater precision than exp(x) - 1 for small values of",
        "Y": "exponential",
        "Z": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the first kind of what function for each element of input?",
        "Y": "exponentially scaled zeroth order modified Bessel function",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the first kind for each element of input. input(Tensor) \u2013 the input tensor. out(",
        "Y": "exponentially scaled zeroth order modified Bessel function",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.input is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one of inputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "If the value contains what that reside on GPUs, Future.done()will returnTrueeven if the asynchronous kernels that",
        "Y": "tensors",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does aFuturecannot be marked twice?",
        "Y": "aFuturecannot be marked completed twice",
        "Z": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of what?",
        "Y": "thisFuture",
        "Z": "is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, what will Future.done() returnTrueeven?",
        "Y": "if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device",
        "Z": "If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a limitation of thisFuture?",
        "Y": "aFuturecannot be marked completed twice",
        "Z": "If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "To enforce a certain order, what can be added to the sameFuture, but the order in which they will be executed cannot be guaranteed?",
        "Y": "chaining:fut.then(cb1).then(cb2)",
        "Z": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "To enforce a certain order, what can be used to add multiple callbacks to the sameFuture?",
        "Y": "chaining:fut.then(cb1).then(cb2)",
        "Z": "result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "To enforce a certain order in which callbacks will be executed, what can be used?",
        "Y": "chaining:fut.then(cb1).then(cb2)",
        "Z": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What can the callback function use to get the value of thisFuture?",
        "Y": "thevalue()method",
        "Z": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "The callback will be invoked with some dedicated streams set as what?",
        "Y": "current",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "To enforce a certain order, what can be added to the sameFuture?",
        "Y": "chaining:fut.then(cb1).then(cb2)",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If theFuture's value contains tensors that reside on GPUs, the callback might be invoked when?",
        "Y": "while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the future's value contain that reside on GPUs?",
        "Y": "tensors",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What are some dedicated streams set as?",
        "Y": "current",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the behavior of wait() similar to?",
        "Y": "non-blocking behavior",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernel",
        "Y": "if the callback returns a value that contains tensors that reside on a GPU",
        "Z": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does is_tensor return ?",
        "Y": "True or False",
        "Z": "is_tensor Returns True if objis a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Is_tensor returns true or false if obj is  a PyTorch tensor?",
        "Y": "True",
        "Z": "is_tensor Returns True if obj is  a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Is_tensor Returns what?",
        "Y": "True or False",
        "Z": "is_tensor Returns True if obj is  a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What  does Is_tensor Return ?",
        "Y": "True or False",
        "Z": "is_tensor Returns True if obj is  a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Is_storage Returns True or False if obj is  a PyTorch storage object?",
        "Y": "True",
        "Z": "is_storage Returns True if obj is  a PyTorch storage object.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "When is_storage return True?",
        "Y": "if the  object is a PyTorch storage object.",
        "Z": "is_storage Returns True if obj is  a PyTorch storage object.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Is_storage Return?",
        "Y": "True or False",
        "Z": "is_storage Returns True if obj is  a PyTorch storage object.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens if the data type of input is a complex data type?",
        "Y": "is_complex Returns True",
        "Z": "is_complex Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Returns True if the data type of input is a complex data type?",
        "Y": "is_complex",
        "Z": "is_complex Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens if the data type of input is a floating point data type?",
        "Y": "is_floating_point Returns True",
        "Z": "is_floating_point Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Returns True if the data type of input is a floating point data type?",
        "Y": "is_floating_point",
        "Z": "is_floating_point Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens if the input is a single element tensor which is not equal to zero after type conversions?",
        "Y": "is_nonzero Returns True",
        "Z": "is_nonzero Returns True if the input is a single element tensor which is not equal to zero after type conversions.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Returns True if the input is a single element tensor which is not equal to zero after type conversions?",
        "Y": "is_nonzero",
        "Z": "is_nonzero Returns True if the input is a single element tensor which is not equal to zero after type conversions.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What sets the default floating point dtype to d?",
        "Y": "set_default_dtype",
        "Z": "set_default_dtype Sets the default floating point dtype to d.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How to get  the current default floating point torch.dtype?",
        "Y": "use  get_default_dtype",
        "Z": "get_default_dtype Get the current default floating point torch.dtype.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is function that returns the current floating point torch.dtype?",
        "Y": "get_default_dtype",
        "Z": "get_default_dtype Get the current default floating point torch.dtype.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is fuction to  get current floating point torch.dtype?",
        "Y": "get_default_dtype",
        "Z": "get_default_dtype Get the current default floating point torch.dtype.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What sets the default torch.Tensor type to floating point tensor type t?",
        "Y": "set_default_tensor_type",
        "Z": "set_default_tensor_type Sets the default torch.Tensor type to floating point tensor type t.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What  type of  Tensor Type  does  set_default_tensor_type Sets the default torch.Tensor type to?",
        "Y": "floating point",
        "Z": "set_default_tensor_type Sets the default torch.Tensor type to floating point tensor type t.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "numel Returns the total number of elements in what?",
        "Y": "the input tensor",
        "Z": "numel Returns the total number of elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns the total number of elements in the input tensor?",
        "Y": "numel",
        "Z": "numel Returns the total number of elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is method  that sets options for printing?",
        "Y": "set_printoptions",
        "Z": "set_printoptions Set options for printing.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What disables denormal floating numbers on the CPU?",
        "Y": "set_flush_denormal",
        "Z": "set_flush_denormal Disables denormal floating numbers on CPU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does  set_flush_denormal do",
        "Y": "set_flush_denormal Disables denormal floating numbers on CPU.",
        "Z": "set_flush_denormal Disables denormal floating numbers on CPU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the use of tensor?",
        "Y": "tensor Constructs a tensor with data.",
        "Z": "tensor Constructs a tensor withdata.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "what is  the method  to Constructs a tensor withdata?",
        "Y": "tensor",
        "Z": "tensor Constructs a tensor withdata.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does sparse_coo_tensor construct?",
        "Y": "a sparse tensor",
        "Z": "sparse_coo_tensor Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Constructs asparse tensor in COO(rdinate) format?",
        "Y": "sparse_coo_tensor",
        "Z": "sparse_coo_tensor Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "In what format does sparse_coo_tensor construct asparse tensor?",
        "Y": "COO(rdinate)",
        "Z": "sparse_coo_tensor Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does as_tensor convert the data into?",
        "Y": "a torch.Tensor",
        "Z": "as_tensor Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What converts the data into a torch.Tensor?",
        "Y": "as_tensor",
        "Z": "as_tensor Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of view creates a view of an existingtorch.Tensorinput with specifiedsize,stride andstor",
        "Y": "as_strided",
        "Z": "as_strided Create a view of an existingtorch.Tensorinput with specifiedsize,strideandstorage_offset.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What creates a view of an existingtorch.Tensorinput with specifiedsize,strideandstorage_off",
        "Y": "as_strided",
        "Z": "as_strided Create a view of an existingtorch.Tensorinput with specifiedsize,strideandstorage_offset.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does from_numpy create a Tensor from?",
        "Y": "a numpy.ndarray",
        "Z": "from_numpy Creates a Tensor froma numpy.ndarray.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does from_numpy create?",
        "Y": "a Tensor",
        "Z": "from_numpy Creates a Tensor froma numpy.ndarray.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does zeros_like return?",
        "Y": "a tensor filled with the scalar value 0",
        "Z": "zeros_like Returns a tensor filled with the scalar value 0, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "what type of function does one return?",
        "Y": "tensor filled with the scalar value 1",
        "Z": "ones Returns a tensor filled with the scalar value 1  , with the shape defined by the variable argument size.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "what does ones Return return?",
        "Y": "a tensor",
        "Z": "ones Returns a tensor filled with the scalar value 1  , with the shape defined by the variable argument size.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does one return that is filled with the scalar value 1  ?",
        "Y": "a tensor",
        "Z": "ones_like Returns a tensor filled with the scalar value 1  , with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does ones_like return?",
        "Y": "a tensor",
        "Z": "ones_like Returns a tensor filled with the scalar value 1  , with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does arange return?",
        "Y": "a 1-D tensor",
        "Z": "arange Returns a 1-D tensor of size  ceilling((end -start) /step) with values from the interval[start,end)taken with common difference step beginning from start.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the use of  ops range?",
        "Y": "a 1-D tensor",
        "Z": "range Returns a 1-D tensor of size  ceilling((end -start) /step)  +1 with values from start to end with step step.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What  does range Return?",
        "Y": "a 1-D tensor",
        "Z": "range Returns a 1-D tensor of size  ceilling((end -start) /step)  +1 with values from start to end with step step.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What creates a one-dimensional tensor of size steps?",
        "Y": "logspace",
        "Z": "logspace Creates a one-dimensional tensor of size steps whose values are evenly spaced from base to the power start  to  base to the power end, inclusive, on a logarithmic scale with base base.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end?",
        "Y": "linspace",
        "Z": "linspace Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is logarithmic scale used to create a one-dimensional tensor of size steps?",
        "Y": "basebase",
        "Z": "logspace Creates a one-dimensional tensor of size steps whose values are evenly spaced from base to the power start  to  base to the power end, inclusive, on a logarithmic scale with base base.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a 2-D tensor with ones on the diagonal and zeros elsewhere?",
        "Y": "eye",
        "Z": "eye Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "empty Returns a tensor filled with what?",
        "Y": "uninitialized data",
        "Z": "empty Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does empty_strided Return ?",
        "Y": "empty_strided Returns a tensor filled with uninitialized data.",
        "Z": "empty_strided Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns an uninitialized tensor with the same size as input?",
        "Y": "empty_like",
        "Z": "empty_like Returns an uninitialized tensor with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of data does empty_strided return?",
        "Y": "uninitialized data",
        "Z": "empty_strided Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "empty_strided Returns a tensor filled with what?",
        "Y": "uninitialized data",
        "Z": "empty_strided Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does full create a tensor of?",
        "Y": "with fill_value",
        "Z": "full Creates a tensor of  size filled with fill_value.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the  size filled with fill_value?",
        "Y": "tensor",
        "Z": "full Creates a tensor of  size filled with fill_value.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Create a tensor of  size filled with fill_value.?",
        "Y": "full",
        "Z": "full Creates a tensor of  size filled with fill_value.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor with the same size as input filled with fill_value?",
        "Y": "full_like",
        "Z": "full_like Returns a tensor with the same size as input filled with fill_value.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "full_like Returns a what with the same size as input filled with fill_value?",
        "Y": "tensor",
        "Z": "full_like Returns a tensor with the same size as input filled with fill_value.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor does quantize_per_tensor convert to?",
        "Y": "float tensor",
        "Z": "quantize_per_tensor Converts a float tensor to a quantized tensor with given scale and zero point.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the scale of a quantized tensor?",
        "Y": "zero point",
        "Z": "quantize_per_tensor Converts a float tensor to a quantized tensor with given scale and zero point.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor does quantize_per_tensor convert a float tensor to?",
        "Y": "quantized tensor with given scale and zero point.",
        "Z": "quantize_per_tensor Converts a float tensor to a quantized tensor with given scale and zero point.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor does quantize_per_tensor convert?",
        "Y": "float",
        "Z": "quantize_per_tensor Converts a float tensor to a quantized tensor with given scale and zero point.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Quantize_per_channel Converts a float tensor to what?",
        "Y": "per-channel quantized tensor with given scales and zero points.",
        "Z": "quantize_per_channel Converts a float tensor to a per-channel quantized tensor with given scales and zero points.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the value of the scales used to convert a float tensor to a per-channel quantized tensor",
        "Y": "zero points",
        "Z": "quantize_per_channel Converts a float tensor to a per-channel quantized tensor with given scales and zero points.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does quantize_per_channel convert a float tensor to?",
        "Y": "quantized tensor with given scales and zero points.",
        "Z": "quantize_per_channel Converts a float tensor to a per-channel quantized tensor with given scales and zero points.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns an fp32 Tensor by dequantizing a quantized Tensor?",
        "Y": "dequantize",
        "Z": "dequantize Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "what part of a complex tensor is equal to real?",
        "Y": "real part",
        "Z": "complex Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "what part of a complex tensor is equal to imag?",
        "Y": "imaginary part",
        "Z": "complex Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What are the elements of polar Construct a complex tensor?",
        "Y": "Cartesian coordinates",
        "Z": "polar Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand",
        "Y": "polar Constructs",
        "Z": "polar Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What are the elements of polar Constructs a complex tensor?",
        "Y": "Cartesian coordinates",
        "Z": "polar Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does heaviside compute for each element in input?",
        "Y": "Heaviside step function",
        "Z": "heaviside Computes the Heaviside step function for each element in input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Computes the Heaviside step function for each element in input?",
        "Y": "heaviside",
        "Z": "heaviside Computes the Heaviside step function for each element in input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What concatenates the given sequence of seq tensors in the given dimension?",
        "Y": "cat",
        "Z": "cat Concatenates the given sequence of seq tensors in the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does cat concatenate in a given dimension?",
        "Y": "given sequence of seq tensors",
        "Z": "cat Concatenates the given sequence of seq tensors in the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "cat Concatenates the given sequence of seq tensors in what?",
        "Y": "given dimension",
        "Z": "cat Concatenates the given sequence of seq tensors in the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does a chunk do?",
        "Y": "Splits a tensor into a specific number of chunks",
        "Z": "chunk Splits a tensor into a specific number of chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What splits a tensor into a specific number of chunks?",
        "Y": "chunk",
        "Z": "chunk Splits a tensor into a specific number of chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "According to What ,does dsplit Split input's depthwise tensors ?",
        "Y": "indices_or_sections",
        "Z": "dsplit Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How many dimensions does dsplit Splits input have?",
        "Y": "three or more dimensions",
        "Z": "dsplit Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "dsplit Splits input is divided into multiple tensors according to what?",
        "Y": "indices",
        "Z": "dsplit Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What creates a new tensor by horizontally stacking the tensors in tensors?",
        "Y": "column_stack",
        "Z": "column_stack Creates a new tensor by horizontally stacking the tensors in tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How does column_stack create a new tensor?",
        "Y": "horizontally stacking",
        "Z": "column_stack Creates a new tensor by horizontally stacking the tensors in tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Along what axis are dstack Stack tensors in sequence depthwise?",
        "Y": "third axis",
        "Z": "dstack Stack tensors in sequence depthwise (along third axis).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is  method  that  stack tensors in sequence depthwise?",
        "Y": "dstack",
        "Z": "dstack Stack tensors in sequence depthwise (along third axis).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does dstack do?",
        "Y": "dstack Stack tensors in sequence depthwise",
        "Z": "dstack Stack tensors in sequence depthwise (along third axis).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does gather do?",
        "Y": "Gathers values along an axis specified bydim",
        "Z": "gather Gathers values along an axis specified bydim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the axis specified by gather?",
        "Y": "bydim",
        "Z": "gather Gathers values along an axis specified bydim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What gathers values along an axis specified bydim?",
        "Y": "gather",
        "Z": "gather Gathers values along an axis specified bydim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the method  that  splits a tensor horizontally?",
        "Y": "hsplit",
        "Z": "hsplit Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does hsplit Splits input have?",
        "Y": "A tensr with one or more dimensions",
        "Z": "hsplit Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How does hsplit Splits input split into multiple tensors?",
        "Y": "horizontally into multiple tensors",
        "Z": "hsplit Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How are hstack Stack tensors sequenced?",
        "Y": "horizontally",
        "Z": "hstack Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "In what way hstack Stack tensors in sequence ?",
        "Y": "horizontally",
        "Z": "hstack Stack tensors in sequence horizontally (column wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a new tensor which indexes the input tensor along dimension dim using the entries inindex?",
        "Y": "index_select",
        "Z": "index_select Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a Long Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a new 1-D tensor which indexes the input tensor according to the boolean mask?",
        "Y": "masked_select",
        "Z": "masked_select Returns a new 1-D tensor which indexes the input tensor according to the booleanmask which using is a Bool Tensor .",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor does masked_select return?",
        "Y": "1-D tensor which indexes the input tensor according to the boolean mask",
        "Z": "masked_select Returns a new 1-D tensor which indexes the input tensor according to the booleanmask which using is a Bool Tensor .",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the boolean mask?",
        "Y": "a Bool Tensor",
        "Z": "masked_select Returns a new 1-D tensor which indexes the input tensor according to the booleanmask which using is a Bool Tensor .",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does masked_select return?",
        "Y": "1-D tensor",
        "Z": "masked_select Returns a new 1-D tensor which indexes the input tensor according to the booleanmask which using is a Bool Tensor .",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does movedim do?",
        "Y": "Moves the dimension(s) of input at  the position(s) in source to the position(s) in destination",
        "Z": "movedim Moves the dimension(s) of input at  the position(s) in source to the position(s) in destination.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Moves the dimension(s) of input at  the position(s) in source to the position(s) in destination?",
        "Y": "movedim",
        "Z": "movedim Moves the dimension(s) of input at  the position(s) in source to the position(s) in destination.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is moveaxis?",
        "Y": "Alias for torch.movedim",
        "Z": "moveaxis Alias for torch.movedim().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias for torch.movedim()?",
        "Y": "moveaxis",
        "Z": "moveaxis Alias for torch.movedim().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is a narrowed version of input tensor?",
        "Y": "a new tensor",
        "Z": "narrow Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of return does reshape return?",
        "Y": "tensor with the same data and number of elements as input, but with the specified shape.",
        "Z": "reshape Returns a tensor with the same data and number of elements as input, but with the specified shape.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor with the same data and number of elements as input?",
        "Y": "reshape",
        "Z": "reshape Returns a tensor with the same data and number of elements as input, but with the specified shape.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does reshape return with the same data and number of elements as input?",
        "Y": "a tensor",
        "Z": "reshape Returns a tensor with the same data and number of elements as input, but with the specified shape.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias of torch.vstack()?",
        "Y": "row_stack",
        "Z": "row_stack Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is row_stack?",
        "Y": "Alias of torch.vstack",
        "Z": "row_stack Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias for torch.vstack()?",
        "Y": "row_stack",
        "Z": "row_stack Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is out-of-place version of torch.Tensor.scatter_()?",
        "Y": "Out-of-place version of torch.Tensor.scatter_()",
        "Z": "scatter Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is a version of of torch.Tensor.scatter_()?",
        "Y": "scatter Out-of-place",
        "Z": "scatter Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is out-of-place version of torch.Tensor.scatter_add_()?",
        "Y": "Out-of-place version of torch.Tensor.scatter_add_()",
        "Z": "scatter_add Out-of-place version of torch.Tensor.scatter_add_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What version of of torch.Tensor.scatter_add_()?",
        "Y": "scatter_add Out-of-place",
        "Z": "scatter_add Out-of-place version of torch.Tensor.scatter_add_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Out-of-place version of of torch.Tensor.scatter_add_()?",
        "Y": "scatter_add",
        "Z": "scatter_add Out-of-place version of torch.Tensor.scatter_add_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does split do to a tensor?",
        "Y": "split Splits the tensor into chunks",
        "Z": "split Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "split Splits the tensor into what?",
        "Y": "chunks",
        "Z": "split Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "The tensor returns a tensor with all the dimensions of input of what?",
        "Y": "size 1",
        "Z": "squeeze Returns a tensor with all the dimensions of input of size 1 removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does squeeze Returns return?",
        "Y": "a tensor",
        "Z": "squeeze Returns a tensor with all the dimensions of input ofsize 1 removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Concatenates a sequence of tensors along a new dimension?",
        "Y": "stack",
        "Z": "stack Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "stack Concatenates a sequence of what?",
        "Y": "tensors",
        "Z": "stack Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What concatenates a sequence of tensors along a new dimension?",
        "Y": "stack",
        "Z": "stack Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is Alias for torch.transpose()?",
        "Y": "swapdims",
        "Z": "swapdims Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is  swapaxes?",
        "Y": "Alias for torch.transpose()",
        "Z": "swapaxes Alias for torch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "what  input  tensor does   t Expect o be and transposes dimensions 0 and 1?",
        "Y": "2-D tensor",
        "Z": "t Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What are the elements of input to  ake?",
        "Y": "the given indices",
        "Z": "take Returns a new tensor with the elements of input at  the given indices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Take_along_dim Selects values from what indices from the given dim?",
        "Y": "1-dimensional indices",
        "Z": "take_along_dim Selects values from input at the 1-dimensional indices from indices along the given dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What selects values from input at the 1-dimensional indices from indices along the given dim?",
        "Y": "take_along_dim",
        "Z": "take_along_dim Selects values from input at the 1-dimensional indices from indices along the given dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does tensor_split do?",
        "Y": "tensor_split Splits a tensor into multiple sub-tensors",
        "Z": "tensor_split Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What are all sub-tensors of a tensor for tensor_split?",
        "Y": "views of input",
        "Z": "tensor_split Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does tensor_split use to split a tensor into multiple sub-tensors?",
        "Y": "indices  or number of sections specified by indices_or_sections",
        "Z": "tensor_split Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What constructs a tensor by repeating the elements of input?",
        "Y": "tile",
        "Z": "tile Constructs a tensor by repeating the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Constructs a tensor by repeating the elements of input?",
        "Y": "tile",
        "Z": "tile Constructs a tensor by repeating the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does transpose return?",
        "Y": "tensor which is transpose of input tensor",
        "Z": "transpose Returns a tensor that is a transposed version of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor that is a transposed version of input?",
        "Y": "transpose",
        "Z": "transpose Returns a tensor that is a transposed version of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does transpose return that is a transposed version of input?",
        "Y": "tensor",
        "Z": "transpose Returns a tensor that is a transposed version of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What removes a tensor dimension?",
        "Y": "unbind",
        "Z": "unbind Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How to remove a tensor dimension?",
        "Y": "use unbind",
        "Z": "unbind Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does unsqueeze return a new tensor with?",
        "Y": "a dimension of size one inserted at the specified position",
        "Z": "unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does unsqueeze Return?",
        "Y": "a new tensor",
        "Z": "unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does vsplit Split input's tensors vertically vary according to?",
        "Y": "indices_or_sections",
        "Z": "vsplit Splits input, a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is input to vsplit?",
        "Y": "a tensor with two or more dimensions",
        "Z": "vsplit Splits input, a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How are vstack Stack tensors organized?",
        "Y": "row wise",
        "Z": "vstack Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How are vstack Stack tensors sequenced?",
        "Y": "vertically",
        "Z": "vstack Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does  vstack do?",
        "Y": "vstack Stack tensors in sequence vertically",
        "Z": "vstack Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "vstack Stack tensors in sequence vertically based on what?",
        "Y": "row wise",
        "Z": "vstack Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the return value of elements selected from either x or y?",
        "Y": "tensor",
        "Z": "where Return a tensor of elements selected from either x or y, depending on condition.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Where are the tensor of elements selected from?",
        "Y": "either x or y",
        "Z": "where Return a tensor of elements selected from either x or y, depending on condition.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Generator create and return that manages the state of the algorithm?",
        "Y": "generator object",
        "Z": "Generator Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "To what  does seed Set the seed for generating random numbers?",
        "Y": "non-deterministic random number",
        "Z": "seed Sets the seed for generating random numbers to a non-deterministic random number.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does set the seed for generating random numbers?",
        "Y": "manual_seed",
        "Z": "manual_seed Sets the seed for generating random numbers.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "what for manual_seed Set the seed ?",
        "Y": "for generating random numbers",
        "Z": "manual_seed Sets the seed for generating random numbers.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does return the initial seed for generating random numbers as a Python  long?",
        "Y": "initial_seed",
        "Z": "initial_seed Returns the initial seed for generating random numbers as a Python  long.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "initial_seed Returns the initial seed for generating random numbers as  what?",
        "Y": "Python long",
        "Z": "initial_seed Returns the initial seed for generating random numbers as a Python  long.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does initial_seed return for generating random numbers as a Python  long?",
        "Y": "initial seed",
        "Z": "initial_seed Returns the initial seed for generating random numbers as a Python  long.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns the random number generator state as a torch.ByteTensor?",
        "Y": "get_rng_state",
        "Z": "get_rng_state Returns the random number generator state as a torch.ByteTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is random number generator state returned by get_rng_state?",
        "Y": "torch.ByteTensor",
        "Z": "get_rng_state Returns the random number generator state as a torch.ByteTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does set_rng_state set?",
        "Y": "random number generator state",
        "Z": "set_rng_state Sets the random number generator state.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What draws binary random numbers from a Bernoulli distribution?",
        "Y": "bernoulli",
        "Z": "bernoulli Draws binary random numbers (0 or 1) from a Bernoulli distribution.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does bernoulli draw from a Bernoulli distribution?",
        "Y": "binary random numbers",
        "Z": "bernoulli Draws binary random numbers (0 or 1) from a Bernoulli distribution.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located",
        "Y": "multinomial",
        "Z": "multinomial Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does multinomial return if each row containsnum_samplesindices sampled from the multinomial probability distribution?",
        "Y": "tensor",
        "Z": "multinomial Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor of random numbers drawn from separate normal distributions?",
        "Y": "normal",
        "Z": "normal Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does poisson use to return a tensor of the same size?",
        "Y": "rate parameter given by the corresponding element in input i",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What distribution is the tensor sampled from by poisson?",
        "Y": "Poisson distribution",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What parameter is given by the corresponding element in input for poisson?",
        "Y": "rate parameter",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "From what distribution is each element sampled by poisson?",
        "Y": "Poisson distribution",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does poisson return for each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.",
        "Y": "a tensor",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What kind of distribution is  referred by rand to return tensor?",
        "Y": "uniform distribution",
        "Z": "rand Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor filled with random numbers from a uniform distribution?",
        "Y": "rand",
        "Z": "rand Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "rand Returns a tensor filled with what?",
        "Y": "random numbers",
        "Z": "rand Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does rand_like return?",
        "Y": "a tensor",
        "Z": "rand_like Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Y": "randint",
        "Z": "randint Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor with the same shape as Tensorinput filled with random integers generated uniformly betweenlow(inclusive",
        "Y": "randint_like",
        "Z": "randint_like Returns a tensor with the same shape as Tensorinput filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is randn's return value?",
        "Y": "tensor",
        "Z": "randn Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does randn return?",
        "Y": "tensor",
        "Z": "randn Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Which distribution  randn_like  refers for returning  a tensor ?",
        "Y": "normal distribution",
        "Z": "randn_like Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does randn_like return?",
        "Y": "a tensor",
        "Z": "randn_like Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a random permutation of integers from 0 to -1?",
        "Y": "randperm",
        "Z": "randperm Returns a random permutation of integers from 0 to -1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is engine that generates Sobol sequences?",
        "Y": "The torch.quasirandom.SobolEngine",
        "Z": "quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of sequence is generated by the SobolEngine ?",
        "Y": "quasirandom  (scrambled) Sobol sequences.",
        "Z": "quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does save do?",
        "Y": "save Saves an object to a disk file",
        "Z": "save Saves an object to a disk file.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What saves an object to a disk file?",
        "Y": "save",
        "Z": "save Saves an object to a disk file.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "save Saves an object to what type of file?",
        "Y": "disk",
        "Z": "save Saves an object to a disk file.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "load Loads an object saved from a file  saved using what method?",
        "Y": "with torch.save()",
        "Z": "load Loads an object saved with torch.save() from a file.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does load Load  from a file?",
        "Y": "the object from a file  earlier saved with torch.save()",
        "Z": "load Loads an object saved with torch.save() from a file.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns the number of threads used for inter-op parallelism on CPU?",
        "Y": "get_num_interop_threads",
        "Z": "get_num_interop_threads Returns the number of threads used for inter-op parallelism on CPU (e.g.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does get_num_interop_threads return?",
        "Y": "the number of threads used for inter-op parallelism on CPU",
        "Z": "get_num_interop_threads Returns the number of threads used for inter-op parallelism on CPU (e.g.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What sets the number of threads used for interop parallelism?",
        "Y": "set_num_interop_threads",
        "Z": "set_num_interop_threads Sets the number of threads used for interop parallelism (e.g.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is program that disabled gradient calculation?",
        "Y": "no_grad Context-manager",
        "Z": "no_grad Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What did no_grad Context-manager disable?",
        "Y": "gradient calculation",
        "Z": "no_grad Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the enable_grad Context-manager enable?",
        "Y": "gradient calculation",
        "Z": "enable_grad Context-manager that enables gradient calculation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is Context-manager that enables gradient calculation?",
        "Y": "enable_grad",
        "Z": "enable_grad Context-manager that enables gradient calculation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is Context-manager that sets gradient calculation to on or off?",
        "Y": "set_grad_enabled",
        "Z": "set_grad_enabled Context-manager that sets gradient calculation to on or off.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is set_grad_enabled?",
        "Y": "Context-manager",
        "Z": "set_grad_enabled Context-manager that sets gradient calculation to on or off.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens if grad mode is currently enabled?",
        "Y": "is_grad_enabled Returns True",
        "Z": "is_grad_enabled Returns True if grad mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What mode is currently enabled by is_grad_enabled?",
        "Y": "grad mode",
        "Z": "is_grad_enabled Returns True if grad mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does inference_mode Context-manager enable or disable?",
        "Y": "inference mode",
        "Z": "inference_mode Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Returns True if inference mode is currently enabled?",
        "Y": "is_inference_mode_enabled",
        "Z": "is_inference_mode_enabled Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does abs compute of each element in input?",
        "Y": "absolute value",
        "Z": "abs Computes the absolute value of each element in input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What computes the absolute value of each element in input?",
        "Y": "abs",
        "Z": "abs Computes the absolute value of each element in input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the absolute ?",
        "Y": "Alias for torch.abs()",
        "Z": "absolute Alias for torch.abs()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the  Alias for torch.abs()?",
        "Y": "absolute",
        "Z": "absolute Alias for torch.abs()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does acos compute?",
        "Y": "inverse cosine",
        "Z": "acos Computes the inverse cosine of each element in input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is arccos?",
        "Y": "Alias for torch.acos",
        "Z": "arccos Alias for torch.acos().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is Alias for torch.acos?",
        "Y": "arccos",
        "Z": "arccos Alias for torch.acos().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "acosh Returns a new tensor with what of the elements of input?",
        "Y": "inverse hyperbolic cosine",
        "Z": "acosh Returns a new tensor with the inverse hyperbolic cosine of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is  arccosh?",
        "Y": "Alias for torch.acosh",
        "Z": "arccosh Alias for torch.acosh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias for torch.acosh()?",
        "Y": "arccosh",
        "Z": "arccosh Alias for torch.acosh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does add do to the scalar other to each element of the input input and return a new resulting tensor?",
        "Y": "add Adds the scalar other to each element of the input input and returns a new resulting tensor",
        "Z": "add Adds the scalar other to each element of the input input and returns a new resulting tensor.",
        "source": "https://pytorch.org/docs/ stable/torch.html#reduction-ops"
    },
    {
        "X": "What performs the element-wise division of tensor 1 by tensor 2 , multiply the result by the scalarvalueand add it to input?",
        "Y": "addcdiv",
        "Z": "addcdiv Performs the element-wise division of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How does addcdiv perform the element-wise division of of tensor 1 by tensor 2?",
        "Y": "multiply the result by the scalarvalue",
        "Z": "addcdiv Performs the element-wise division of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What performs the element-wise multiplication of tensor 1 by tensor 2?",
        "Y": "addcmul",
        "Z": "addcmul Performs the element-wise multiplication of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "to what inputs the element-wise multiplication performed by addcmul?",
        "Y": "element-wise multiplication of tensor 1 by tensor 2",
        "Z": "addcmul Performs the element-wise multiplication of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does angle compute?",
        "Y": "the element-wise angle",
        "Z": "angle Computes the element-wise angle (in radians) of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "In what units does angle compute the element-wise angle of the given input tensor?",
        "Y": "radians",
        "Z": "angle Computes the element-wise angle (in radians) of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "In what units is the element-wise angle of a given input tensor computed?",
        "Y": "radians",
        "Z": "angle Computes the element-wise angle (in radians) of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a new tensor with arcsine of each element ?",
        "Y": "arcsine",
        "Z": "asin Returns a new tensor with the arcsine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is arcsin?",
        "Y": "torch.asin",
        "Z": "arcsin Alias for torch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is Alias for torch.asin()?",
        "Y": "arcsin",
        "Z": "arcsin Alias for torch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of sine does asinh return?",
        "Y": "hyperbolic",
        "Z": "asinh Returns a new tensor with the inverse hyperbolic sine of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Asinh Returns a new tensor with what of the elements of input?",
        "Y": "inverse hyperbolic sine",
        "Z": "asinh Returns a new tensor with the inverse hyperbolic sine of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Atan returns a new tensor with what of the elements of input?",
        "Y": "arctangent",
        "Z": "atan Returns a new tensor with the arctangent  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is arctan?",
        "Y": "Alias for torch.atan",
        "Z": "arctan Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is Alias for torch.atan?",
        "Y": "arctan",
        "Z": "arctan Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Atanh Returns a new tensor with the inverse of what type of tangent of the elements of input?",
        "Y": "hyperbolic",
        "Z": "atanh Returns a new tensor with the inverse hyperbolic tangent of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Atanh Returns a new tensor with what of the elements of input?",
        "Y": "inverse hyperbolic tangent",
        "Z": "atanh Returns a new tensor with the inverse hyperbolic tangent of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is arctanh?",
        "Y": "Alias for torch.atanh",
        "Z": "arctanh Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the  Alias for torch.atanh()?",
        "Y": "arctanh",
        "Z": "arctanh Alias for torch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the element-wise arctangent ofinput i/otheri?",
        "Y": "atan2",
        "Z": "atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "With consideration of what is the arctangent ofinput i/otheri / textotheritextotheri",
        "Y": "the quadrant",
        "Z": "atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Computes the bitwise NOT of the given input tensor?",
        "Y": "bitwise_not",
        "Z": "bitwise_not Computes the bitwise NOT of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What computes the bitwise AND of input and other?",
        "Y": "bitwise_and",
        "Z": "bitwise_and Computes the bitwise AND of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is bitwise_or for?",
        "Y": "Computes the bitwise OR of input and other",
        "Z": "bitwise_or Computes the bitwise OR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What computes the bitwise XOR of input and other?",
        "Y": "bitwise_xor",
        "Z": "bitwise_xor Computes the bitwise XOR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does ceil return with the ceil of the elements of input?",
        "Y": "a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element",
        "Z": "ceil Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the ceil of the elements of input?",
        "Y": "the smallest integer greater than or equal to each element",
        "Z": "ceil Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What are all elements in input into the range?",
        "Y": "clamp Clamps inputs to a range bentween min and max",
        "Z": "clamp Clamps all elements in input into the range[min,max].",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is clip?",
        "Y": "Alias for torch.clamp()",
        "Z": "clip Alias for torch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias for torch.clamp()?",
        "Y": "clip",
        "Z": "clip Alias for torch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of clip Alias?",
        "Y": "for torch.clamp()",
        "Z": "clip Alias for torch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does conj compute?",
        "Y": "the element-wise conjugate of the given input tensor",
        "Z": "conj Computes the element-wise conjugate of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "conj Computes what?",
        "Y": "the element-wise conjugate of the given input tensor",
        "Z": "conj Computes the element-wise conjugate of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Computes the element-wise conjugate of the given input tensor?",
        "Y": "conj",
        "Z": "conj Computes the element-wise conjugate of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the magnitude of a floating-point tensor?",
        "Y": "the magnitude of input",
        "Z": "copysign Create a new floating-point tensor with the magnitude of input and the sign of other, element wise.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a new tensor with the cosine of the elements of input?",
        "Y": "cos",
        "Z": "cos Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Returns a new tensor with the cosine of the elements of input?",
        "Y": "cos",
        "Z": "cos Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "cosh Returns a new tensor with what type of cosine?",
        "Y": "hyperbolic",
        "Z": "cosh Returns a new tensor with the hyperbolic cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a new tensor with the hyperbolic cosine of the elements of input?",
        "Y": "cosh",
        "Z": "cosh Returns a new tensor with the hyperbolic cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a new tensor with each element of input converted from angles in degrees to radians?",
        "Y": "deg2rad",
        "Z": "deg2rad Returns a new tensor with each of the elements of input converted from angles in degrees to radians.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a new tensor with each of the elements of input converted from angles in degrees to radians?",
        "Y": "deg2rad",
        "Z": "deg2rad Returns a new tensor with each of the elements of input converted from angles in degrees to radians.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What divides each element of the input input by the corresponding element of the other?",
        "Y": "div",
        "Z": "div Divides each element of the input input by the corresponding element of other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What divides each element of the input input by the corresponding element of other?",
        "Y": "div",
        "Z": "div Divides each element of the input input by the corresponding element of other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is Alias for torch.div() divide?",
        "Y": "Alias for torch.div()",
        "Z": "divide Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is Alias for torch.div()?",
        "Y": "divide Alias for torch.div()",
        "Z": "divide Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "digamma Computes the derivative of the gamma function on input?",
        "Y": "logarithmic",
        "Z": "digamma Computes the logarithmic derivative of the gamma function on input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "digamma Computes the logarithmic derivative of what on input?",
        "Y": "gamma function",
        "Z": "digamma Computes the logarithmic derivative of the gamma function on input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "digamma Computes what derivative of the gamma function on input?",
        "Y": "logarithmic derivative",
        "Z": "digamma Computes the logarithmic derivative of the gamma function on input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What computes the logarithmic derivative of the gamma function on input?",
        "Y": "digamma",
        "Z": "digamma Computes the logarithmic derivative of the gamma function on input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does erf?",
        "Y": "Alias for torch.special.erf()",
        "Z": "erf Alias for torch.special.erf().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is erf?",
        "Y": "Alias for torch.special.erf",
        "Z": "erf Alias for torch.special.erf().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is erfc function?",
        "Y": "Alias for torch.special.erfc()",
        "Z": "erfc Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is erfc Alias?",
        "Y": "for torch.special.erfc()",
        "Z": "erfc Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does erfinv do?",
        "Y": "Alias for torch.special.erfinv()",
        "Z": "erfinv Alias for torch.special.erfinv().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does erfinv stand for?",
        "Y": "Alias for torch.special.erfinv",
        "Z": "erfinv Alias for torch.special.erfinv().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is erfinv Alias?",
        "Y": "for torch.special.erfinv",
        "Z": "erfinv Alias for torch.special.erfinv().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "exp Returns a new tensor with what of the elements of the input tensorinput?",
        "Y": "exponential",
        "Z": "exp Returns a new tensor with the exponential of the elements of the input tensorinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias for torch.special.exp2()?",
        "Y": "exp2",
        "Z": "exp2 Alias for torch.special.exp2().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias for torch.special.expm1?",
        "Y": "expm1",
        "Z": "expm1 Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does expm1 stand for?",
        "Y": "Alias for torch.special.expm1",
        "Z": "expm1 Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the Alias for torch.special.expm1()?",
        "Y": "expm1",
        "Z": "expm1 Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    }
]