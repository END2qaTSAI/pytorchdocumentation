{
    "What happens if the global deterministic flag is turned on?": {
        "answer": "Returns True",
        "question": "What happens if the global deterministic flag is turned on?",
        "context": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    "What documentation does the global deterministic flag turn on?": {
        "answer": "totorch.use_deterministic_algorithms()",
        "question": "What documentation does the global deterministic flag turn on?",
        "context": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    "Returns what if the global deterministic flag is turned on?": {
        "answer": "True",
        "question": "Returns what if the global deterministic flag is turned on?",
        "context": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    "What documentation does totorch.use_deterministic_algorithms() refer to for more details?": {
        "answer": "totorch.use_deterministic_algorithms()",
        "question": "What documentation does totorch.use_deterministic_algorithms() refer to for more details?",
        "context": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    "What represents if each element isfiniteor not?": {
        "answer": "Returns a new tensor with boolean elements representing if each element is finite or not",
        "question": "What does torch.isfinite do?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "What are finite when they are not NaN, negative infinity, or infinity?": {
        "answer": "Real values",
        "question": "What are finite when they are not NaN, negative infinity, or infinity?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "Complex values are what when both their real and imaginary parts are finite?": {
        "answer": "finite",
        "question": "Complex values are what when both their real and imaginary parts are finite?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "What are finite when both their real and imaginary parts are finite?": {
        "answer": "Complex values",
        "question": "What are finite when both their real and imaginary parts are finite?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "What type of distribution does fillselftensor with elements drawn from?": {
        "answer": "geometric distribution",
        "question": "What type of distribution does fillselftensor with elements drawn from with torch.Tensor.geometric_?",
        "context": "Fillsselftensor with elements drawn from the geometric distribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    "In what direction does the flipud return a new tensor?": {
        "answer": "Flip the entries in each column in the up/down direction",
        "question": "In what direction does the flipud return a new tensor?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What are preserved, but appear in a different order than before when flipud is used?": {
        "answer": "Rows",
        "question": "What are preserved, but appear in a different order than before when flipud is used?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What is requirement on tensor when using flipud?": {
        "answer": "2-D",
        "question": "What is requirement on tensor when using flipud?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What is requirement on tensor when using fliplr?": {
        "answer": "2-D",
        "question": "What is requirement on tensor when using fliplr?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "Does torch.flip make a copy of input's data?": {
        "answer": "Yes torch.flip makes a copy ofinput\u2019s data",
        "question": "Does torch.flip make a copy of input's data?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flip makes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is the difference between torch.flipud and numpy.flipud?": {
        "answer": "torch.flipud is slower than numpy.flipud",
        "question": "What is the difference between torch.flipudis and numpy.flipud?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What does flip tensor in the up/down direction return?": {
        "answer": "torch.flipud",
        "question": "What does flip tensor in the up/down direction return?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "In which direction do the entries in each column flip?": {
        "answer": "up/down",
        "question": "In which direction do the entries in each column flip?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What is required for the tensor to be at least for torch.flipud?": {
        "answer": "1-D",
        "question": "What is required for the tensor to be at least?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What returns a view in constant time?": {
        "answer": "NumPy\u2019snp.flip",
        "question": "What returns a view in constant time?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "Why is copying a tensor's data slower than viewing that data?": {
        "answer": "more work",
        "question": "Why is copying a tensor's data slower than viewing that data?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "How is the tensor's dtype inferred with torch.full?": {
        "answer": "from fill_value",
        "question": "How is the tensor's dtype inferred with torch.full?",
        "context": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What is the value to fill the output tensor with torch.full?": {
        "answer": "fill_value(Scalar)",
        "question": "What is the value to fill the output tensor with torch.full?",
        "context": "fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What is the desired data type of returned tensor with torch.sum?": {
        "answer": "returned tensor is casted to dtype, if specified",
        "question": "What is the desired data type of returned tensor?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is the default for a global default?": {
        "answer": "ifNone",
        "question": "What is the default for a global default?",
        "context": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (see torch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What  type of tensor is created with torch.full?": {
        "answer": "a tensor of sizesizefilled withfill_value",
        "question": "What  type of tensor is created with torch.full?",
        "context": "Creates a tensor of size filled with fill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What is inferred from fill_value?": {
        "answer": "datatype",
        "question": "What is inferred from fill_value?",
        "context": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What defines the shape of the output tensor with torch.ones?": {
        "answer": "a sequence of integers",
        "question": "What defines the shape of the output tensor with torch.ones?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "How is the desired layout of returned Tensor with torch.ones?": {
        "answer": "with torch.layout",
        "question": "How is the desired layout of returned Tensor with torch.ones?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the name of the layout of returned Tensor with torch.ones?": {
        "answer": "Default:torch.strided",
        "question": "What is the name of the layout of returned Tensor with torch.ones?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is out(Tensor,optional)?": {
        "answer": "output tensor",
        "question": "What is out(Tensor,optional)?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the desired data type of returned Tensor with torch.zeros_like?": {
        "answer": "dtype, if specified",
        "question": "What is the desired data type of returned Tensor with torch.zeros_like?",
        "context": "Returns a tensor filled with the scalar value0, with the same size as input.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the default layout of returned Tensor with torch.zeros_like?": {
        "answer": "Default:torch.strided",
        "question": "What is the default layout of returned Tensor with torch.zeros_like?",
        "context": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "How are numbers sampled from the continuous uniform distribution?": {
        "answer": "using torch.Tensor.uniform_",
        "question": "How are numbers sampled from the continuous uniform distribution?",
        "context": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    "What distribution does fillselftensor with numbers sampled from?": {
        "answer": "uniform",
        "question": "What distribution does fillselftensor with numbers sampled from?",
        "context": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    "What is filled with numbers sampled from the continuous uniform distribution?": {
        "answer": "self.tensor",
        "question": "What is filled with numbers sampled from the continuous uniform distribution?",
        "context": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    "What is the current state of the generator?": {
        "answer": "a torch.ByteTensor",
        "question": "What is the current state of the generator?",
        "context": "Gets the current device of the generator. Example: Returns the Generator state as a torch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the current device of the generator state?": {
        "answer": "atorch.ByteTensor",
        "question": "What is the current device of the generator state?",
        "context": "Gets the current device of the generator. Example: Returns the Generator state as a torch.ByteTensor. A torch.ByteTensor which contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What contains all the necessary bits to restore a Generator to a specific point in time?": {
        "answer": "Tensor",
        "question": "What contains all the necessary bits to restore a Generator to a specific point in time?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the first seed for generating random numbers?": {
        "answer": "initial seed",
        "question": "What is the name of the first seed for generating random numbers?",
        "context": "Example: Returns the Generator state as a torch.ByteTensor. A torch.ByteTensor which contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of a seed that has a good balance of 0 and 1 bits in the seed?": {
        "answer": "a torch.Generator object",
        "question": "What is the name of a seed that has a good balance of 0 and 1 bits in the seed?",
        "context": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is atorch.Generatorobject recommended to set?": {
        "answer": "a large seed",
        "question": "What is atorch.Generatorobject recommended to set?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a good balance of a large seed?": {
        "answer": "0 and 1 bits",
        "question": "What is a good balance of a large seed?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does atorch.ByteTensor get?": {
        "answer": "current device",
        "question": "What does atorch.ByteTensor get?",
        "context": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does atorch.Generatorobject do?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What does atorch.Generatorobject do?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What returns the seed for generating random numbers?": {
        "answer": "a torch.Generator object",
        "question": "What returns the seed for generating random numbers?",
        "context": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits?": {
        "answer": "Avoid having many 0 bits in the seed",
        "question": "What is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the pupose of initial seed for generating random numbers?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What is the pupose of initial seed for generating random numbers?",
        "context": "Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the Generator state returned as?": {
        "answer": "a torch.ByteTensor",
        "question": "What is the Generator state returned as?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What  Sets the seed for generating random numbers return?": {
        "answer": "a torch.Generator object",
        "question": "What Sets the seed for generating random numbers return?",
        "context": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What should you do when setting a large seed?": {
        "answer": "Avoid having many 0 bits in the seed",
        "question": "What should you do when setting a large seed?",
        "context": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is an example of a seed that contains all the necessary bits to restore a Generator to a specific point in time?": {
        "answer": "Tensor",
        "question": "What is an example of a seed that contains all the necessary bits to restore a Generator to a specific point in time?",
        "context": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What returns the initial seed for generating random numbers?": {
        "answer": "a torch.Generator object",
        "question": "What returns the initial seed for generating random numbers?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does A torch.ByteTensor do?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What does Atorch.ByteTensor do?",
        "context": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is recommended to set a number that has a good balance of 0 and 1 bits?": {
        "answer": "large seed",
        "question": "What is recommended to set a number that has a good balance of 0 and 1 bits?",
        "context": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What  returns the initial seed for generating random numbers?": {
        "answer": "a torch.Generator object",
        "question": "What  returns the initial seed for generating random numbers?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the initial seed for generating random numbers?": {
        "answer": "Pythonlong",
        "question": "What is the initial seed for generating random numbers?",
        "context": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a torch.Generatorobject return for generating random numbers?": {
        "answer": "Sets the seed",
        "question": "What does atorch.Generatorobject return for generating random numbers?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What should you avoid in the seed?": {
        "answer": "Avoid having many 0 bits",
        "question": "What should you avoid in the seed?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does atorch.Generatorobject mean?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What does atorch.Generatorobject mean?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does torch.Generatorobject return for generating random numbers?": {
        "answer": "Returns the initial seed",
        "question": "What does it return for generating random numbers?",
        "context": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the desired seed?": {
        "answer": "seed(int)",
        "question": "What is the desired seed?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "Value must be within what range?": {
        "answer": "inclusive range",
        "question": "Value must be within what range?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is raised if the value is not within the inclusive range?": {
        "answer": "RuntimeError",
        "question": "What is raised if the value is not within the inclusive range?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What inputs are remapped to positive values with the formula0xfff_fff_fff_ffff": {
        "answer": "Negative inputs",
        "question": "What inputs are remapped to positive values with the formula0xfff_fff_fff_ffff",
        "context": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the object that sets the seed for generating random numbers?": {
        "answer": "torch.Generator",
        "question": "What is the object that sets the seed for generating random numbers?",
        "context": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is an example of a torch.Generator object?": {
        "answer": "Generator",
        "question": "What is an example of a torch.Generator object?",
        "context": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is returned if inference mode is enabled?": {
        "answer": "True",
        "question": "What is returned if inference mode is enabled?",
        "context": "Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled"
    },
    "Which correction will be used to calculate the variance?": {
        "answer": "Bessel correction",
        "question": "Which correction will be used to calculate the variance?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is calculated if Bessel's correction is true?": {
        "answer": "the sample variance",
        "question": "What is calculated if Bessel's correction is true?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is reduced with torch.var_mean?": {
        "answer": "dimension",
        "question": "What is reduced with torch.var_mean?",
        "context": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "When is the correction is used to calculate the variance?": {
        "answer": "when unbiased is true",
        "question": "What type of correction is used to calculate the variance?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "How to specify that output tensor should retain the dimension or not?": {
        "answer": "by specifying keepdim(bool)",
        "question": "How to specify that output tensor should retain the dimension or not?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is the name of correction that will be used to calculate the variance?": {
        "answer": "Bessel's correction",
        "question": "What is the name of correction that will be used to calculate the variance?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "If unbiasedisTrue, what is calculated, without any correction?": {
        "answer": "the sample deviation",
        "question": "If unbiasedisTrue, what is calculated, without any correction?",
        "context": "out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). A tuple (var, mean) containing the variance and mean. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is unbiased(bool)?": {
        "answer": "whether to use Bessel\u2019s correction",
        "question": "What is unbiased(bool)?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the name of the function that determines whether the output tensor has dim retained or not?": {
        "answer": "keepdim",
        "question": "What is the name of the function that determines whether the output tensor has dim retained or not?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What does a tuple contain when using torch.var_mean?": {
        "answer": "variance and mean",
        "question": "What does the tuple contain when using torch.var_mean?",
        "context": "out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). A tuple (var, mean) containing the variance and mean. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What calculates the variance and mean of all elements in theinputtensor?": {
        "answer": "torch.var_mean Calculates the variance and mean of all elements in theinputtensor",
        "question": "What calculates the variance and mean of all elements in theinputtensor?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What does unbiased(bool) mean?": {
        "answer": "whether to use Bessel\u2019s correction",
        "question": "What does unbiased(bool) mean?",
        "context": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is calculated by a tuple containing the variance and mean of all elements in the inputtensor?": {
        "answer": "the variance and mean of all elements in theinputtensor with torch.var_mean",
        "question": "What is calculated by a tuple containing the variance and mean of all elements in the inputtensor with torch.var_mean?",
        "context": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is a tuple that calculates the variance and mean of all elements in?": {
        "answer": "the input tensor",
        "question": "What is a tuple that calculates the variance and mean of all elements in?",
        "context": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "When is Bessel's correction applied?": {
        "answer": "If unbiased is True",
        "question": "When is Bessel's correction applied",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "IfunbiasedisTrue, Bessel\u2019s correction will be used?": {
        "answer": "the sample deviation",
        "question": "IfunbiasedisTrue, Bessel\u2019s correction will be used?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "If funbiasedisTrue, what will be used?": {
        "answer": "Bessel's correction is used",
        "question": "If funbiasedisTrue, what will be used?",
        "context": "out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). A tuple (var, mean) containing the variance and mean. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What happens if Bessel's correction is not used?": {
        "answer": "the sample deviation is calculated, without any correction",
        "question": "What happens if Bessel's correction is not used?",
        "context": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What  is used to determine whether to use Bessel's correction?": {
        "answer": "unbiased as True",
        "question": "What is used to determine whether to use Bessel's correction?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is a contiguous, one-dimensional array of a single data type?": {
        "answer": "A torch.Storage is",
        "question": "What is a contiguous, one-dimensional array of a single data type?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Who has a corresponding storage of the same data type?": {
        "answer": "Every torch.Tensor",
        "question": "Who has a corresponding storage of the same data type?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type does Everytorch.Tensor have a corresponding storage of the same data type?": {
        "answer": "bfloat16",
        "question": "What type of type does Everytorch.Tensor have a corresponding storage of the same data type?",
        "context": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has a corresponding storage of the same data type?": {
        "answer": "Everytorch.Tensor",
        "question": "What has a corresponding storage of the same data type?",
        "context": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Everytorch.Tensor cast?": {
        "answer": "bfloat16",
        "question": "What type of storage does Everytorch.Tensor cast?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Everytorch.Tensor return a copy of?": {
        "answer": "char",
        "question": "What type of storage does Everytorch.Tensor return a copy of?",
        "context": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in CUDA memory and on the correct device, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If the object is already in CUDA memory and on the correct device, what is performed and the original object is returned?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens if the object is already in CUDA memory and on the correct device?": {
        "answer": "no copy is performed and the original object is returned",
        "question": "What happens if the object is already in CUDA memory and on the correct device?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type returns a copy of this storage?": {
        "answer": "char",
        "question": "What type of type returns a copy of this storage?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is device(int) called?": {
        "answer": "destination GPU id",
        "question": "What is device(int) called?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Casts this storage to byte type Return?": {
        "answer": "a copy",
        "question": "What type of storage does Casts this storage to byte type Return?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in CUDA memory and on the correct device, what happens?": {
        "answer": "no copy is performed",
        "question": "If the object is already in CUDA memory and on the correct device, what happens?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the destination GPU id?": {
        "answer": "current device",
        "question": "What is the destination GPU id?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does Casts this storage return?": {
        "answer": "a copy",
        "question": "What does Casts this storage return?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does a copy of this storage return if it\u2019s not already on the CPU?": {
        "answer": "a CPU copy",
        "question": "What does a copy of this storage return if it\u2019s not already on the CPU?",
        "context": "Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of what type of storage?": {
        "answer": "float type",
        "question": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of what type of storage?",
        "context": "Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type returns a CPU copy of this storage if it\u2019s not already on the CPU?": {
        "answer": "float",
        "question": "What type of type returns a CPU copy of this storage if it\u2019s not already on the CPU?",
        "context": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the device(int)?": {
        "answer": "destination GPU id",
        "question": "What is the device(int)?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Casts this storage to complex what type?": {
        "answer": "float",
        "question": "Casts this storage to complex what type?",
        "context": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the float type return if it\u2019s not already on the CPU?": {
        "answer": "a CPU copy",
        "question": "What does the float type return if it\u2019s not already on the CPU?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Where is a copy of this object stored?": {
        "answer": "CUDA memory",
        "question": "Where is a copy of this object stored?",
        "context": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in CUDA memory and on the correct device, what is performed?": {
        "answer": "no copy",
        "question": "If the object is already in CUDA memory and on the correct device, what is performed?",
        "context": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "The destination GPU id Defaults to what?": {
        "answer": "current device",
        "question": "The destination GPU id Defaults to what?",
        "context": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens to a CPU copy of this storage?": {
        "answer": "if it\u2019s not already on the CPU",
        "question": "What happens to a CPU copy of this storage?",
        "context": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If True and the source is in pinned memory, the copy will be what?": {
        "answer": "asynchronous",
        "question": "IfTrueand the source is in pinned memory, the copy will be what?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has no effect on the copy of a CUDA memory?": {
        "answer": "argument",
        "question": "What has no effect on the copy of a CUDA memory?",
        "context": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What if true and the source is in pinned memory, the copy will be asynchronous with respect to the host?": {
        "answer": "non_blocking(bool)",
        "question": "What if true and the source is in pinned memory, the copy will be asynchronous with respect to the host?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, what happens?": {
        "answer": "the argument has no effect",
        "question": "If the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, what happens?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Where does a copy of the object return?": {
        "answer": "CUDA memory",
        "question": "Where does a copy of the object return?",
        "context": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host?": {
        "answer": "the argument has no effect",
        "question": "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "**kwargs\u2013 For compatibility, may contain what?": {
        "answer": "key a sync in place of the non_blocking argument",
        "question": "**kwargs\u2013 For compatibility, may contain what?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Where does the object return a copy of?": {
        "answer": "CUDA memory",
        "question": "Where does the object return a copy of?",
        "context": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does device(int) default to?": {
        "answer": "current device",
        "question": "What does device(int) default to?",
        "context": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "For compatibility, may contain what place of thenon_blockingargument?": {
        "answer": "keyasyncin",
        "question": "For compatibility, may contain what place of thenon_blockingargument?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in what memory, then no copy is performed and the original object is returned?": {
        "answer": "CUDA memory",
        "question": "If the object is already in what memory, then no copy is performed and the original object is returned?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the keyasyncin place of the keyasyncin place of thenon_blockingargument?": {
        "answer": "double type",
        "question": "What is the keyasyncin place of the keyasyncin place of thenon_blockingargument?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in what?": {
        "answer": "CUDA memory",
        "question": "If the object is already in what?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does the keyasyncin cast?": {
        "answer": "double type",
        "question": "What type of storage does the keyasyncin cast?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the keyasyncin place of thenon_blockingargument?": {
        "answer": "keyasyncin place of thenon_blockingargument",
        "question": "What is the keyasyncin place of thenon_blockingargument?",
        "context": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does bool stand for?": {
        "answer": "non_blocking",
        "question": "What does bool stand for?",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does double type cast?": {
        "answer": "float type",
        "question": "What type of storage does double type cast?",
        "context": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What effect does the argument have ifTrueand the source is in pinned memory?": {
        "answer": "no effect",
        "question": "What effect does the argument have ifTrueand the source is in pinned memory?",
        "context": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the float type of the storage?": {
        "answer": "IfsharedisTrue",
        "question": "What is the float type of the storage?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "How are changes written to the file?": {
        "answer": "All changes are written to the file",
        "question": "How are changes written to the file?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the changes on the storage do not affect the file?": {
        "answer": "IfsharedisFalse",
        "question": "What does the changes on the storage do not affect the file?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of argument has no effect if the source is in pinned memory?": {
        "answer": "non_blocking(bool)",
        "question": "What type of argument has no effect if the source is in pinned memory?",
        "context": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is cast to double type?": {
        "answer": "float type",
        "question": "What type of storage is cast to double type?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens when memory is shared between all processes?": {
        "answer": "All changes are written to the file",
        "question": "What happens when memory is shared between all processes?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the changes on the storage do not affect the file, what is the default?": {
        "answer": "IfsharedisFalse",
        "question": "If the changes on the storage do not affect the file, what is the default?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does **kwargs contain for compatibility?": {
        "answer": "keyasyncin place of thenon_blockingargument",
        "question": "What does **kwargs contain for compatibility?",
        "context": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the storage that can be used to float?": {
        "answer": "IfsharedisTrue",
        "question": "What is the name of the storage that can be used to float?",
        "context": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is casts this storage to?": {
        "answer": "float type",
        "question": "What type of storage is casts this storage to?",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens when a storage is cast to float type IfsharedisTrue?": {
        "answer": "memory is shared between all processes",
        "question": "What happens when a storage is cast to float type IfsharedisTrue?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the number of elements in the storage?": {
        "answer": "sizeis the number of elements in the storage",
        "question": "What is the number of elements in the storage?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfsharedisFalse, the file must contain what?": {
        "answer": "at leastsize * sizeof(Type)bytes",
        "question": "IfsharedisFalse, the file must contain what?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What file will be created if needed?": {
        "answer": "IfsharedisTruethe file will be created if needed",
        "question": "What file will be created if needed?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is casts?": {
        "answer": "float type",
        "question": "What type of storage is casts?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the changes on the storage do not affect the file, what is the name of the type of storage?": {
        "answer": "IfsharedisFalse",
        "question": "If the changes on the storage do not affect the file, what is the name of the type of storage?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does filename(str) - file name to map?": {
        "answer": "filename(str) \u2013 file name to map",
        "question": "What does filename(str) - file name to map?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the file that is shared between all processes?": {
        "answer": "All changes are written to the file",
        "question": "What is the name of the file that is shared between all processes?",
        "context": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the file name to map shared(bool)?": {
        "answer": "share memory size(int)",
        "question": "What is the file name to map shared(bool)?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens when a float type is shared between all processes?": {
        "answer": "All changes are written to the file",
        "question": "What happens when a float type is shared between all processes?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory?": {
        "answer": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "question": "What does filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is shared between all processes?": {
        "answer": "memory",
        "question": "What is shared between all processes?",
        "context": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does filename(str) \u2013 file name to map shared(bool)?": {
        "answer": "filename(str) \u2013 file name to map shared(bool)",
        "question": "What does filename(str) \u2013 file name to map shared(bool)?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Copies the storage to what if it's not already pinned?": {
        "answer": "pinned memory",
        "question": "Copies the storage to what if it's not already pinned?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the filename of the storage that is not already pinned?": {
        "answer": "Copies the storage to pinned memory",
        "question": "What is the filename of the storage that is not already pinned?",
        "context": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is a no-op for storages already in shared memory and CUDA storages?": {
        "answer": "Storages in shared memory cannot be resized",
        "question": "What is a no-op for storages already in shared memory and CUDA storages?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is a no-op for storages already in shared memory?": {
        "answer": "CUDA storages",
        "question": "What is a no-op for storages already in shared memory?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the return of storages in shared memory?": {
        "answer": "self Casts this storage to short type",
        "question": "What is the return of storages in shared memory?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is a no-op for storages already in shared memory and for CUDA storages?": {
        "answer": "Storages in shared memory cannot be resized",
        "question": "What is a no-op for storages already in shared memory and for CUDA storages?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What storage does not need to be moved for sharing across processes?": {
        "answer": "CUDA",
        "question": "What storage does not need to be moved for sharing across processes?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of return does CUDA return?": {
        "answer": "self",
        "question": "What type of return does CUDA return?",
        "context": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does self Casts this storage to long type?": {
        "answer": "Copies the storage to pinned memory",
        "question": "What type of storage does self Casts this storage to long type?",
        "context": "shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does this no-op do for storages already in shared memory and for CUDA storages?": {
        "answer": "Moves the storage to shared memory",
        "question": "What does this no-op do for storages already in shared memory and for CUDA storages?",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what storage?": {
        "answer": "CUDA",
        "question": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what storage?",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the size of the storage Casts this storage to int type Casts this storage to long type?": {
        "answer": "Copies the storage to pinned memory",
        "question": "What is the size of the storage Casts this storage to int type Casts this storage to long type?",
        "context": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the number of elements in a storage?": {
        "answer": "size(int)",
        "question": "What is the name of the number of elements in a storage?",
        "context": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what type of storage?": {
        "answer": "CUDA",
        "question": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what type of storage?",
        "context": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What cannot be resized?": {
        "answer": "Storages in shared memory",
        "question": "What cannot be resized?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does self Cast this storage to short type Returns a list containing the elements of this storage?": {
        "answer": "if dtype is not provided",
        "question": "What does self Cast this storage to short type Returns a list containing the elements of this storage?",
        "context": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, otherwise casts this object to the": {
        "answer": "self Casts this storage to short type",
        "question": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, otherwise casts this object to the",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If this is already of the correct type, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If this is already of the correct type, what is performed and the original object is returned?",
        "context": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If this is already of the correct type, no copy is performed and what is returned?": {
        "answer": "the original object is returned",
        "question": "If this is already of the correct type, no copy is performed and what is returned?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already of the correct type, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If the object is already of the correct type, what is performed and the original object is returned?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the desired type non_blocking(bool)?": {
        "answer": "dtype(typeorstring)",
        "question": "What is the desired type non_blocking(bool)?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has no effect if the copy is performed asynchronously with respect to the host?": {
        "answer": "argument has no effect",
        "question": "What has no effect if the copy is performed asynchronously with respect to the host?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the type non_blocking(bool)?": {
        "answer": "dtype(typeorstring)",
        "question": "What is the name of the type non_blocking(bool)?",
        "context": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens if the copy is performed asynchronously with respect to the host?": {
        "answer": "the argument has no effect",
        "question": "What happens if the copy is performed asynchronously with respect to the host?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the type that returns the object to the specified type?": {
        "answer": "ifdtypeis not provided",
        "question": "What is the type that returns the object to the specified type?",
        "context": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What effect does the argument have if the source is in pinned memory and destination is on the GPU or vice versa?": {
        "answer": "no effect",
        "question": "What effect does the argument have if the source is in pinned memory and destination is on the GPU or vice versa?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If true how copy is performed what with respect to the host?": {
        "answer": "asynchronously",
        "question": "If true how copy is performed what with respect to the host?",
        "context": "dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is performed if the original object is returned?": {
        "answer": "no copy",
        "question": "What is performed if the original object is returned?",
        "context": "If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the copy perform asynchronously with respect to the host?": {
        "answer": "argument has no effect",
        "question": "What does the copy perform asynchronously with respect to the host?",
        "context": "If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is the async arg deprecated?": {
        "answer": "bfloat16",
        "question": "What type of storage is the async arg deprecated?",
        "context": "Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If no copy is performed and the original object is returned, what happens?": {
        "answer": "If this is already of the correct type",
        "question": "If no copy is performed and the original object is returned, what happens?",
        "context": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Theasyncarg cast this storage to?": {
        "answer": "bfloat16 type",
        "question": "What type of storage does Theasyncarg cast this storage to?",
        "context": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type Casts this storage to bfloat16 type?": {
        "answer": "byte",
        "question": "What type of type Casts this storage to bfloat16 type?",
        "context": "dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfTrue and the source are in pinned memory and destination is on the GPU or vice versa, how is copy  performed?": {
        "answer": "asynchronously",
        "question": "IfTrue and the source are in pinned memory and destination is on the GPU or vice versa, how is copy performed?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What effect does the argument have ifTrue and the source is in pinned memory and destination is on the GPU or vice versa?": {
        "answer": "no effect",
        "question": "What effect does the argument have ifTrue and the source is in pinned memory and destination is on the GPU or vice versa?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type does this storage have?": {
        "answer": "char",
        "question": "What type of type does this storage have?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the function that performs asynchronously with respect to the host?": {
        "answer": "non_blocking(bool)",
        "question": "What is the name of the function that performs asynchronously with respect to the host?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does the keyasyncin cast this storage to?": {
        "answer": "bfloat16 type",
        "question": "What type of storage does the keyasyncin cast this storage to?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type does this storage go to?": {
        "answer": "bfloat16",
        "question": "What type of type does this storage go to?",
        "context": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does the keyasyncarg cast this storage to?": {
        "answer": "bfloat16",
        "question": "What type of storage does the keyasyncarg cast this storage to?",
        "context": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What part of a complex tensor is equal to real?": {
        "answer": "real",
        "question": "What part of a complex tensor is equal to real?",
        "context": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    "What is the real part of the complex tensor?": {
        "answer": "real(Tensor)",
        "question": "What is the real part of the complex tensor?",
        "context": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    "Real(Tensor) \u2013 The real part of the complex tensor. Must be what?": {
        "answer": "float or double",
        "question": "Real(Tensor) \u2013 The real part of the complex tensor. Must be what?",
        "context": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    "What is the imaginary part of the complex tensor?": {
        "answer": "imag",
        "question": "What is the imaginary part of the complex tensor?",
        "context": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    "What must the imag(Tensor) be?": {
        "answer": "same dtype asreal",
        "question": "What must the imag(Tensor) be?",
        "context": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    "If the inputs are torch.float64, ouput tensor must be what?": {
        "answer": "be torch.complex128",
        "question": "If the inputs are torch.float64, ouput tensor must be what?",
        "context": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    "What does solve AX = b assume A to be?": {
        "answer": "upper-triangular",
        "question": "What does solve AX = b assume A to be?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What can torch.triangular_solve take in?": {
        "answer": "2D inputs A and b",
        "question": "What can torch.triangular_solve take in?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "If the inputs are batches, what does X support input of float, double, cfloat and cdouble data types?": {
        "answer": "batched outputs",
        "question": "If the inputs are batches, what does X support input of float, double, cfloat and cdouble data types?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does solve AX = b  solve?": {
        "answer": "a system of equations with a triangular coefficient matrix",
        "question": "What does solve AX = b  solve?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does solve solve AX = b  assume A is?": {
        "answer": "upper-triangular",
        "question": "What does solve solve AX = b  assume A is?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What can torch.triangular_solve(b, A) take in?": {
        "answer": "2D inputs",
        "question": "What can torch.triangular_solve(b, A) take in?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does torch.triangular_solve(b, A) return if the inputs are batches?": {
        "answer": "batched outputs",
        "question": "What does torch.triangular_solve(b, A) return if the inputs are batches?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does torch.triangular_solve take in?": {
        "answer": "2D inputsb",
        "question": "What does torch.triangular_solve take in?",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does the torch.triangular_solve return if the inputs are batches?": {
        "answer": "batched outputs",
        "question": "What does the torch.triangular_solve return if the inputs are batches?",
        "context": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the name for multiple right-hand sides of size(,m,k)(*, m, k)(,": {
        "answer": "b(Tensor)",
        "question": "What is the name for multiple right-hand sides of size(,m,k)(*, m, k)(,",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What supports input of float, double, cfloat and cdouble data types?": {
        "answer": "batched outputs X",
        "question": "What supports input of float, double, cfloat and cdouble data types?",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does torch.triangular_solve return if the inputs are batches?": {
        "answer": "batched outputs",
        "question": "What does torch.triangular_solve return if the inputs are batches?",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the name of the input triangular coefficient matrix of size(,m,k)(*, m, k)(": {
        "answer": "b(Tensor)",
        "question": "What is the name of the input triangular coefficient matrix of size(,m,k)(*, m, k)(",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the name of the multiple right-hand sides of size(,m,k)(*, m, k)(": {
        "answer": "b(Tensor)",
        "question": "What is the name of the multiple right-hand sides of size(,m,k)(*, m, k)(",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default for the lower-triangular system of equations?": {
        "answer": "True",
        "question": "What is the default for the lower-triangular system of equations?",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "Supports input of float, double, cfloat and what other data type?": {
        "answer": "cdouble",
        "question": "Supports input of float, double, cfloat and what other data type?",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default value for the upper-triangular system of equations?": {
        "answer": "True",
        "question": "What is the default value for the upper-triangular system of equations?",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the multiple right-hand sides of size(,m,k)(*, m, k)(,m,": {
        "answer": "b(Tensor)",
        "question": "What is the multiple right-hand sides of size(,m,k)(*, m, k)(,m,",
        "context": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default for transpose?": {
        "answer": "True",
        "question": "What is the default for transpose?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default for a b(Tensor)?": {
        "answer": "False",
        "question": "What is the default for a b(Tensor)?",
        "context": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the input triangular coefficient matrix of size(,m,m)(*, m, k)(,m": {
        "answer": "b(Tensor)",
        "question": "What is the input triangular coefficient matrix of size(,m,m)(*, m, k)(,m",
        "context": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is transpose(bool,optional)?": {
        "answer": "whether A should be transposed before being sent into the solver",
        "question": "What is transpose(bool,optional)?",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default value of transpose(bool,optional)?": {
        "answer": "False",
        "question": "What is the default value of transpose(bool,optional)?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the input triangular coefficient matrix of size?": {
        "answer": "A(Tensor)",
        "question": "What is the input triangular coefficient matrix of size?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default matrix system of equations?": {
        "answer": "upper triangular",
        "question": "What is the default matrix system of equations?",
        "context": "upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default value for transpose?": {
        "answer": "Default:False",
        "question": "What is the default value for transpose?",
        "context": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default value of unitriangular(bool,optional)?": {
        "answer": "Default:False",
        "question": "What is the default value of unitriangular(bool,optional)?",
        "context": "upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "If True, the diagonal elements of A are assumed to be 1 and not referenced from A.": {
        "answer": "Default:False",
        "question": "If True, the diagonal elements of A are assumed to be 1 and not referenced from A..",
        "context": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is a solution, cloned_coefficient?": {
        "answer": "namedtuple",
        "question": "What is a solution, cloned_coefficient?",
        "context": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is a tensor filled with?": {
        "answer": "uninitialized data",
        "question": "What is a tensor filled with?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a sequence of integers defining the shape of the output tensor?": {
        "answer": "size(int...)",
        "question": "What is a sequence of integers defining the shape of the output tensor?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What can be a variable number of arguments?": {
        "answer": "a collection like a list or tuple",
        "question": "What can be a variable number of arguments?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is out a variable number of arguments or a collection like a list or tuple?": {
        "answer": "output tensor",
        "question": "What is out a variable number of arguments or a collection like a list or tuple?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What module is modeled after SciPy's specialmodule?": {
        "answer": "The torch.special module",
        "question": "What module is modeled after SciPy'sspecialmodule?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the torch.special module modeled after?": {
        "answer": "SciPy",
        "question": "What is the torch.special module modeled after?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What does the torch.special module compute?": {
        "answer": "error function ofinput",
        "question": "What does the torch.special module compute?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the error function defined as?": {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the error function defined as?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is still being added to the torch.special module?": {
        "answer": "New functions",
        "question": "What is still being added to the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the error function of input defined as?": {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the error function of input defined as?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is out(Tensor,optional) defined as?": {
        "answer": "output tensor",
        "question": "What is out(Tensor,optional) defined as?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is an example of a PyTorch module?": {
        "answer": "Example:",
        "question": "What is an example of a PyTorch module?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the function that computes the error function of input?": {
        "answer": "torch.special.expit Computes the error function ofinput",
        "question": "What is the function that computes the error function of input?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the error function of input do?": {
        "answer": "Computes the complementary error function ofinput",
        "question": "What is the error function of input do?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the complementary error function of input do?": {
        "answer": "Computes the complementary error function ofinput",
        "question": "What is the complementary error function of input do?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the error function defined as: input(Tensor) \u2013 the input tensor. out(Tensor,": {
        "answer": "Computes the error function ofinput",
        "question": "What is the error function defined as: input(Tensor) \u2013 the input tensor. out(Tensor,",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the complementary error function defined as: input(Tensor) \u2013 the input tensor?": {
        "answer": "output tensor",
        "question": "What is the complementary error function defined as: input(Tensor) \u2013 the input tensor?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the complementary error function defined as?": {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the complementary error function defined as?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What error function is defined in the range(1,1)(-1,1)(1,1)?": {
        "answer": "inverse error function",
        "question": "What error function is defined in the range(1,1)(-1,1)(1,1)?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is defined in the range(1,1)(-1, 1)(1,1)?": {
        "answer": "inverse error function",
        "question": "What is defined in the range(1,1)(-1, 1)(1,1)?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the logistic sigmoid function?": {
        "answer": "the expit",
        "question": "What is the logistic sigmoid function?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What error function is defined in the range(1,1)(-1, 1)(1,1)?": {
        "answer": "inverse error function",
        "question": "What error function is defined in the range(1,1)(-1, 1)(1,1)?",
        "context": "Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the expit also known as?": {
        "answer": "the logistic sigmoid function",
        "question": "What is the expit also known as?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "Computes the expit also known as what?": {
        "answer": "the logistic sigmoid function",
        "question": "Computes the expit also known as what?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the natural value of the gamma function oninput?": {
        "answer": "logarithm",
        "question": "What is the natural value of the gamma function oninput?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the first kind of the first kind ofinput?": {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is the first kind of the first kind ofinput?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "Computes the exponentially scaled zeroth order modified Bessel function of what kind?": {
        "answer": "first kind",
        "question": "Computes the exponentially scaled zeroth order modified Bessel function of what kind?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the base two exponential function ofinput?": {
        "answer": "Computes the base two exponential function ofinput",
        "question": "What is the base two exponential function ofinput?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the absolute value of the gamma function oninput?": {
        "answer": "natural logarithm",
        "question": "What is the absolute value of the gamma function oninput?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the order modified Bessel function of the first kind?": {
        "answer": "zeroth",
        "question": "What is the order modified Bessel function of the first kind?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the exponentially scaled zeroth order modified Bessel function of?": {
        "answer": "first kind",
        "question": "What is the exponentially scaled zeroth order modified Bessel function of?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the first kind of the input tensor?": {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is the first kind of the input tensor?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is computed for each element of input with torch.special.expit?": {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is computed for each element of input with torch.special.expit?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the first kind of gamma function oninput?": {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is the first kind of gamma function oninput?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the result of the natural logarithm of the absolute value of the gamma function oninput?": {
        "answer": "Computes the natural logarithm of the absolute value of the gamma function oninput",
        "question": "What is the result of the natural logarithm of the absolute value of the gamma function oninput?",
        "context": "Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the logit of the elements ofinput.input is clamped to when eps is not None?": {
        "answer": "eps",
        "question": "What is the logit of the elements ofinput.inputis clamped to when eps is not None?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "When eps is None and input 0 or input> 1, the function will yield what?": {
        "answer": "NaN",
        "question": "When eps is None andinput 0 orinput> 1, the function will yield what?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is the epsilon for input clamp bound?": {
        "answer": "float",
        "question": "What is the epsilon for input clamp bound?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is another example of the output tensor?": {
        "answer": "Computesinput*log1p(other)with the following cases",
        "question": "What is another example of the output tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is sscipy.special.xlog1py similar to?": {
        "answer": "scipy.special of SciPy ",
        "question": "What is sscipy.special.xlog1py similar to?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    "What is a tensor whose shape isbroadcastablewith the first argument?": {
        "answer": "Computesinput",
        "question": "What is a tensor whose shape isbroadcastablewith the first argument?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What is a boolean tensor that is True whereinputis greater thanotherand False elsewhere?": {
        "answer": "output tensor",
        "question": "What is a boolean tensor that is True whereinputis greater thanotherand False elsewhere?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What does input return with each of the elements of inputrounded to the closest integer?": {
        "answer": "Returns a new tensor",
        "question": "What does input return with each of the elements of inputrounded to the closest integer?",
        "context": "Returns a new tensor with each of the elements ofinputrounded\nto the closest integer. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.round.html#torch.round"
    },
    "What does PCA perform on a low-rank matrix?": {
        "answer": "linear Principal Component Analysis",
        "question": "What does PCA perform on a low-rank matrix?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What does U,S,V return?": {
        "answer": "namedtuple",
        "question": "What does U,S,V return?",
        "context": "This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represents the principal directions of PCA?": {
        "answer": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns",
        "question": "What represents the principal directions of PCA?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What does PCA stand for?": {
        "answer": "Principal Component Analysis",
        "question": "What does PCA stand for?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the nearly optimal approximation of a singular value decomposition of a centered matrix?": {
        "answer": "namedtuple(U,S,V)",
        "question": "What is the nearly optimal approximation of a singular value decomposition of a centered matrix?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What do theVVVcolumns represent?": {
        "answer": "the principal directions",
        "question": "What do theVVVcolumns represent?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the relation of the data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S": {
        "answer": "AAAis",
        "question": "What is the relation of the data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S",
        "context": "This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the nearly optimal approximation of a singular value decomposition of a centered matrixAAA?": {
        "answer": "namedtuple(U,S,V)",
        "question": "What is the nearly optimal approximation of a singular value decomposition of a centered matrixAAA?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represents the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont": {
        "answer": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns",
        "question": "What represents the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont",
        "context": "The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the size of returned matrices?": {
        "answer": "UUUis m x q matrix",
        "question": "What is the size of returned matrices?",
        "context": "The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is a data matrix withmsamples andnfeatures theVVVcolumns?": {
        "answer": "AAAis",
        "question": "What is a data matrix withmsamples andnfeatures theVVVcolumns?",
        "context": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "matmul(A,V[:,:k])projects data to what?": {
        "answer": "first k principal components",
        "question": "matmul(A,V[:,:k])projects data to what?",
        "context": "Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represent the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont": {
        "answer": "theVVVcolumns",
        "question": "What represent the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont",
        "context": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represent the principal directions?": {
        "answer": "theVVVcolumns",
        "question": "What represent the principal directions?",
        "context": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What does S2/(m1)S ** 2 / (m - 1)S2/(m1)contains?": {
        "answer": "eigenvalues",
        "question": "What does S2/(m1)S ** 2 / (m - 1)S2/(m1)contains?",
        "context": "Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What does the pseudorandom number generator do to obtain repeatable results?": {
        "answer": "reset the seed",
        "question": "What does the pseudorandom number generator do to obtain repeatable results?",
        "context": "S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the default value of q?": {
        "answer": "min(6,m,n)",
        "question": "What is the default value of q?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the input tensor of size(,m,n)(*, m, n)(,m,": {
        "answer": "A(Tensor)",
        "question": "What is the input tensor of size(,m,n)(*, m, n)(,m,",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "By default, what is the center of the input tensor?": {
        "answer": "By default,q=min(6,m,n)",
        "question": "By default, what is the center of the input tensor?",
        "context": "Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "By default, q=min(6,m,n). center(bool,optional) \u2013 if True, center the": {
        "answer": "By default,q=min(6,m,n)",
        "question": "By default, q=min(6,m,n). center(bool,optional) \u2013 if True, center the",
        "context": "UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the number of subspace iterations to conduct?": {
        "answer": "specified by niter",
        "question": "What is the number of subspace iterations to conduct?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "By default, what is the seed for the pseudorandom number generator A(Tensor)?": {
        "answer": "By default,q=min(6,m,n)",
        "question": "By default, what is the seed for the pseudorandom number generator A(Tensor)?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What does the returned tensor andinputtensor share?": {
        "answer": "underlying storage",
        "question": "What does the returned tensor andinputtensor share?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What is the starting dimension length of the tensor?": {
        "answer": "the distance to the ending dimension",
        "question": "What is the starting dimension length of the tensor?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "Returns a new tensor that is what?": {
        "answer": "a narrowed version ofinputtensor",
        "question": "Returns a new tensor that is what?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What is the name of the inputtensor that returns a new tensor that is a narrowed version ofinputten": {
        "answer": "dimensiondimis input fromstarttostart+length",
        "question": "What is the name of the inputtensor that returns a new tensor that is a narrowed version ofinputten",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What do the returned tensor andinputtensor share?": {
        "answer": "underlying storage",
        "question": "What do the returned tensor andinputtensor share?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "Input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow what": {
        "answer": "start",
        "question": "Input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow what",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What function returns the minimum value of all elements in?": {
        "answer": "theinputtensor",
        "question": "What function returns the minimum value of all elements in?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the value of each row of theinputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the value of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is argmin?": {
        "answer": "index location",
        "question": "What is argmin?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What kind of (sub)gradients does this function produce?": {
        "answer": "deterministic",
        "question": "What kind of (sub)gradients does this function produce?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "Returns a namedtuple(values,indices) wherevaluesis the minimum value of each row of the inputtensor": {
        "answer": "wherevaluesis the minimum value of each row of theinputtensor in the given dimensiondim",
        "question": "Returns a namedtuple(values,indices) wherevaluesis the minimum value of each row of the inputtensor",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the minimum value of each row of the inputtensor in the given dimensiondim?": {
        "answer": "index location",
        "question": "What is the minimum value of each row of the inputtensor in the given dimensiondim?",
        "context": "Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What type of subgradients does this function produce?": {
        "answer": "deterministic",
        "question": "What type of subgradients does this function produce?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the value of each row of the inputtensor in the given dimensiondim?": {
        "answer": "the minimum value of each row of theinputtensor in the given dimensiondim",
        "question": "What is the value of each row of the inputtensor in the given dimensiondim?",
        "context": "Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What does indices mean for each minimum value found?": {
        "answer": "index location",
        "question": "What does indices mean for each minimum value found?",
        "context": "input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "IfkeepdimisTrue, the output tensors have what?": {
        "answer": "1 fewer dimension than input",
        "question": "IfkeepdimisTrue, the output tensors have what?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What does the output tensors have the same size asinput?": {
        "answer": "If keepdim is True",
        "question": "What does the output tensors have the same size asinput?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "If the output tensors are of the same size as input except in the dimensiondimwhere they are of size 1 what is the default": {
        "answer": "IfkeepdimisTrue",
        "question": "If the output tensors are of the same size as input except in the dimensiondimwhere they are of size 1 what is the default",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "If keepdimisTrue, the output tensors are of the same size asinput except in the dimensiondim where they": {
        "answer": "1 fewer dimension",
        "question": "If keepdimisTrue, the output tensors are of the same size asinput except in the dimensiondim where they",
        "context": "Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What happens if there are multiple minimal values in a reduced row?": {
        "answer": "the indices of the first minimal value are returned",
        "question": "What happens if there are multiple minimal values in a reduced row?",
        "context": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the dimension to reduce?": {
        "answer": "dim(int)",
        "question": "What is the dimension to reduce?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "If the output tensors are of the same size as input, what is the default?": {
        "answer": "IfkeepdimisTrue",
        "question": "If the output tensors are of the same size as input, what is the default?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "If keepdimisTrue, the output tensors are of the same size as input except in the dimensiondimwhere they are": {
        "answer": "1 fewer dimension",
        "question": "If keepdimisTrue, the output tensors are of the same size as input except in the dimensiondimwhere they are",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What are returned if there are multiple minimal values in a reduced row?": {
        "answer": "the indices of the first minimal value",
        "question": "What are returned if there are multiple minimal values in a reduced row?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What determines whether the output tensor hasdimretained or not?": {
        "answer": "keepdim(bool)",
        "question": "What determines whether the output tensor hasdimretained or not?",
        "context": "IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "Out(tuple,optional) \u2013 what is the tuple of two output tensors?": {
        "answer": "the tuple of two output tensors",
        "question": "Out(tuple,optional) \u2013 what is the tuple of two output tensors?",
        "context": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What type of function does this function call LAPACK's geqrf directly?": {
        "answer": "low-level",
        "question": "What type of function does this function call LAPACK's geqrf directly?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What does this function return for geqrf?": {
        "answer": "namedtuple",
        "question": "What does this function return for geqrf?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is the function called for calling LAPACK's geqrf directly?": {
        "answer": "low-level function",
        "question": "What is the function called for calling LAPACK's geqrf directly?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What are stored in the same output tensora?": {
        "answer": "Both Q and R matrices",
        "question": "What are stored in the same output tensora?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Where are elements ofRare stored?": {
        "answer": "on and above the diagonal",
        "question": "Where are elements ofRare stored?",
        "context": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Where is matrix Q are stored?": {
        "answer": "below the diagonal",
        "question": "Where is matrixQare stored?",
        "context": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What does withtorch.ormqr() use for?": {
        "answer": "matrix-matrix multiplication",
        "question": "What does withtorch.ormqr() use for?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "For more details, see the LAPACK documentation for what?": {
        "answer": "geqrf",
        "question": "For more details, see the LAPACK documentation for what?",
        "context": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What does thedriver=\"gels\"option use to solve matrix equations?": {
        "answer": "a QR decomposition",
        "question": "What does thedriver=\"gels\"option use to solve matrix equations?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is the output tuple of (Tensor, Tensor)?": {
        "answer": "input(Tensor)",
        "question": "What is the output tuple of (Tensor, Tensor)?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is Ignored by a function that can solve matrix equations using a QR decomposition?": {
        "answer": "ifNone is ignored",
        "question": "What is Ignored by a function that can solve matrix equations using a QR decomposition?",
        "context": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Where does each row containnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ": {
        "answer": "tensor",
        "question": "Where does each row containnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What do rows of input do not need to sum to one?": {
        "answer": "non-negative, finite and have a non-zero sum",
        "question": "What do rows ofinputdo not need to sum to one?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What are ordered from left to right according to when each was sampled?": {
        "answer": "Indices",
        "question": "What are ordered from left to right according to when each was sampled?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is a vector of sizenum_samples?": {
        "answer": "If input is a vector",
        "question": "What is a vector of sizenum_samples?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is returned when each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row": {
        "answer": "a tensor",
        "question": "What is returned when each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row",
        "context": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What must the rows ofinput be?": {
        "answer": "non-negative, finite and have a non-zero sum",
        "question": "What must the rows ofinput be?",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Indices are ordered from left to right according to when each was sampled.": {
        "answer": "first samples are placed in first column",
        "question": "Indices are ordered from left to right according to when each was sampled.",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What are ordered from left to right according to when each sample was sampled?": {
        "answer": "Indices",
        "question": "What are ordered from left to right according to when each sample was sampled?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a vector,outis a vector of what?": {
        "answer": "sizenum_samples",
        "question": "Ifinputis a vector,outis a vector of what?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is ifinputis a matrix of shape?": {
        "answer": "matrix with m rows",
        "question": "What is ifinputis a matrix of shape?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If what isTrue, samples are drawn with replacement?": {
        "answer": "replacement",
        "question": "If what isTrue, samples are drawn with replacement?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "How many rows of inputdo not need to sum to one?": {
        "answer": "rows ofinputdo not need to sum to one",
        "question": "How many rows of inputdo not need to sum to one?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a vector,outis a matrix of what?": {
        "answer": "matrix withmrows",
        "question": "Ifinputis a vector,outis a matrix of what?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If what isTrue, samples are drawn with what?": {
        "answer": "replacement",
        "question": "If what isTrue, samples are drawn with what?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What do rows ofinputdo have to be?": {
        "answer": "non-negative, finite and have a non-zero sum",
        "question": "What do rows ofinputdo have to be?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What must the rows of inputdo not need to sum to one?": {
        "answer": "non-negative, finite and have a non-zero sum",
        "question": "What must the rows of inputdo not need to sum to one?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a matrix withmrows,outis a matrix of shape(mnum_samples)(m": {
        "answer": "Ifinputis a vector",
        "question": "Ifinputis a matrix withmrows,outis a matrix of shape(mnum_samples)(m",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a matrix,outis a matrix of what?": {
        "answer": "matrix withmrows",
        "question": "Ifinputis a matrix,outis a matrix of what?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If what is true, samples are drawn with replacement?": {
        "answer": "replacement",
        "question": "If what is true, samples are drawn with replacement?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What happens when a sample index is drawn for a row?": {
        "answer": "they are drawn without replacement",
        "question": "What happens when a sample index is drawn for a row?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the name of a sample index that cannot be drawn again for a row?": {
        "answer": "Note",
        "question": "What is the name of a sample index that cannot be drawn again for a row?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "How are samples ordered from left to right?": {
        "answer": "first samples are placed in first column",
        "question": "How are samples ordered from left to right?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a matrix,outis a matrix of shape(mnum_samples)(m times": {
        "answer": "matrix withmrows",
        "question": "Ifinputis a matrix,outis a matrix of shape(mnum_samples)(m times",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If a sample index is drawn for what, it cannot be drawn again for that row?": {
        "answer": "a row",
        "question": "If a sample index is drawn for what, it cannot be drawn again for that row?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the difference between a vector and a matrix of sizenum_samples?": {
        "answer": "Note",
        "question": "What is the difference between a vector and a matrix of sizenum_samples?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What means that when a sample index is drawn for a row, it cannot be drawn again for that row?": {
        "answer": "they are drawn without replacement",
        "question": "What means that when a sample index is drawn for a row, it cannot be drawn again for that row?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is outputis a vector of sizenum_samples?": {
        "answer": "Ifinputis a vector",
        "question": "What is outputis a vector of sizenum_samples?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the difference between samples drawn with replacement?": {
        "answer": "If replacement isTrue",
        "question": "What is the difference between samples drawn with replacement?",
        "context": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the min number of non-zero elements in each row ofinputif it is a matrix?": {
        "answer": "number of non-zero elements in input",
        "question": "What is the min number of non-zero elements in each row ofinputif it is a matrix?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is input tensor?": {
        "answer": "input tensor",
        "question": "What is input tensor?",
        "context": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If what is true, samples are drawn with replacement. If not, they are drawn without replacement?": {
        "answer": "If replacement isTrue",
        "question": "If what is true, samples are drawn with replacement. If not, they are drawn without replacement?",
        "context": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "When a sample index is drawn without replacement, it cannot be drawn again for what row?": {
        "answer": "a row",
        "question": "When a sample index is drawn without replacement, it cannot be drawn again for what row?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If num_samples is lower than the min number of non-zero elements in each row of input, what is it?": {
        "answer": "matrix",
        "question": "If num_samples is lower than the min number of non-zero elements in each row of input, what is it?",
        "context": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the input tensor containing probabilities?": {
        "answer": "input(Tensor)",
        "question": "What is the input tensor containing probabilities?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is drawn for a row when it cannot be drawn again for that row?": {
        "answer": "a sample index",
        "question": "What is drawn for a row when it cannot be drawn again for that row?",
        "context": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the input tensor containing probabilities num_samples?": {
        "answer": "input(Tensor)",
        "question": "What is the input tensor containing probabilities num_samples?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "num_samples must be lower than the min number of non-zero elements in each row of input if it is a": {
        "answer": "matrix",
        "question": "num_samples must be lower than the min number of non-zero elements in each row of input if it is a",
        "context": "Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is an example of a pseudorandom number generator?": {
        "answer": "Example",
        "question": "What is an example of a pseudorandom number generator?",
        "context": "Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What does PyTorch use to represent neural networks?": {
        "answer": "modules",
        "question": "What does PyTorch use to represent neural networks?",
        "context": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the building blocks of stateful computation?": {
        "answer": "Modules",
        "question": "What are the building blocks of stateful computation?",
        "context": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How does PyTorch integrate with its autogradsystem?": {
        "answer": "Tightly integrated",
        "question": "How does PyTorch integrate with its autogradsystem?",
        "context": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What type of networks can PyTorch modules allow for?": {
        "answer": "multi-layer neural networks",
        "question": "What type of networks can PyTorch modules allow for?",
        "context": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How does PyTorch'sautogradsystem integrate?": {
        "answer": "Tightly integrated",
        "question": "How does PyTorch'sautogradsystem integrate?",
        "context": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are some of PyTorch's modules able to do?": {
        "answer": "prune, quantize",
        "question": "What are some of PyTorch's modules able to do?",
        "context": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Modules make it simple to specify what for PyTorch\u2019s Optimizers to update?": {
        "answer": "learnable parameters",
        "question": "Modules make it simple to specify what for PyTorch\u2019s Optimizers to update?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How are Modules easy to work with and transform?": {
        "answer": "Easy to work with and transform",
        "question": "How are Modules easy to work with and transform?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is PyTorch's Optimizers tightly integrated with?": {
        "answer": "PyTorch\u2019sautogradsystem",
        "question": "What is PyTorch's Optimizers tightly integrated with?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are modules easy to work with and do?": {
        "answer": "transform",
        "question": "What are modules easy to work with and do?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is easy to work with and transform?": {
        "answer": "Easy to work with and transform",
        "question": "What is easy to work with and transform?",
        "context": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Why are modules so fundamental to PyTorch?": {
        "answer": "modules are so fundamental to PyTorch",
        "question": "Why are modules so fundamental to PyTorch?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a simple custom module module called?": {
        "answer": "Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "question": "What is a simple custom module module called?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How are modules to save and restore?": {
        "answer": "straightforward",
        "question": "How are modules to save and restore?",
        "context": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Who is this note intended for?": {
        "answer": "all PyTorch users",
        "question": "Who is this note intended for?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are so fundamental to PyTorch?": {
        "answer": "modules",
        "question": "What are so fundamental to PyTorch?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the Building Blocks of Neural Network Training?": {
        "answer": "Modules Module State",
        "question": "What are the Building Blocks of Neural Network Training?",
        "context": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does this note describe?": {
        "answer": "modules",
        "question": "What does this note describe?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a Simple Custom Module Module Module as Building Blocks Neural Network Training with Modules?": {
        "answer": "Module State Module Hooks Advanced Features",
        "question": "What is a Simple Custom Module Module Module as Building Blocks Neural Network Training with Modules?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What type of training does A Simple Custom Module Modules provide?": {
        "answer": "Neural Network Training",
        "question": "What type of training does A Simple Custom Module Modules provide?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module applies an affine transformation to its input?": {
        "answer": "PyTorch\u2019sLinearmodule",
        "question": "What module applies an affine transformation to its input?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does PyTorch'sLinearmodule apply to its input?": {
        "answer": "an affine transformation",
        "question": "What does PyTorch'sLinearmodule apply to its input?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module inherits from the base Module class?": {
        "answer": "module",
        "question": "What module inherits from the base Module class?",
        "context": "This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are Modules as Building Blocks Neural Network Training with?": {
        "answer": "Module State Module Hooks Advanced Features",
        "question": "What are Modules as Building Blocks Neural Network Training with?",
        "context": "Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What basic characteristics does PyTorch'sLinearmodule have?": {
        "answer": "module has the following fundamental characteristics of modules",
        "question": "What basic characteristics does PyTorch'sLinearmodule have?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features?": {
        "answer": "Simple Custom Module Modules",
        "question": "What are Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does PyTorch'sLinearmodule have?": {
        "answer": "Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "question": "What does PyTorch'sLinearmodule have?",
        "context": "Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the advanced features of Neural Network Training?": {
        "answer": "Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "question": "What is the name of the advanced features of Neural Network Training?",
        "context": "Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a custom version of PyTorch'sLinearmodule?": {
        "answer": "Module Hooks Advanced Features",
        "question": "What is a custom version of PyTorch'sLinearmodule?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the basic features of PyTorch'sLinearmodule?": {
        "answer": "Module Hooks Advanced Features",
        "question": "What are the basic features of PyTorch'sLinearmodule?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module uses an affine transformation to its input?": {
        "answer": "PyTorch\u2019sLinearmodule",
        "question": "What module uses an affine transformation to its input?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a simple, custom version of PyTorch'sLinearmodule?": {
        "answer": "a simpler, custom version of PyTorch\u2019sLinearmodule",
        "question": "What is a simple, custom version of PyTorch'sLinearmodule?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What should subclassModule for composability with other modules?": {
        "answer": "module",
        "question": "What should subclassModule for composability with other modules?",
        "context": "It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the module define in computation?": {
        "answer": "state",
        "question": "What does the module define in computation?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is each of the state defined as?": {
        "answer": "aParameter",
        "question": "What is each of the state defined as?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be considered the \"learnable\" aspects of the module\u2019s computation?": {
        "answer": "Parameters",
        "question": "What can be considered the \"learnable\" aspects of the module\u2019s computation?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Parameters are not required to have what?": {
        "answer": "state",
        "question": "Parameters are not required to have what?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What do random-initializedweightandbiastensors define?": {
        "answer": "affine transformation",
        "question": "What do random-initializedweightandbiastensors define?",
        "context": "This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is each of the randomly-initializedweightandbiastensors defined as?": {
        "answer": "aParameter",
        "question": "What is each of the randomly-initializedweightandbiastensors defined as?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Parameters can be considered what?": {
        "answer": "the \u201clearnable\u201d aspects of the module\u2019s computation",
        "question": "Parameters can be considered what?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are not required to have state, and can also be stateless?": {
        "answer": "modules",
        "question": "What are not required to have state, and can also be stateless?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What function performs the computation?": {
        "answer": "forward()",
        "question": "What function performs the computation?",
        "context": "It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can perform arbitrary computation involving any number of inputs and outputs?": {
        "answer": "theforward()implementation",
        "question": "What can perform arbitrary computation involving any number of inputs and outputs?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "The input is matrix-multiplied with what?": {
        "answer": "theweightparameter",
        "question": "The input is matrix-multiplied with what?",
        "context": "It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can theforward()implementation for a module perform?": {
        "answer": "arbitrary computation involving any number of inputs and outputs",
        "question": "What can theforward()implementation for a module perform?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module demonstrates how modules package state and computation together?": {
        "answer": "module",
        "question": "What module demonstrates how modules package state and computation together?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be constructed and called?": {
        "answer": "Instances",
        "question": "What can be constructed and called?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What demonstrates how modules package state and computation together?": {
        "answer": "module",
        "question": "What demonstrates how modules package state and computation together?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are aspects of the module\u2019s computation that should be \u201clearned\u201d?": {
        "answer": "parameters registered by a module",
        "question": "What are aspects of the module\u2019s computation that should be \u201clearned\u201d?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the Optimizer used by a module?": {
        "answer": "PyTorch",
        "question": "What is the name of the Optimizer used by a module?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Before we get to that, let\u2019s first examine what?": {
        "answer": "how modules can be composed with one another",
        "question": "Before we get to that, let\u2019s first examine what?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are aspects of a module's computation that should be learned?": {
        "answer": "the parameters registered by a module",
        "question": "What are aspects of a module's computation that should be learned?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does PyTorch use to update parameters?": {
        "answer": "Optimizers",
        "question": "What does PyTorch use to update parameters?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What should we examine first?": {
        "answer": "how modules can be composed with one another",
        "question": "What should we examine first?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the easiest way to do this?": {
        "answer": "using theSequentialmodule",
        "question": "What is the easiest way to do this?",
        "context": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Where does theSequentialmodule feed the output of the firstMyLinearmodule?": {
        "answer": "theReLU",
        "question": "Where does theSequentialmodule feed the output of the firstMyLinearmodule?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is theSequentialmodule limited to?": {
        "answer": "in-order chaining of modules",
        "question": "What is theSequentialmodule limited to?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can contain other modules, making them useful building blocks for developing more elaborate functionality?": {
        "answer": "Modules",
        "question": "What can contain other modules, making them useful building blocks for developing more elaborate functionality?",
        "context": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What allows us to chain together multiple modules?": {
        "answer": "theSequentialmodule",
        "question": "What allows us to chain together multiple modules?",
        "context": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does Sequential feed the output of the first MyLinearmodule as input into?": {
        "answer": "theReLU",
        "question": "What does Sequential feed the output of the first MyLinearmodule as input into?",
        "context": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What doesSequential automatically feed the output of the firstMyLinearmodule?": {
        "answer": "input into theReLU",
        "question": "What doesSequential automatically feed the output of the firstMyLinearmodule?",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How is the output of the firstMyLinearmodule limited?": {
        "answer": "in-order chaining of modules",
        "question": "How is the output of the firstMyLinearmodule limited?",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is recommended for anything beyond the simplest use cases?": {
        "answer": "a custom module",
        "question": "What is recommended for anything beyond the simplest use cases?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a simple neural network implemented as?": {
        "answer": "a custom module",
        "question": "What is a simple neural network implemented as?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does Sequentialautomatically feed the output of the first MyLinearmodule as input into?": {
        "answer": "theReLU",
        "question": "What does Sequentialautomatically feed the output of the first MyLinearmodule as input into?",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the limit of Sequentialautomatically feeding the output of the firstMyLinearmodule as input into the secondMyLinearmodule?": {
        "answer": "in-order chaining of modules",
        "question": "What is the limit of Sequentialautomatically feeding the output of the firstMyLinearmodule as input into the secondMyLinearmodule?",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Why is it recommended to define a custom module for anything beyond the simplest use cases?": {
        "answer": "full flexibility on how submodules are used for a module\u2019s computation",
        "question": "Why is it recommended to define a custom module for anything beyond the simplest use cases?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is implemented as a custom module?": {
        "answer": "a simple neural network",
        "question": "What is implemented as a custom module?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a custom module?": {
        "answer": "a simple neural network",
        "question": "What is a custom module?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does defining a custom module give you?": {
        "answer": "full flexibility",
        "question": "What does defining a custom module give you?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the two submodules of a neural network called?": {
        "answer": "children",
        "question": "What are the two submodules of a neural network called?",
        "context": "For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be iterated through via a call tochildren() ornamed_children()?": {
        "answer": "children",
        "question": "What can be iterated through via a call tochildren() ornamed_children()?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module registers submodules from a list or dict?": {
        "answer": "TheModuleListandModuleDictmodules",
        "question": "What module registers submodules from a list or dict?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What do TheModuleListandModuleDictmodules do?": {
        "answer": "register submodules from a list or dict",
        "question": "What do TheModuleListandModuleDictmodules do?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is sometimes necessary for a module to do?": {
        "answer": "dynamically define submodules",
        "question": "What is sometimes necessary for a module to do?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the call toparameters()andnamed_parameters() include?": {
        "answer": "child parameters",
        "question": "What does the call toparameters()andnamed_parameters() include?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Sometimes, it\u2019s necessary for a module to do what?": {
        "answer": "dynamically define submodules",
        "question": "Sometimes, it\u2019s necessary for a module to do what?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What do calls toparameters() andnamed_parameters() do?": {
        "answer": "recursively include child parameters",
        "question": "What do calls toparameters() andnamed_parameters() do?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the parameters of a module?": {
        "answer": "its direct parameters as well as the parameters of all submodules",
        "question": "What are the parameters of a module?",
        "context": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can elaborate neural networks be formed?": {
        "answer": "module composition",
        "question": "How can elaborate neural networks be formed?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "PyTorch provides a large library of modules that perform computation commonly found within neural networks?": {
        "answer": "thetorch.nnnamespace",
        "question": "PyTorch provides a large library of modules that perform computation commonly found within neural networks?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What provides a large library of performant modules within thetorch.nnnamespace?": {
        "answer": "PyTorch",
        "question": "What provides a large library of performant modules within thetorch.nnnamespace?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "In the next section, we give a full example of what?": {
        "answer": "training a neural network",
        "question": "In the next section, we give a full example of what?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does PyTorch.nnnamespace provide?": {
        "answer": "more information",
        "question": "What does PyTorch.nnnamespace provide?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What do we give in the next section of training a neural network?": {
        "answer": "a full example",
        "question": "What do we give in the next section of training a neural network?",
        "context": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can you find on PyTorch's website?": {
        "answer": "more information",
        "question": "What can you find on PyTorch's website?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the next section give a full example of?": {
        "answer": "training a neural network",
        "question": "What does the next section give a full example of?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a function that can be used to train a neural network?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is a function that can be used to train a neural network?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be used to optimize a neural network?": {
        "answer": "PyTorch\u2019s Optimizers",
        "question": "What can be used to optimize a neural network?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a function that can be used to a module and its submodules library of PyTorch-provided modules?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is a function that can be used to a module and its submodules library of PyTorch-provided modules?",
        "context": "For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be used to optimize a network's parameters?": {
        "answer": "PyTorch\u2019s Optimizers",
        "question": "What can be used to optimize a network's parameters?",
        "context": "For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What function does a module and its submodules library of PyTorch-provided modules:torch.nn have?": {
        "answer": "Recursivelyapply()a function",
        "question": "What function does a module and its submodules library of PyTorch-provided modules:torch.nn have?",
        "context": "Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be used to optimize a neural net?": {
        "answer": "PyTorch\u2019s Optimizers",
        "question": "What can be used to optimize a neural net?",
        "context": "Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of a network that needs to be trained?": {
        "answer": "Defining neural net modules",
        "question": "What is the name of a network that needs to be trained?",
        "context": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is present in a network?": {
        "answer": "key parts of training",
        "question": "What is present in a network?",
        "context": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "When a network is built, what does it need to be trained?": {
        "answer": "it has to be trained",
        "question": "When a network is built, what does it need to be trained?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the key part of training?": {
        "answer": "A network is created",
        "question": "What is the key part of training?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What type of optimizer is created?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What type of optimizer is created?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does an optimizer do?": {
        "answer": "computes a loss",
        "question": "What does an optimizer do?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of an optimizer created?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What is the name of an optimizer created?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What happens to the network's parameters' gradients?": {
        "answer": "zeros",
        "question": "What happens to the network's parameters' gradients?",
        "context": "An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the optimizer that is created when a network is created?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What is the name of the optimizer that is created when a network is created?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the optimizer do?": {
        "answer": "computes a loss",
        "question": "What does the optimizer do?",
        "context": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the optimizer that is created?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What is the name of the optimizer that is created?",
        "context": "An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the value of the network's parameters' gradients?": {
        "answer": "zeros",
        "question": "What is the value of the network's parameters' gradients?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the value ofl1'sweightparameter closer to?": {
        "answer": "0",
        "question": "What is the value ofl1'sweightparameter closer to?",
        "context": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the network do?": {
        "answer": "computes a loss",
        "question": "What does the network do?",
        "context": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the function used to train a neural network?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is the name of the function used to train a neural network?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is an example of training a neural network?": {
        "answer": "Using Optimizers",
        "question": "What is an example of training a neural network?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does loss.backward() call to apply the gradients to the parameters?": {
        "answer": "optimizer.step()",
        "question": "What does loss.backward() call to apply the gradients to the parameters?",
        "context": "calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Who computes a loss, zeros the network's parameters' gradients, calls loss.backward() to update the parameters' gradients": {
        "answer": "runs the network",
        "question": "Who computes a loss, zeros the network's parameters' gradients, calls loss.backward() to update the parameters' gradients",
        "context": "runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can sometimes be tricky?": {
        "answer": "Training neural networks",
        "question": "What can sometimes be tricky?",
        "context": "zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How many layers does _layer_net_optim.html have?": {
        "answer": "two",
        "question": "How many layers does _layer_net_optim.html have?",
        "context": "Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is called to apply gradients to the parameters?": {
        "answer": "optimizer.step()",
        "question": "What is called to apply gradients to the parameters?",
        "context": "calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How many layers of a neural network can be optimized?": {
        "answer": "two",
        "question": "How many layers of a neural network can be optimized?",
        "context": "calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What shows that the value ofl1's weightparameter is now much closer to 0?": {
        "answer": "examining the value ofl1\u2019sweightparameter",
        "question": "What shows that the value ofl1's weightparameter is now much closer to 0?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the network that trains neural networks?": {
        "answer": "Using Optimizers",
        "question": "What is the name of the network that trains neural networks?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the network that is used to train neural networks?": {
        "answer": "Neural network training",
        "question": "What is the name of the network that is used to train neural networks?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the value ofl1\u2019sweightparameter now closer to?": {
        "answer": "0",
        "question": "What is the value ofl1\u2019sweightparameter now closer to?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What type of training can be tricky?": {
        "answer": "Neural network training",
        "question": "What type of training can be tricky?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can we save if we want to save a model to disk?": {
        "answer": "itsstate_dict",
        "question": "What can we save if we want to save a model to disk?",
        "context": "In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is itsstate_dict?": {
        "answer": "state dictionary",
        "question": "What is itsstate_dict?",
        "context": "In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What state does a module's _dictcontains state that affects its computation?": {
        "answer": "state",
        "question": "What state does a module's _dictcontains state that affects its computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does a module'sstate_dictcontains state that affects its computation?": {
        "answer": "module\u2019s parameters",
        "question": "What does a module'sstate_dictcontains state that affects its computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a module'sstate_dictcontains state that affects module computation?": {
        "answer": "beyond parameters",
        "question": "What is a module'sstate_dictcontains state that affects module computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What provides the concept of \"buffers\"?": {
        "answer": "PyTorch",
        "question": "What provides the concept of \"buffers\"?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the parameters of a module'sstate_dictcontains state that affects its computation?": {
        "answer": "learnable aspects of computation",
        "question": "What are the parameters of a module'sstate_dictcontains state that affects its computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does a module'sstate_dict contain?": {
        "answer": "state that affects its computation",
        "question": "What does a module'sstate_dict contain?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "A module'sstate_dict includes state that affects its computation. This includes, but is not limited to, what?": {
        "answer": "the module\u2019s parameters",
        "question": "A module'sstate_dict includes state that affects its computation. This includes, but is not limited to, what?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "For some modules, it may be useful to have state beyond parameters that affects module computation but is what?": {
        "answer": "not learnable",
        "question": "For some modules, it may be useful to have state beyond parameters that affects module computation but is what?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does PyTorch provide for state beyond parameters that affects module computation but is not learnable?": {
        "answer": "buffers",
        "question": "What does PyTorch provide for state beyond parameters that affects module computation but is not learnable?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Parameters are contained within what?": {
        "answer": "thestate_dict",
        "question": "Parameters are contained within what?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Where are learnable aspects of computation contained?": {
        "answer": "thestate_dict",
        "question": "Where are learnable aspects of computation contained?",
        "context": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are non-learnable aspects of computation?": {
        "answer": "Persistentbuffers",
        "question": "What are non-learnable aspects of computation?",
        "context": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are non-persistentbuffers?": {
        "answer": "not contained within thestate_dict",
        "question": "What are non-persistentbuffers?",
        "context": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Non-persistentbuffers are left out of what?": {
        "answer": "serialization",
        "question": "Non-persistentbuffers are left out of what?",
        "context": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Where are Persistentbuffers contained?": {
        "answer": "thestate_dict",
        "question": "Where are Persistentbuffers contained?",
        "context": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Why are non-persistentbuffers not contained within thestate_dict?": {
        "answer": "left out of serialization",
        "question": "Why are non-persistentbuffers not contained within thestate_dict?",
        "context": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a motivating example for the use of buffers?": {
        "answer": "a simple module that maintains a running mean",
        "question": "What is a motivating example for the use of buffers?",
        "context": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the snippet show how to do this?": {
        "answer": "register_buffer()",
        "question": "What does the snippet show how to do this?",
        "context": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the purpose of the current value of the running mean?": {
        "answer": "it will be restored when loading a serialized form of the module",
        "question": "What is the purpose of the current value of the running mean?",
        "context": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does this snippet show how to use to accomplish this?": {
        "answer": "register_buffer()",
        "question": "What does this snippet show how to use to accomplish this?",
        "context": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is considered part of the module\u2019sstate_dictand?": {
        "answer": "the current value of the running mean",
        "question": "What is considered part of the module\u2019sstate_dictand?",
        "context": "Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "For what do you need to check out:": {
        "answer": "more information",
        "question": "For what do you need to check out:",
        "context": "Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be iterated over usingbuffers() ornamed_buffers()?": {
        "answer": "Buffers of a module",
        "question": "What can be iterated over usingbuffers() ornamed_buffers()?",
        "context": "Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html What is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "For more information, check out what?": {
        "answer": "Saving and loading",
        "question": "For more information, check out what?",
        "context": "Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html What is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are called during the backward pass?": {
        "answer": "Backward hooks",
        "question": "What are called during the backward pass?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can backward hooks be installed?": {
        "answer": "withregister_full_backward_hook()",
        "question": "How can backward hooks be installed?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "When will backward hooks be called?": {
        "answer": "when the backward for this Module has been computed",
        "question": "When will backward hooks be called?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can backward hooks be installed for all modules?": {
        "answer": "globally",
        "question": "How can backward hooks be installed for all modules?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "When are backward hooks called?": {
        "answer": "when the backward for this Module has been computed",
        "question": "When are backward hooks called?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What will be used throughout the rest of the computation?": {
        "answer": "updated value",
        "question": "What will be used throughout the rest of the computation?",
        "context": "All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can hooks be used to execute arbitrary code along the regular module forward/backward?": {
        "answer": "modify some inputs/outputs",
        "question": "How can hooks be used to execute arbitrary code along the regular module forward/backward?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What allows the user to return an updated value that will be used throughout the remaining computation?": {
        "answer": "hooks",
        "question": "What allows the user to return an updated value that will be used throughout the remaining computation?",
        "context": "All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can hooks be used to do without having to change the module'sforward()function?": {
        "answer": "modify some inputs/outputs",
        "question": "What can hooks be used to do without having to change the module'sforward()function?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What provides several more advanced features that are designed to work with modules?": {
        "answer": "PyTorch",
        "question": "What provides several more advanced features that are designed to work with modules?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the functionalities that PyTorch provides when writing a new module?": {
        "answer": "inherited",
        "question": "What are the functionalities that PyTorch provides when writing a new module?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Where can a detailed discussion of these features be found?": {
        "answer": "links below",
        "question": "Where can a detailed discussion of these features be found?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the feature that PyTorch provides?": {
        "answer": "Profiling",
        "question": "What is the name of the feature that PyTorch provides?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What company provides several more advanced features that are designed to work with modules?": {
        "answer": "PyTorch",
        "question": "What company provides several more advanced features that are designed to work with modules?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What happens to PyTorch's functionalities when writing a new module?": {
        "answer": "inherited",
        "question": "What happens to PyTorch's functionalities when writing a new module?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What type of discussion of PyTorch's advanced features can be found in the links below?": {
        "answer": "In-depth",
        "question": "What type of discussion of PyTorch's advanced features can be found in the links below?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is another name for profiling?": {
        "answer": "Pruning",
        "question": "What is another name for profiling?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the export of modules to?": {
        "answer": "TorchScript",
        "question": "What is the export of modules to?",
        "context": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "From what language does TorchScript export modules to?": {
        "answer": "C++",
        "question": "From what language does TorchScript export modules to?",
        "context": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Exporting modules to what?": {
        "answer": "TorchScript",
        "question": "Exporting modules to what?",
        "context": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What language is TorchScript used for?": {
        "answer": "C++",
        "question": "What language is TorchScript used for?",
        "context": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a multi-dimensional matrix containing elements of a single data type?": {
        "answer": "Atorch.Tensoris",
        "question": "What is a multi-dimensional matrix containing elements of a single data type?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many tensor types are defined by Torch?": {
        "answer": "10",
        "question": "How many tensor types are defined by Torch?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is Atorch.Tensoris?": {
        "answer": "multi-dimensional matrix",
        "question": "What is Atorch.Tensoris?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor type defined by Torch?": {
        "answer": "bfloat16",
        "question": "What is the name of the tensor type defined by Torch?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the bit floating point torch?": {
        "answer": "32",
        "question": "What is the bit floating point torch?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?": {
        "answer": "Data type dtype",
        "question": "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the data type dtype CPU tensor GPU tensor?": {
        "answer": "32-bit floating point torch",
        "question": "What is the data type dtype CPU tensor GPU tensor?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the float32ortorch?": {
        "answer": "32-bit floating point torch",
        "question": "What is the float32ortorch?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a 32-bit floating point torch?": {
        "answer": "CPU tensor GPU tensor",
        "question": "What is a 32-bit floating point torch?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the floating point torch?": {
        "answer": "32-bit floating point torch",
        "question": "What is the name of the floating point torch?",
        "context": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How many bits of complex torch is it?": {
        "answer": "8-bit",
        "question": "How many bits of complex torch is it?",
        "context": "GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a double 8-bit integer?": {
        "answer": "unsigned",
        "question": "What is a double 8-bit integer?",
        "context": "32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch?": {
        "answer": "bfloat16 torch",
        "question": "What is the name of the torch?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a double torch?": {
        "answer": "double torch",
        "question": "What is a double torch?",
        "context": "torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch?": {
        "answer": "128",
        "question": "How many -bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of tensoris float64?": {
        "answer": "double ",
        "question": "What type of tensoris float64?",
        "context": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the double torch?": {
        "answer": "torch.float64ortorch",
        "question": "What is the name of the double torch?",
        "context": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the floating point1 torch?": {
        "answer": "16-bit",
        "question": "What is the floating point1 torch?",
        "context": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.cuda.DoubleTensor?": {
        "answer": "16-bit floating point1 torch",
        "question": "What is the name of the torch.cuda.DoubleTensor?",
        "context": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a half torch?": {
        "answer": "half torch",
        "question": "What is a half torch?",
        "context": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the floating point1 torch?": {
        "answer": "16-bit floating point1 torch",
        "question": "What is the name of the floating point1 torch?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits floating point1 torch?": {
        "answer": "16",
        "question": "How many bits floating point1 torch?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.float16ortorch?": {
        "answer": "half torch",
        "question": "What is the name of the torch.float16ortorch?",
        "context": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the term for torch.float16ortorch?": {
        "answer": "half torch",
        "question": "What is the term for torch.float16ortorch?",
        "context": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits of complex torch does BFloat16Tensor have?": {
        "answer": "32",
        "question": "How many bits of complex torch does BFloat16Tensor have?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many byte complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128or": {
        "answer": "32",
        "question": "How many byte complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128or",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.cuda.HalfTensor?": {
        "answer": "16-bit floating point2 torch",
        "question": "What is the name of the torch.cuda.HalfTensor?",
        "context": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many byte complex torch?": {
        "answer": "32",
        "question": "How many byte complex torch?",
        "context": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the bfloat16 torch?": {
        "answer": "HalfTensor 16-bit floating point2 torch",
        "question": "What is the name of the bfloat16 torch?",
        "context": "torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What type of torch is bfloat16?": {
        "answer": "16-bit floating point2",
        "question": "What type of torch is bfloat16?",
        "context": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch does bfloat16Tensor have?": {
        "answer": "32-bit complex torch",
        "question": "What type of torch does bfloat16Tensor have?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is the BFloat16Tensor 32-bit complex torch.complex64 128-bit complex torch?": {
        "answer": "64-bit",
        "question": "What type of complex torch is the BFloat16Tensor 32-bit complex torch.complex64 128-bit complex torch?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128": {
        "answer": "32",
        "question": "How many -bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128",
        "context": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is unsigned torch.uint8 torch?": {
        "answer": "cdouble 8-bit integer",
        "question": "What is unsigned torch.uint8 torch?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is complex32 64-bit complex torch?": {
        "answer": "32-bit",
        "question": "What type of complex torch is complex32 64-bit complex torch?",
        "context": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits of complex torch is torch.complex32?": {
        "answer": "64",
        "question": "How many bits of complex torch is torch.complex32?",
        "context": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the LongTensor torch?": {
        "answer": "LongTensor torch.cuda",
        "question": "What is the name of the LongTensor torch?",
        "context": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is it?": {
        "answer": "64-bit",
        "question": "What type of complex torch is it?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the LongTensor Boolean torch?": {
        "answer": "LongTensor torch.cuda",
        "question": "What is the name of the LongTensor Boolean torch?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the Boolean torch?": {
        "answer": "Boolean torch",
        "question": "What is the name of the Boolean torch?",
        "context": "torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits complex torch?": {
        "answer": "128",
        "question": "How many bits complex torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the unsigned number of torch.uint8 torch?": {
        "answer": "8-bit",
        "question": "What is the unsigned number of torch.uint8 torch?",
        "context": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the unsigned part of a torch?": {
        "answer": "8-bit integer",
        "question": "What is the unsigned part of a torch?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.int8 torch.ByteTensor torch.cuda.Byte": {
        "answer": "8-bit integer",
        "question": "What is the name of the torch.int8 torch.ByteTensor torch.cuda.Byte",
        "context": "torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does ByteTensor stand for?": {
        "answer": "8-bit integer",
        "question": "What does ByteTensor stand for?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.int8 torch?": {
        "answer": "8-bit integer",
        "question": "What is the name of the torch.int8 torch?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the sign of a ByteTensor torch?": {
        "answer": "8-bit integer",
        "question": "What is the sign of a ByteTensor torch?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the sign of a torch?": {
        "answer": "16-bit integer",
        "question": "What is the sign of a torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.": {
        "answer": "16",
        "question": "How many -bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit integer (signed) torch.int16ortorch.short torch?": {
        "answer": "16",
        "question": "How many -bit integer (signed) torch.int16ortorch.short torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the signature of a torch?": {
        "answer": "16-bit integer",
        "question": "What is the signature of a torch?",
        "context": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the number of integers in torch.int32ortorch.int torch.IntTensor torch.cu": {
        "answer": "32-bit",
        "question": "What is the number of integers in torch.int32ortorch.int torch.IntTensor torch.cu",
        "context": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does CharTensor stand for?": {
        "answer": "16-bit integer",
        "question": "What does CharTensor stand for?",
        "context": "torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How large is the torch?": {
        "answer": "16-bit",
        "question": "How large is the torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.int32ortorch.int torch?": {
        "answer": "32-bit integer",
        "question": "What is the name of the torch.int32ortorch.int torch?",
        "context": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the short torch?": {
        "answer": "int16ortorch",
        "question": "What is the short torch?",
        "context": "torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits does the torch contain?": {
        "answer": "4-bit integer",
        "question": "How many bits does the torch contain?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the sign of an IntTensor?": {
        "answer": "4-bit integer",
        "question": "What is the sign of an IntTensor?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cu": {
        "answer": "4",
        "question": "How many -bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cu",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch.cuda.LongTensor Boolean torch.bool torch.BoolTen": {
        "answer": "LongTensor",
        "question": "What type of torch.cuda.LongTensor Boolean torch.bool torch.BoolTen",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does torch.tensor() always copydata?": {
        "answer": "torch.tensor()always copiesdata",
        "question": "What does torch.tensor() always copydata?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do you need to change to avoid a copy of a Tensordata?": {
        "answer": "userequires_grad_()ordetach()",
        "question": "What do you need to change to avoid a copy of a Tensordata?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used if you have a numpy array and want to avoid a copy?": {
        "answer": "usetorch.as_tensor()",
        "question": "What is used if you have a numpy array and want to avoid a copy?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "A tensor of specific data type can be constructed by passing what to a constructor or tensor creation op?": {
        "answer": "atorch.dtype",
        "question": "A tensor of specific data type can be constructed by passing what to a constructor or tensor creation op?",
        "context": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the indexing and slicing notation used to create a tensor?": {
        "answer": "Python",
        "question": "What is the name of the indexing and slicing notation used to create a tensor?",
        "context": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of Python's indexing and slicing notation to get a Python number from a tensor?": {
        "answer": "Usetorch.Tensor.item()",
        "question": "What is the name of Python's indexing and slicing notation to get a Python number from a tensor?",
        "context": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the purpose of creating a tensor?": {
        "answer": "automatic differentiation",
        "question": "What is the purpose of creating a tensor?",
        "context": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of Python's indexing and slicing notation?": {
        "answer": "Usetorch.Tensor.item()",
        "question": "What is the name of Python's indexing and slicing notation?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the purpose of a tensor?": {
        "answer": "change an existing tensor\u2019",
        "question": "What is the purpose of a tensor?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What can a tensor be created for automatic differentiation?": {
        "answer": "withrequires_grad=Trueso thattorch.autogradrecords operations",
        "question": "What can a tensor be created for automatic differentiation?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What holds the data of each tensor?": {
        "answer": "associatedtorch.Storage",
        "question": "What holds the data of each tensor?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the tensor class provide?": {
        "answer": "multi-dimensional,stridedview of a storage",
        "question": "What does the tensor class provide?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of a tensor class that provides multi-dimensional,stridedview of a storage?": {
        "answer": "Note",
        "question": "What is the name of a tensor class that provides multi-dimensional,stridedview of a storage?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Usetorch.Tensor.item() get a Python number from?": {
        "answer": "a tensor",
        "question": "What does Usetorch.Tensor.item() get a Python number from?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the associatedtorch.Storage do?": {
        "answer": "holds its data",
        "question": "What does the associatedtorch.Storage do?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the tensor class do?": {
        "answer": "defines numeric operations on it",
        "question": "What does the tensor class do?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage and defines numeric operations": {
        "answer": "Note",
        "question": "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage and defines numeric operations",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How can a tensor be created?": {
        "answer": "withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation",
        "question": "How can a tensor be created?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "For more information on tensor views, see what?": {
        "answer": "Tensor Views",
        "question": "For more information on tensor views, see what?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage?": {
        "answer": "Note",
        "question": "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What can a tensor be created?": {
        "answer": "withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation",
        "question": "What can a tensor be created?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What holds the data of a tensor?": {
        "answer": "associatedtorch.Storage",
        "question": "What holds the data of a tensor?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "For more information on tensor views, seeTensor what?": {
        "answer": "Views",
        "question": "For more information on tensor views, seeTensor what?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor class that can be created withrequires_grad=Trueso thattorch": {
        "answer": "Note",
        "question": "What is the name of the tensor class that can be created withrequires_grad=Trueso thattorch",
        "context": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor attributes of atorch.Tensor?": {
        "answer": "Note",
        "question": "What is the name of the tensor attributes of atorch.Tensor?",
        "context": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the tensor class provide of a storage?": {
        "answer": "multi-dimensional,stridedview",
        "question": "What does the tensor class provide of a storage?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What are some examples of atorch.Tensor attributes?": {
        "answer": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "question": "What are some examples of atorch.Tensor attributes?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Note Methods which mutate a tensor are marked with what?": {
        "answer": "underscore suffix",
        "question": "Note Methods which mutate a tensor are marked with what?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does torch.FloatTensor.abs()compute the result in?": {
        "answer": "a new tensor",
        "question": "What does torch.FloatTensor.abs()compute the result in?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of a method that mutates a tensor?": {
        "answer": "Warning",
        "question": "What is the name of a method that mutates a tensor?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of atorch.Tensor?": {
        "answer": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "question": "What is the name of atorch.Tensor?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the tensor'storch.deviceand/ortorch.dtype?": {
        "answer": "to()method",
        "question": "What is the tensor'storch.deviceand/ortorch.dtype?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor'storch.device and/ortorch.dtype?": {
        "answer": "Warning",
        "question": "What is the name of the tensor'storch.device and/ortorch.dtype?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is another name for a tensor?": {
        "answer": "atorch.Tensor",
        "question": "What is another name for a tensor?",
        "context": "Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does torch.FloatTensor.abs()compute?": {
        "answer": "the result in a new tensor",
        "question": "What does torch.FloatTensor.abs()compute?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What method is used to change an existing tensor'storch.deviceand/ortorch.dtype?": {
        "answer": "usingto()method",
        "question": "What method is used to change an existing tensor'storch.deviceand/ortorch.dtype?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of what": {
        "answer": "atorch.Tensor",
        "question": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of what",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usingto()method on a tensor do to change an existing tensor'storch.deviceand/or": {
        "answer": "Warning",
        "question": "What does usingto()method on a tensor do to change an existing tensor'storch.deviceand/or",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do Note Methods which mutate a tensor are marked with an underscore suffix?": {
        "answer": "usingto()method",
        "question": "What do Note Methods which mutate a tensor are marked with an underscore suffix?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does torch.FloatTensor.abs_() compute?": {
        "answer": "the absolute value in-place",
        "question": "What does torch.FloatTensor.abs_() compute?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What suffix does a method that mutate a tensor have?": {
        "answer": "underscore",
        "question": "What suffix does a method that mutate a tensor have?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the tensor need to change?": {
        "answer": "usingto()method",
        "question": "What does the tensor need to change?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does a tensor use to change their tensor'storch.deviceand/ortorch.dtype": {
        "answer": "usingto()method",
        "question": "What does a tensor use to change their tensor'storch.deviceand/ortorch.dtype",
        "context": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What could a current implementation oftorch.Tensor lead to?": {
        "answer": "unexpectedly high memory usage",
        "question": "What could a current implementation oftorch.Tensor lead to?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of tensor should you use to change existing tensor'storch.deviceand/ortorch.d": {
        "answer": "one large structure",
        "question": "What type of tensor should you use to change existing tensor'storch.deviceand/ortorch.d",
        "context": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does current implementation oftorch.Tensor introduce?": {
        "answer": "memory overhead",
        "question": "What does current implementation oftorch.Tensor introduce?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What should you use if you have many tiny tensors?": {
        "answer": "one large structure",
        "question": "What should you use if you have many tiny tensors?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How do you create a tensor with pre-existing data?": {
        "answer": "tensor",
        "question": "How do you create a tensor with pre-existing data?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch.tensor() do to create a tensor?": {
        "answer": "pre-existing data",
        "question": "What does usetorch.tensor() do to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch.tensor() have to do to create a tensor?": {
        "answer": "specific size",
        "question": "What does usetorch.tensor() have to do to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with the same size as another tensor?": {
        "answer": "*tensor creation ops",
        "question": "What is used to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a tensor with the same size as another tensor?": {
        "answer": "usetorch",
        "question": "What is a tensor with the same size as another tensor?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor created by usetorch.tensor?": {
        "answer": "*_liketensor creation ops",
        "question": "What is the name of the tensor created by usetorch.tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor with its dimensions reversed?": {
        "answer": "usetensor.new_*creation ops",
        "question": "What is a Tensor with its dimensions reversed?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor with reversed?": {
        "answer": "its dimensions",
        "question": "What is a Tensor with reversed?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the main way to create a tensor?": {
        "answer": "tensor",
        "question": "What is the main way to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch.tensor() create a tensor with?": {
        "answer": "pre-existing data",
        "question": "What does usetorch.tensor() create a tensor with?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do you need to create a tensor with?": {
        "answer": "specific size",
        "question": "What do you need to create a tensor with?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch do to create a tensor with specific size?": {
        "answer": "*tensor creation ops",
        "question": "What does usetorch do to create a tensor with specific size?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the size of a tensor?": {
        "answer": "same size",
        "question": "What is the size of a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What creates a tensor with the same size as another tensor?": {
        "answer": "*_liketensor creation ops",
        "question": "What creates a tensor with the same size as another tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of tensor is used to create a tensor?": {
        "answer": "similar type but different size",
        "question": "What type of tensor is used to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is reversed in a tensor?": {
        "answer": "its dimensions",
        "question": "What is reversed in a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the pre-existing data to create a tensor with pre-existing data?": {
        "answer": "usetorch.tensor()",
        "question": "What is the name of the pre-existing data to create a tensor with pre-existing data?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch.tensor() have to do to create a tensor with?": {
        "answer": "specific size",
        "question": "What does usetorch.tensor() have to do to create a tensor with?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor with the same size as another tensor?": {
        "answer": "*_liketensor creation ops",
        "question": "What is the name of the tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor with its dimensions reversed?": {
        "answer": "usetensor.new_*creation ops",
        "question": "What is the name of the tensor with its dimensions reversed?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetensor.new_*creation ops have?": {
        "answer": "its dimensions reversed",
        "question": "What does usetensor.new_*creation ops have?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the number of dimensions inx,x.Tis equivalent tox.permute?": {
        "answer": "Ifnis",
        "question": "What is the number of dimensions inx,x.Tis equivalent tox.permute?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with specific size?": {
        "answer": "*tensor creation ops",
        "question": "What is used to create a tensor with specific size?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Is this Tensor with its dimensions reversed or reversed?": {
        "answer": "reversed",
        "question": "Is this Tensor with its dimensions reversed or reversed?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Ifnis the number of dimensions inx,x.Tis equivalent what?": {
        "answer": "tox.permute",
        "question": "Ifnis the number of dimensions inx,x.Tis equivalent what?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch mean to create a tensor?": {
        "answer": "specific size",
        "question": "What does usetorch mean to create a tensor?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the Tensor have its dimensions?": {
        "answer": "reversed",
        "question": "What does the Tensor have its dimensions?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor equivalent tox.permute?": {
        "answer": "Tensor.new_tensor",
        "question": "What is the Tensor equivalent tox.permute?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Usetorch to create a tensor with what?": {
        "answer": "specific size",
        "question": "Usetorch to create a tensor with what?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do you use to create a tensor with specific size?": {
        "answer": "*tensor creation ops",
        "question": "What do you use to create a tensor with specific size?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with reversed dimensions?": {
        "answer": "Tensor.new_tensor",
        "question": "What is used to create a tensor with reversed dimensions?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor to create a tensor with the same size as another tensor?": {
        "answer": "usetorch",
        "question": "What is the name of the tensor to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of a tensor with similar type but different size as another tensor?": {
        "answer": "usetensor.new_*creation ops",
        "question": "What is the name of a tensor with similar type but different size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor with its dimensions?": {
        "answer": "reversed",
        "question": "What is a Tensor with its dimensions?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_tensor Returns a new Tensor with what?": {
        "answer": "data",
        "question": "Tensor.new_tensor Returns a new Tensor with what?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_full return?": {
        "answer": "Tensor of sizesizefilled withfill_value",
        "question": "What does Tensor.new_full return?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor of sizesizefilled withfill_value?": {
        "answer": "Tensor.new_empty",
        "question": "What is a Tensor of sizesizefilled withfill_value?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch do to create a tensor with the same size as another tensor?": {
        "answer": "*_liketensor creation ops",
        "question": "What does usetorch do to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of tensor does usetorch. *_liketensor creation ops create?": {
        "answer": "similar type but different size",
        "question": "What type of tensor does usetorch. *_liketensor creation ops create?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What Returns a new Tensor withdataas the tensor data?": {
        "answer": "new_tensor",
        "question": "What Returns a new Tensor withdataas the tensor data?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What value does Tensor.new_full return?": {
        "answer": "withfill_value",
        "question": "What value does Tensor.new_full return?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of tensor returns a new Tensor?": {
        "answer": "empty",
        "question": "What type of tensor returns a new Tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_tensor Returns a new Tensor with what as the tensor data?": {
        "answer": "data",
        "question": "Tensor.new_tensor Returns a new Tensor with what as the tensor data?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What returns a Tensor of sizesizefilled withfill_value?": {
        "answer": "Tensor.new_full",
        "question": "What returns a Tensor of sizesizefilled withfill_value?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_empty return a Tensor of sizesizefilled with?": {
        "answer": "uninitialized data",
        "question": "What does Tensor.new_empty return a Tensor of sizesizefilled with?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_ones Returns a Tensor of sizesizefilled with what?": {
        "answer": "1",
        "question": "Tensor.new_ones Returns a Tensor of sizesizefilled with what?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What ops is used to create a tensor with similar type but different size as another tensor?": {
        "answer": "tensor.new_*creation",
        "question": "What ops is used to create a tensor with similar type but different size as another tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Is this tensor with its dimensions reversed or reversed?": {
        "answer": "reversed",
        "question": "Is this tensor with its dimensions reversed or reversed?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_tensor return?": {
        "answer": "new Tensor",
        "question": "What does Tensor.new_tensor return?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What value does Tensor.new_full return a Tensor of sizesizefilled?": {
        "answer": "withfill_value",
        "question": "What value does Tensor.new_full return a Tensor of sizesizefilled?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What returns a Tensor of sizesizefilled with1?": {
        "answer": "Tensor.new_ones",
        "question": "What returns a Tensor of sizesizefilled with1?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the dimensions of the Tensor?": {
        "answer": "reversed",
        "question": "What is the dimensions of the Tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_zeros return?": {
        "answer": "a Tensor of sizesizefilled with0",
        "question": "What does Tensor.new_zeros return?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the Tensor with its dimensions reversed?": {
        "answer": "Tensor.is_cuda",
        "question": "What is the name of the Tensor with its dimensions reversed?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Ifnis the number of dimensions inx,x.Tis equivalent to what?": {
        "answer": "tox.permute",
        "question": "Ifnis the number of dimensions inx,x.Tis equivalent to what?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What returns a Tensor of sizesizefilled?": {
        "answer": "withfill_value",
        "question": "What returns a Tensor of sizesizefilled?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_ones Returns a Tensor of sizesizefilled what?": {
        "answer": "with1",
        "question": "Tensor.new_ones Returns a Tensor of sizesizefilled what?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_tensor return a new Tensor withdataas?": {
        "answer": "tensor data",
        "question": "What does Tensor.new_tensor return a new Tensor withdataas?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_empty Returns a Tensor of sizesizefilled with what?": {
        "answer": "uninitialized data",
        "question": "Tensor.new_empty Returns a Tensor of sizesizefilled with what?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is the Tensor stored?": {
        "answer": "GPU",
        "question": "Where is the Tensor stored?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is true if the Tensor is stored on the GPU?": {
        "answer": "Tensor.is_quantized",
        "question": "What is true if the Tensor is stored on the GPU?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_tensor Returns a new Tensor withdataas what?": {
        "answer": "tensor data",
        "question": "Tensor.new_tensor Returns a new Tensor withdataas what?",
        "context": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_full Returns a Tensor of what?": {
        "answer": "sizesizefilled withfill_value",
        "question": "Tensor.new_full Returns a Tensor of what?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_zeros Returns a Tensor of sizesizefilled with what?": {
        "answer": "0",
        "question": "Tensor.new_zeros Returns a Tensor of sizesizefilled with what?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What isTrueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What isTrueif the Tensor is quantized?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the Tensor?": {
        "answer": "Tensor",
        "question": "What is the name of the Tensor?",
        "context": "torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a new Tensor withdataas?": {
        "answer": "tensor data",
        "question": "What is a new Tensor withdataas?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Who returns a Tensor of sizesizefilled withfill_value?": {
        "answer": "Tensor",
        "question": "Who returns a Tensor of sizesizefilled withfill_value?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor?": {
        "answer": "quantized",
        "question": "What is the Tensor?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.is_cuda IsTrueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What does Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the meta tensor?": {
        "answer": "meta",
        "question": "What is the name of the meta tensor?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a meta tensor?": {
        "answer": "Tensor.device",
        "question": "What is a meta tensor?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor of?": {
        "answer": "sizesizefilled withfill_value",
        "question": "What is the Tensor of?",
        "context": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor.device?": {
        "answer": "thetorch.device",
        "question": "What is the Tensor.device?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor.is_cuda IsTrueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What is the Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the meta tensor?": {
        "answer": "meta",
        "question": "What is the meta tensor?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is thetorch.device where this Tensor is?": {
        "answer": "Tensor.grad",
        "question": "What is thetorch.device where this Tensor is?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does a Tensor of sizesizefilled with?": {
        "answer": "uninitialized data",
        "question": "What does a Tensor of sizesizefilled with?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_zeros returns a Tensor of sizesizefilled with what?": {
        "answer": "0",
        "question": "Tensor.new_zeros returns a Tensor of sizesizefilled with what?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor.is_trueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What is the Tensor.is_trueif the Tensor is quantized?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the first call for a Tensor?": {
        "answer": "tobackward()",
        "question": "What is the first call for a Tensor?",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor of sizesizefilled with?": {
        "answer": "1",
        "question": "What is the Tensor of sizesizefilled with?",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the Tensor are quantized?": {
        "answer": "quantized",
        "question": "What does the Tensor are quantized?",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What attribute isnoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself": {
        "answer": "Tensor.ndim Alias fordim() Tensor.real",
        "question": "What attribute isnoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself",
        "context": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_zeros returns a Tensor of what size?": {
        "answer": "size size filled with0",
        "question": "Tensor.new_zeros returns a Tensor of what size?",
        "context": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the Tensor.is_cuda IsTrueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What does the Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "context": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.grad isnoneby default and becomes a Tensor the first time a call?": {
        "answer": "tobackward()",
        "question": "Tensor.grad isnoneby default and becomes a Tensor the first time a call?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does a Tensor return?": {
        "answer": "sizesizefilled with0",
        "question": "What does a Tensor return?",
        "context": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor that is quantized?": {
        "answer": "quantized",
        "question": "What is the name of the tensor that is quantized?",
        "context": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor the first time a call tobackward()computes gradients forself?": {
        "answer": "Tensor.ndim Alias fordim() Tensor.real",
        "question": "What is a Tensor the first time a call tobackward()computes gradients forself?",
        "context": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What GPU is Tensor.is_cuda IsTrueif the Tensor is stored on?": {
        "answer": "GPU",
        "question": "What GPU is Tensor.is_cuda IsTrueif the Tensor is stored on?",
        "context": "Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What Returns a new tensor containing real values of theselftensor?": {
        "answer": "real",
        "question": "What Returns a new tensor containing real values of theselftensor?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a new tensor containing real values of theselftensor?": {
        "answer": "real",
        "question": "What is a new tensor containing real values of theselftensor?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is IsTrueif the Tensor is stored?": {
        "answer": "GPU",
        "question": "Where is IsTrueif the Tensor is stored?",
        "context": "IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What meta tensor isTrueif the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "What meta tensor isTrueif the Tensor is a meta tensor?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.grad isNoneby default and becomes a Tensor the first time a call?": {
        "answer": "tobackward()computes gradients forself",
        "question": "Tensor.grad isNoneby default and becomes a Tensor the first time a call?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing what?": {
        "answer": "real values of theselftensor",
        "question": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing what?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.imag Returns a new tensor containing what?": {
        "answer": "imaginary values of theselftensor",
        "question": "Tensor.imag Returns a new tensor containing what?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a new tensor containing imaginary values of theselftensor?": {
        "answer": "Tensor.abs",
        "question": "What is a new tensor containing imaginary values of theselftensor?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does it mean to be a meta tensor?": {
        "answer": "IsTrueif the Tensor is quantized",
        "question": "What does it mean to be a meta tensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor.is_?": {
        "answer": "meta",
        "question": "What is the Tensor.is_?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of Tensor.abs?": {
        "answer": "Seetorch.abs()",
        "question": "What is the name of Tensor.abs?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "IsTrueif the Tensor is what?": {
        "answer": "quantized",
        "question": "IsTrueif the Tensor is what?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is true if the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "What is true if the Tensor is a meta tensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the device where the Tensor is?": {
        "answer": "thetorch.device",
        "question": "What is the name of the device where the Tensor is?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself": {
        "answer": "Tensor.grad",
        "question": "What attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself",
        "context": "Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.ndim return a new tensor containing?": {
        "answer": "real values of theselftensor",
        "question": "What does Tensor.ndim return a new tensor containing?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What Returns a new tensor containing imaginary values of theselftensor?": {
        "answer": "Tensor.imag",
        "question": "What Returns a new tensor containing imaginary values of theselftensor?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What Seetorch.abs() returns a new tensor containing imaginary values of theselftensor?": {
        "answer": "Tensor.abs",
        "question": "What Seetorch.abs() returns a new tensor containing imaginary values of theselftensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What isTrueif the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "What isTrueif the Tensor is a meta tensor?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the olute of the Tensor?": {
        "answer": "In-place version ofabs() Tensor.abs",
        "question": "What is the olute of the Tensor?",
        "context": "Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What isTrueif the Tensor is?": {
        "answer": "meta tensor",
        "question": "What isTrueif the Tensor is?",
        "context": "IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the in-place version of abs()?": {
        "answer": "Alias forabs() Tensor.abs",
        "question": "What is the name of the in-place version of abs()?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is Tensor.device?": {
        "answer": "thetorch.device",
        "question": "What is Tensor.device?",
        "context": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the In-place version ofabs() Tensor.absolute?": {
        "answer": "Alias forabs",
        "question": "What is the In-place version ofabs() Tensor.absolute?",
        "context": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is the Tensor.device located?": {
        "answer": "thetorch.device",
        "question": "Where is the Tensor.device located?",
        "context": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the Alias fordim() Tensor.real Return a new tensor containing?": {
        "answer": "real values of theselftensor",
        "question": "What does the Alias fordim() Tensor.real Return a new tensor containing?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What returns a new tensor containing imaginary values of theselftensor?": {
        "answer": "Tensor.imag",
        "question": "What returns a new tensor containing imaginary values of theselftensor?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alia": {
        "answer": "Tensor.abs",
        "question": "What does Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alia",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does thetorch.device make a Tensor the first time a call?": {
        "answer": "tobackward()computes gradients forself",
        "question": "What does thetorch.device make a Tensor the first time a call?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the in-place version of a Tensor?": {
        "answer": "Alias forabs_()",
        "question": "What is the name of the in-place version of a Tensor?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is this Tensor located?": {
        "answer": "thetorch.device",
        "question": "Where is this Tensor located?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the in-place version ofabs() Tensor.absolute?": {
        "answer": "Alias forabs() Tensor",
        "question": "What is the in-place version ofabs() Tensor.absolute?",
        "context": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.grad become a Tensor the first time a call?": {
        "answer": "tobackward()computes gradients forself",
        "question": "What does Tensor.grad become a Tensor the first time a call?",
        "context": "Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the name of the name of the name of the name of the name of the name of the name of the name of the name": {
        "answer": "Tensor.addbmm",
        "question": "What is the name of the name of the name of the name of the name of the name of the name of the name of the name of the name",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does this attribute become a Tensor the first time a call?": {
        "answer": "tobackward()computes gradients forself",
        "question": "What does this attribute become a Tensor the first time a call?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What attribute becomes a Tensor the first time a call tobackward()computes gradients forself?": {
        "answer": "isNoneby",
        "question": "What attribute becomes a Tensor the first time a call tobackward()computes gradients forself?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is arccos Seetorch.arccos() Tensor.add?": {
        "answer": "In-place version ofacos() Tensor",
        "question": "What is arccos Seetorch.arccos() Tensor.add?",
        "context": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the new tensor contain?": {
        "answer": "real values of theselftensor",
        "question": "What does the new tensor contain?",
        "context": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What kind of values of theselftensor does Tensor.imag return?": {
        "answer": "imaginary",
        "question": "What kind of values of theselftensor does Tensor.imag return?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.real Returns a new tensor containing real values of theselftensor?": {
        "answer": "Alias fordim",
        "question": "Tensor.real Returns a new tensor containing real values of theselftensor?",
        "context": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the in-place version of arccos() Tensor?": {
        "answer": "In-place version ofarccos() Tensor",
        "question": "What is the name of the in-place version of arccos() Tensor?",
        "context": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Who returns a new tensor containing real values of theselftensor?": {
        "answer": "Alias fordim",
        "question": "Who returns a new tensor containing real values of theselftensor?",
        "context": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is added to the selftensor?": {
        "answer": "scalar or tensor",
        "question": "What is added to the selftensor?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What adds a scalar or tensor toselftensor?": {
        "answer": "Tensor.add_",
        "question": "What adds a scalar or tensor toselftensor?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.imag returns a new tensor containing what?": {
        "answer": "imaginary values of theselftensor",
        "question": "Tensor.imag returns a new tensor containing what?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does tensor add toselftensor?": {
        "answer": "scalar",
        "question": "What does tensor add toselftensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.imag return a new tensor containing?": {
        "answer": "imaginary values of theselftensor",
        "question": "What does Tensor.imag return a new tensor containing?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do you add to theselftensor?": {
        "answer": "scalar or tensor",
        "question": "What do you add to theselftensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is another name for Add a scalar or tensor toselftensor?": {
        "answer": "addbmm",
        "question": "What is another name for Add a scalar or tensor toselftensor?",
        "context": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Returns a new tensor containing what?": {
        "answer": "imaginary values of theselftensor",
        "question": "Returns a new tensor containing what?",
        "context": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is added to theselftensor?": {
        "answer": "scalar or tensor",
        "question": "What is added to theselftensor?",
        "context": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is in-place version ofabs() Tensor.absolute?": {
        "answer": "Alias forabs",
        "question": "What is in-place version ofabs() Tensor.absolute?",
        "context": "Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the In-place version ofaddbmm() Tensor?": {
        "answer": "addbmm",
        "question": "What is the In-place version ofaddbmm() Tensor?",
        "context": "Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.absolute stand for?": {
        "answer": "Alias forabs",
        "question": "What does Tensor.absolute stand for?",
        "context": "Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the In-place version ofadd() Tensor?": {
        "answer": "addbmm",
        "question": "What is the In-place version ofadd() Tensor?",
        "context": "Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor": {
        "answer": "addbmm",
        "question": "What does Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor",
        "context": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do you add to a Tensor?": {
        "answer": "scalar or tensor toselftensor",
        "question": "What do you add to a Tensor?",
        "context": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the In-place version of addbmm() Tensor?": {
        "answer": "addbmm",
        "question": "What is the In-place version of addbmm() Tensor?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "In what axis does a n-D tensor reverse the order of a n-D tensor?": {
        "answer": "dims",
        "question": "In what axis does a n-D tensor reverse the order of a n-D tensor?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is the name of'snp.flip'?": {
        "answer": "NumPy",
        "question": "What is the name of'snp.flip'?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What does torch.flipis expect to be?": {
        "answer": "slower thannp.flip",
        "question": "What does torch.flipis expect to be?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is a listortuple to flip on?": {
        "answer": "axis",
        "question": "What is a listortuple to flip on?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is the order of a tensor along a given axis in dims?": {
        "answer": "n-D",
        "question": "What is the order of a tensor along a given axis in dims?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "Why is copying a tensor's data slower than numPy'snp.flip?": {
        "answer": "more work",
        "question": "Why is copying a tensor's data slower than numPy'snp.flip?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What does dims(a listortuple) represent?": {
        "answer": "axis to flip on",
        "question": "What does dims(a listortuple) represent?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "Computes what of either a matrix or batch of matricesinput?": {
        "answer": "singular value decomposition",
        "question": "Computes what of either a matrix or batch of matricesinput?",
        "context": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the singular value decomposition represented as?": {
        "answer": "namedtuple(U, S, V)",
        "question": "What is the singular value decomposition represented as?",
        "context": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the transpose ofVfor?": {
        "answer": "real inputs",
        "question": "What is the transpose ofVfor?",
        "context": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Ifinputis a batch of what, thenU,S, andVare also batched with the same batch dimensions asinput?": {
        "answer": "matrices",
        "question": "Ifinputis a batch of what, thenU,S, andVare also batched with the same batch dimensions asinput?",
        "context": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What method returns the reduced singular value decomposition?": {
        "answer": "IfsomeisTrue",
        "question": "What method returns the reduced singular value decomposition?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the returnedUandVmatrices contain if the last two dimensions ofinputaremandn?": {
        "answer": "onlymin(n, m)orthonormal columns",
        "question": "What will the returnedUandVmatrices contain if the last two dimensions ofinputaremandn?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Whencompute_uvisFalse has no effect?": {
        "answer": "argumentsomehas no effect",
        "question": "Whencompute_uvisFalse has no effect?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the default value of the method that returns the reduced singular value decomposition?": {
        "answer": "IfsomeisTrue",
        "question": "What is the default value of the method that returns the reduced singular value decomposition?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "If someisTrue(default), the returnedUandVmatrices will contain what?": {
        "answer": "onlymin(n, m)orthonormal columns",
        "question": "If someisTrue(default), the returnedUandVmatrices will contain what?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Ifcompute_uvisFalse, the returnedUandVwill be what?": {
        "answer": "zero-filled matrices",
        "question": "Ifcompute_uvisFalse, the returnedUandVwill be what?",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What effect does argumentsome have whencompute_uvisFalse?": {
        "answer": "argumentsomehas no effect whencompute_uvisFalse",
        "question": "What effect does argumentsome have whencompute_uvisFalse?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the returnedUandV be ifcompute_uvisFalse?": {
        "answer": "zero-filled matrices",
        "question": "What will the returnedUandV be ifcompute_uvisFalse?",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does the argumentsome have no effect whencompute_uvisFalse?": {
        "answer": "float, double, cfloat and cdouble data types",
        "question": "What does the argumentsome have no effect whencompute_uvisFalse?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the dtypes ofUandV always be?": {
        "answer": "real-valued",
        "question": "What will the dtypes ofUandV always be?",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is deprecated in favor oftorch.linalg.svd()?": {
        "answer": "Warning torch.svd()",
        "question": "What is deprecated in favor oftorch.linalg.svd()?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What happens whencompute_uvisFalse?": {
        "answer": "argumentsomehas no effect",
        "question": "What happens whencompute_uvisFalse?",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What data type is supported by float, double, cfloat and cfloat?": {
        "answer": "cdouble",
        "question": "What data type is supported by float, double, cfloat and cfloat?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "The dtypes ofUandVare the same asinput\u2019s.Swill always be what?": {
        "answer": "real-valued",
        "question": "The dtypes ofUandVare the same asinput\u2019s.Swill always be what?",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is deprecated in favor of oftorch.linalg.svd()?": {
        "answer": "torch.svd()",
        "question": "What is deprecated in favor of oftorch.linalg.svd()?",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What are the supportsinput of?": {
        "answer": "float, double, cfloat and cdouble data types",
        "question": "What are the supportsinput of?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the dtypes ofUandVare always be?": {
        "answer": "real-valued",
        "question": "What will the dtypes ofUandVare always be?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What should U,S,V=torch.svd(A,some=some,compute_uv=True)": {
        "answer": "Note Differences",
        "question": "What should U,S,V=torch.svd(A,some=some,compute_uv=True)",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Along with cfloat and cfloat, what data type is supported by Torch?": {
        "answer": "cdouble",
        "question": "Along with cfloat and cfloat, what data type is supported by Torch?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "The dtypes ofUandVare will always be what?": {
        "answer": "real-valued",
        "question": "The dtypes ofUandVare will always be what?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What should be replaced with Note Differences?": {
        "answer": "withtorch.linalg.svd()",
        "question": "What should be replaced with Note Differences?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What should be replaced with _,S,_=torch.svd(A,some=some,compute_uv": {
        "answer": "U,S,V",
        "question": "What should be replaced with _,S,_=torch.svd(A,some=some,compute_uv",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the default value for both oftorch.linalg.svd()'sfull_matrices?": {
        "answer": "default value for both isTrue",
        "question": "What is the default value for both oftorch.linalg.svd()'sfull_matrices?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does torch.svd() return?": {
        "answer": "torch.svd()returnsV",
        "question": "What does torch.svd() return?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What should be replaced with withtorch.linalg.svd()?": {
        "answer": "Note Differences",
        "question": "What should be replaced with withtorch.linalg.svd()?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does whereastorch.linalg.svd()returnsVh?": {
        "answer": "torch.svd()returnsV",
        "question": "What does whereastorch.linalg.svd()returnsVh?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does torch.svd()return forUandVh?": {
        "answer": "zero-filled tensors",
        "question": "What does torch.svd()return forUandVh?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "In what order are the singular values returned?": {
        "answer": "descending order",
        "question": "In what order are the singular values returned?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of a batch of matrices?": {
        "answer": "Ifinputis",
        "question": "What is the name of a batch of matrices?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Note TheStensor can only be used to what?": {
        "answer": "compute gradients",
        "question": "Note TheStensor can only be used to what?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What can be used to compute gradients ifcompute_uvisTrue?": {
        "answer": "Note",
        "question": "What can be used to compute gradients ifcompute_uvisTrue?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does torch.svd() return forUandVh?": {
        "answer": "zero-filled tensors",
        "question": "What does torch.svd() return forUandVh?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of the tensor that can only be used to compute gradients ifcompute_uvisTrue?": {
        "answer": "Note",
        "question": "What is the name of the tensor that can only be used to compute gradients ifcompute_uvisTrue?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "WhensomeisFalse, the gradients onU[..., :, min(m, n):]andV[...": {
        "answer": "backward pass",
        "question": "WhensomeisFalse, the gradients onU[..., :, min(m, n):]andV[...",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What algorithm does the implementation oftorch.linalg.svd()on CPU use instead of?gesvdfor speed?": {
        "answer": "LAPACK",
        "question": "What algorithm does the implementation oftorch.linalg.svd()on CPU use instead of?gesvdfor speed?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Which CPU version of CUDA is a routinegesddon?": {
        "answer": "MAGMA",
        "question": "Which CPU version of CUDA is a routinegesddon?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of CUDA's routinegesdd?": {
        "answer": "Note",
        "question": "What is the name of CUDA's routinegesdd?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does the implementation oftorch.linalg.svd() on CPU use instead of?gesvdfor speed?": {
        "answer": "LAPACK\u2019s routine?gesdd",
        "question": "What does the implementation oftorch.linalg.svd() on CPU use instead of?gesvdfor speed?",
        "context": "The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What version of CUDA does cuSOLVER's routinesgesvdjandgesvdjBatchedon?": {
        "answer": "10.1.243",
        "question": "What version of CUDA does cuSOLVER's routinesgesvdjandgesvdjBatchedon?",
        "context": "The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What happens when a matrix is represented as a column-major matrix?": {
        "answer": "returnedUwill not be contiguous",
        "question": "What happens when a matrix is represented as a column-major matrix?",
        "context": "Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "The matrix (or batch of matrices) will be represented as what?": {
        "answer": "column-major matrix",
        "question": "The matrix (or batch of matrices) will be represented as what?",
        "context": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "The matrix (or batch of matrices) will be represented as a column-major matrix (i.e. what?": {
        "answer": "Fortran-contiguous",
        "question": "The matrix (or batch of matrices) will be represented as a column-major matrix (i.e. what?",
        "context": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of the warning that the matrix will not be contiguous?": {
        "answer": "Warning",
        "question": "What is the name of the warning that the matrix will not be contiguous?",
        "context": "The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does returnedUnot be contiguous?": {
        "answer": "returnedUwill not be contiguous",
        "question": "What does returnedUnot be contiguous?",
        "context": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the gradients with respect toUandV only be when the input does not have zero nor repeated singular values?": {
        "answer": "finite",
        "question": "What will the gradients with respect toUandV only be when the input does not have zero nor repeated singular values?",
        "context": "Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the gradients with respect toUandV be if the distance between two singular values is close to zero?": {
        "answer": "numerically unstable",
        "question": "What will the gradients with respect toUandV be if the distance between two singular values is close to zero?",
        "context": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does the matrix have?": {
        "answer": "small singular values",
        "question": "What does the matrix have?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What happens when the matrix has small singular values?": {
        "answer": "Warning",
        "question": "What happens when the matrix has small singular values?",
        "context": "The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "When will the gradients with respect toUandVonly be finite?": {
        "answer": "when the input does not have zero nor repeated singular values",
        "question": "When will the gradients with respect toUandVonly be finite?",
        "context": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be what?": {
        "answer": "numerically unstable",
        "question": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be what?",
        "context": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "When do the gradients with respect toUandVbe numerically unstable?": {
        "answer": "when the matrix has small singular values",
        "question": "When do the gradients with respect toUandVbe numerically unstable?",
        "context": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will be numerically unstable if the distance between two singular values is close to zero?": {
        "answer": "the gradients with respect toUandV",
        "question": "What will be numerically unstable if the distance between two singular values is close to zero?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "When do gradients depend onS1?": {
        "answer": "when the matrix has small singular values",
        "question": "When do gradients depend onS1?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the warning if the distance between singular values is close to zero?": {
        "answer": "Warning",
        "question": "What is the warning if the distance between singular values is close to zero?",
        "context": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What can be multiplied byUandV?": {
        "answer": "arbitrary phase factor",
        "question": "What can be multiplied byUandV?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What can be used to multiply the columns of the spanning subspace inUandV?": {
        "answer": "a rotation matrix",
        "question": "What can be used to multiply the columns of the spanning subspace inUandV?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Different platforms, like NumPy, or inputs on different device types, may produce what?": {
        "answer": "differentUandVtensors",
        "question": "Different platforms, like NumPy, or inputs on different device types, may produce what?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "For what is the singular value decomposition not unique?": {
        "answer": "complex-valuedinput",
        "question": "For what is the singular value decomposition not unique?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What happens wheninputhas repeated singular values?": {
        "answer": "wheninputhas repeated singular values",
        "question": "What happens wheninputhas repeated singular values?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What may produce differentUandVtensors?": {
        "answer": "Different platforms",
        "question": "What may produce differentUandVtensors?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is used to multiply the columns of the spanning subspace inUandV?": {
        "answer": "a rotation matrix",
        "question": "What is used to multiply the columns of the spanning subspace inUandV?",
        "context": "For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the input tensor of size(*, m, n)where*is zero or more batch dimensions consisting of(": {
        "answer": "input(Tensor)",
        "question": "What is the input tensor of size(*, m, n)where*is zero or more batch dimensions consisting of(",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What controls whether to compute the reduced or full decomposition?": {
        "answer": "some(bool,optional)",
        "question": "What controls whether to compute the reduced or full decomposition?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the default value of some(bool,optional) that controls whether to compute the reduced or full decomposition?": {
        "answer": "Default:True",
        "question": "What is the default value of some(bool,optional) that controls whether to compute the reduced or full decomposition?",
        "context": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of the function that controls whether to computeUandV?": {
        "answer": "compute_uv",
        "question": "What is the name of the function that controls whether to computeUandV?",
        "context": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the default value of compute_uv(bool,optional)?": {
        "answer": "Default:True",
        "question": "What is the default value of compute_uv(bool,optional)?",
        "context": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the output tuple of tensors?": {
        "answer": "out(tuple,optional)",
        "question": "What is the output tuple of tensors?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does SciPy do with the following cases?": {
        "answer": "Computesinput*log(other)",
        "question": "What does SciPy do with the following cases?",
        "context": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"
    },
    "Computesinput*log(other)with the following cases. Similar to what?": {
        "answer": "SciPy\u2019sscipy.special.xlogy",
        "question": "Computesinput*log(other)with the following cases. Similar to what?",
        "context": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"
    },
    "At least one of inputorothermust be what?": {
        "answer": "a tensor",
        "question": "At least one of inputorothermust be what?",
        "context": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"
    },
    "What does a DLPack do to a tensor?": {
        "answer": "Decodes a DLPack to a tensor",
        "question": "What does a DLPack do to a tensor?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is dlpack?": {
        "answer": "PyCapsule object",
        "question": "What is dlpack?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "How often can each dlpack be consumed?": {
        "answer": "once",
        "question": "How often can each dlpack be consumed?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What represents the tensor?": {
        "answer": "a DLPack",
        "question": "What represents the tensor?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is a tensor to be exported?": {
        "answer": "tensor",
        "question": "What is a tensor to be exported?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "Decodes a DLPack to what?": {
        "answer": "a tensor",
        "question": "Decodes a DLPack to what?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is a PyCapsule object with the dltensor?": {
        "answer": "dlpack",
        "question": "What is a PyCapsule object with the dltensor?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "How many times can a DLPack be consumed?": {
        "answer": "once",
        "question": "How many times can a DLPack be consumed?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is the tensor represented by?": {
        "answer": "DLPack",
        "question": "What is the tensor represented by?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What does a float tensor convert to?": {
        "answer": "per-channel quantized tensor",
        "question": "What does a float tensor convert to?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What is input to quantize scales?": {
        "answer": "float 1D tensor",
        "question": "What is input to quantize scales?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "Converts a float tensor to what?": {
        "answer": "a per-channel quantized tensor",
        "question": "Converts a float tensor to what?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What is input(Tensor) to quantize scales?": {
        "answer": "float tensor",
        "question": "What is input(Tensor) to quantize scales?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What must be used to convert a float tensor to a per-channel quantized tensor?": {
        "answer": "one of the quantized dtypes",
        "question": "What must be used to convert a float tensor to a per-channel quantized tensor?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What is input(Tensor) used to quantize scales?": {
        "answer": "float tensor",
        "question": "What is input(Tensor) used to quantize scales?",
        "context": "input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What is an example of a quantized tensor Tensor?": {
        "answer": "newly quantized tensor Tensor Example:",
        "question": "What is an example of a quantized tensor Tensor?",
        "context": "input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What enables gradient calculation?": {
        "answer": "Context-manager",
        "question": "What enables gradient calculation?",
        "context": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    "What does the context-manager do if it has been disabled viano_gradorset_grad_enabled?": {
        "answer": "Enables gradient calculation",
        "question": "What does the context-manager do if it has been disabled viano_gradorset_grad_enabled?",
        "context": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    "Why is this context manager thread local?": {
        "answer": "it will not affect computation in other threads",
        "question": "Why is this context manager thread local?",
        "context": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    "What does this context manager function as?": {
        "answer": "decorator",
        "question": "What does this context manager function as?",
        "context": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    "Make sure to instantiate with what?": {
        "answer": "parenthesis",
        "question": "Make sure to instantiate with what?",
        "context": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    "What is one of several mechanisms that can enable or disable gradients locally?": {
        "answer": "enable_grad",
        "question": "What is one of several mechanisms that can enable or disable gradients locally?",
        "context": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    "What is supported only if eigenvalues and eigenvectors are all real valued?": {
        "answer": "backward pass",
        "question": "What is supported only if eigenvalues and eigenvectors are all real valued?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Wheninputis on CUDA,torch.eig() causes what?": {
        "answer": "host-device synchronization",
        "question": "Wheninputis on CUDA,torch.eig() causes what?",
        "context": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What might be complex?": {
        "answer": "eigenvalues and eigenvectors",
        "question": "What might be complex?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "When input is on CUDA torch.eig()causes what?": {
        "answer": "host-device synchronization",
        "question": "When input is on CUDA torch.eig()causes what?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a what?": {
        "answer": "random permutation of integers from0ton-1",
        "question": "Returns a what?",
        "context": "  Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Sets the default floating point dtype tod get?": {
        "answer": "current default floating pointtorch.dtype",
        "question": "What does Sets the default floating point dtype tod get?",
        "context": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a what type of tensor of sizeendstartstepleftlceil fractextend": {
        "answer": "1-D tensor",
        "question": "Returns a what type of tensor of sizeendstartstepleftlceil fractextend",
        "context": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the current default floating pointtorch.dtype?": {
        "answer": "current default floating pointtorch.dtype",
        "question": "What is the current default floating pointtorch.dtype?",
        "context": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with what value1?": {
        "answer": "scalar",
        "question": "Returns a tensor filled with what value1?",
        "context": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a what type of tensor?": {
        "answer": "1-D tensor",
        "question": "Returns a what type of tensor?",
        "context": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a what type of tensor of sizeendstartstep+1leftlfloor fractext": {
        "answer": "1-D tensor",
        "question": "Returns a what type of tensor of sizeendstartstep+1leftlfloor fractext",
        "context": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Creation ops are listed under Random sampling and include:torch.rand()torch.rand()torch.randin": {
        "answer": "Random sampling",
        "question": "Creation ops are listed under Random sampling and include:torch.rand()torch.rand()torch.randin",
        "context": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what tensor of sizeendstartstep+1leftlfloor fractextend -": {
        "answer": "1-D tensor",
        "question": "Returns what tensor of sizeendstartstep+1leftlfloor fractextend -",
        "context": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of tensor is returned?": {
        "answer": "1-D tensor",
        "question": "What type of tensor is returned?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is created of sizestepswhose values are evenly spaced fromstarttoend inclusive?": {
        "answer": "one-dimensional tensor",
        "question": "What is created of sizestepswhose values are evenly spaced fromstarttoend inclusive?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the Random sampling creation ops?": {
        "answer": "Create",
        "question": "What is the name of the Random sampling creation ops?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is constructed in COO(rdinate) format with specified values at the givenindices?": {
        "answer": "asparse tensor",
        "question": "What is constructed in COO(rdinate) format with specified values at the givenindices?",
        "context": "Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a what size tensor of sizeendstartstep+1leftlfloor fractextend": {
        "answer": "1-D",
        "question": "Returns a what size tensor of sizeendstartstep+1leftlfloor fractextend",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive?": {
        "answer": "one-dimensional",
        "question": "What type of tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive?",
        "context": "Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of tensor of sizestepswhose values are evenly spaced frombasestarttextbasetextstartbasestart": {
        "answer": "one-dimensional",
        "question": "What type of tensor of sizestepswhose values are evenly spaced frombasestarttextbasetextstartbasestart",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a what tensor with ones on the diagonal and zeros elsewhere?": {
        "answer": "2-D",
        "question": "Returns a what tensor with ones on the diagonal and zeros elsewhere?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns an uninitialized tensor with what size asinput?": {
        "answer": "same size asinput",
        "question": "Returns an uninitialized tensor with what size asinput?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with uninitialized data. Returns an uninitialized tensor with the": {
        "answer": "Return",
        "question": "Returns a tensor filled with uninitialized data. Returns an uninitialized tensor with the",
        "context": "Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tensor of sizeendstartstep+1leftlfloor fractextend -": {
        "answer": "1-D",
        "question": "What is the tensor of sizeendstartstep+1leftlfloor fractextend -",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of tensor of sizestepswhose values are evenly spaced fromstarttoend inclusive?": {
        "answer": "one-dimensional tensor",
        "question": "What type of tensor of sizestepswhose values are evenly spaced fromstarttoend inclusive?",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what tensor with ones on the diagonal and zeros elsewhere?": {
        "answer": "2-D tensor",
        "question": "Returns what tensor with ones on the diagonal and zeros elsewhere?",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns an uninitialized tensor with what size?": {
        "answer": "same size asinput",
        "question": "Returns an uninitialized tensor with what size?",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.": {
        "answer": "Con",
        "question": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Concatenates what in the given dimension?": {
        "answer": "given sequence ofseqtensors",
        "question": "Concatenates what in the given dimension?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Splitsinput, a tensor with three or more dimensions, into what?": {
        "answer": "multiple tensors depthwise",
        "question": "Splitsinput, a tensor with three or more dimensions, into what?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Stack tensors in sequence depthwise (along what axis)?": {
        "answer": "third axis",
        "question": "Stack tensors in sequence depthwise (along what axis)?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Gathers values along an axis specified what?": {
        "answer": "bydim",
        "question": "Gathers values along an axis specified what?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Splitsinput, a tensor with one or more dimensions, into what?": {
        "answer": "multiple tensors",
        "question": "Splitsinput, a tensor with one or more dimensions, into what?",
        "context": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it mean to split a tensor into a specific number of chunks?": {
        "answer": "Concatenates the given sequence ofseqtensors in the given dimension",
        "question": "What does it mean to split a tensor into a specific number of chunks?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension. Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the difference between a tensor and a specific number of chunks?": {
        "answer": "Splits a tensor into a specific number of chunks",
        "question": "What is the difference between a tensor and a specific number of chunks?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor with three or more dimensions?": {
        "answer": "Splitsinput",
        "question": "What is a tensor with three or more dimensions?",
        "context": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How does Splitsinput create a new tensor?": {
        "answer": "horizontally stacking",
        "question": "How does Splitsinput create a new tensor?",
        "context": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Stack tensors in sequence depthwise?": {
        "answer": "Gathers values along an axis specified bydim",
        "question": "What does Stack tensors in sequence depthwise?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What axis does Splitsinput gather values along?": {
        "answer": "bydim",
        "question": "What axis does Splitsinput gather values along?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the given sequence ofseqtensors in the given dimension do?": {
        "answer": "Concatenates",
        "question": "What does the given sequence ofseqtensors in the given dimension do?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What happens when a tensor is broken into chunks?": {
        "answer": "Splits",
        "question": "What happens when a tensor is broken into chunks?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Splitsinput use to split a tensor into multiple tensors depthwise?": {
        "answer": "indices_or_sections",
        "question": "What does Splitsinput use to split a tensor into multiple tensors depthwise?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What creates a new tensor?": {
        "answer": "horizontally stacking the tensors intensors",
        "question": "What creates a new tensor?",
        "context": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How many chunks does a tensor split into?": {
        "answer": "chunks",
        "question": "How many chunks does a tensor split into?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How many dimensions does Splitsinput have?": {
        "answer": "one or more dimensions",
        "question": "How many dimensions does Splitsinput have?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that Stacks tensors in sequence depthwise?": {
        "answer": "Gathers values along an axis specified bydim",
        "question": "What is the name of the tensor that Stacks tensors in sequence depthwise?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Splits a tensor into?": {
        "answer": "a specific number of chunks",
        "question": "What does Splits a tensor into?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How many tensors does Splitsinput split into?": {
        "answer": "multiple tensors depthwise",
        "question": "How many tensors does Splitsinput split into?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor do in sequence depthwise?": {
        "answer": "Stack tensors",
        "question": "What does a tensor do in sequence depthwise?",
        "context": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the axis specified by Splitsinput?": {
        "answer": "bydim",
        "question": "What is the axis specified by Splitsinput?",
        "context": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How many tensors can a tensor with one or more dimensions be split into?": {
        "answer": "multiple tensors horizontally",
        "question": "How many tensors can a tensor with one or more dimensions be split into?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is Splitsinput?": {
        "answer": "a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections",
        "question": "What is Splitsinput?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How do tensors stack in sequence?": {
        "answer": "horizontally",
        "question": "How do tensors stack in sequence?",
        "context": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Splitsinput use to split a tensor?": {
        "answer": "indices_or_sections",
        "question": "What does Splitsinput use to split a tensor?",
        "context": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Splitsinput do to create a new tensor?": {
        "answer": "Stack tensors",
        "question": "What does Splitsinput do to create a new tensor?",
        "context": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What do tensors with one or more dimensions splitsinput into?": {
        "answer": "multiple tensors horizontally",
        "question": "What do tensors with one or more dimensions splitsinput into?",
        "context": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Stack tensors in sequence horizontally (what?": {
        "answer": "column wise",
        "question": "Stack tensors in sequence horizontally (what?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How does a tensor create a new tensor?": {
        "answer": "horizontally stacking",
        "question": "How does a tensor create a new tensor?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Stack tensors in sequence depthwise mean?": {
        "answer": "third axis",
        "question": "What does Stack tensors in sequence depthwise mean?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is acolumn wise?": {
        "answer": "Stack tensors in sequence horizontally",
        "question": "What is acolumn wise?",
        "context": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tensor that indexes theinputtensor along dimensiondimusing the entries inindex?": {
        "answer": "aLongTensor",
        "question": "What is the tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "context": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Where do tensors stack in sequence?": {
        "answer": "depthwise",
        "question": "Where do tensors stack in sequence?",
        "context": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Stack tensors do?": {
        "answer": "Gathers values along an axis specified bydim",
        "question": "What does Stack tensors do?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that splitsinput into multiple tensors horizontally?": {
        "answer": "indices_or_sections",
        "question": "What is the name of the tensor that splitsinput into multiple tensors horizontally?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Stack tensors in sequence horizontally (what way)?": {
        "answer": "column wise",
        "question": "Stack tensors in sequence horizontally (what way)?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does aLongTensor do?": {
        "answer": "indexes theinputtensor along dimensiondimusing the entries inindex",
        "question": "What does aLongTensor do?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the length of Stack tensors in sequence depthwise?": {
        "answer": "third axis",
        "question": "What is the length of Stack tensors in sequence depthwise?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Stack tensors in sequence depthwise gather?": {
        "answer": "Gathers values along an axis specified bydim",
        "question": "What does Stack tensors in sequence depthwise gather?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How do Stack tensors in sequence?": {
        "answer": "horizontally",
        "question": "How do Stack tensors in sequence?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?": {
        "answer": "aLongTensor",
        "question": "What is the new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "context": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the new tensor that indexes theinputtensor according to the boolean maskmask?": {
        "answer": "1-D",
        "question": "What is the new tensor that indexes theinputtensor according to the boolean maskmask?",
        "context": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the boolean maskmask?": {
        "answer": "aBoolTensor",
        "question": "What is the boolean maskmask?",
        "context": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that indexes theinputtensor along dimensiondimusing the entries inindex?": {
        "answer": "Alias fortorch.transpose()",
        "question": "What is the name of the tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "context": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Splitsinput do?": {
        "answer": "Gathers values along an axis specified bydim",
        "question": "What does Splitsinput do?",
        "context": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How do you stack tensors in sequence horizontally?": {
        "answer": "Stack tensors",
        "question": "How do you stack tensors in sequence horizontally?",
        "context": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of tensor is in sequence horizontally?": {
        "answer": "Stack",
        "question": "What type of tensor is in sequence horizontally?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?": {
        "answer": "aLongTensor",
        "question": "What is a new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?": {
        "answer": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination",
        "question": "What moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?",
        "context": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor with one or more dimensions do horizontally?": {
        "answer": "Stack tensors",
        "question": "What does a tensor with one or more dimensions do horizontally?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does move the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?": {
        "answer": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination",
        "question": "What does move the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?",
        "context": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is dimensiondimusing the entries inindex?": {
        "answer": "aLongTensor",
        "question": "What is dimensiondimusing the entries inindex?",
        "context": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a new tensor that indexes theinputtensor according to the boolean maskmask?": {
        "answer": "1-D",
        "question": "What is a new tensor that indexes theinputtensor according to the boolean maskmask?",
        "context": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that is a narrowed version ofinputtensor?": {
        "answer": "Alias oftorch.vstack()",
        "question": "What is the name of the tensor that is a narrowed version ofinputtensor?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of version ofinputtensor is Alias fortorch.movedim()?": {
        "answer": "narrowed",
        "question": "What type of version ofinputtensor is Alias fortorch.movedim()?",
        "context": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Stack tensors in sequence what?": {
        "answer": "horizontally",
        "question": "Stack tensors in sequence what?",
        "context": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does fortorch.movedim() do?": {
        "answer": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination",
        "question": "What does fortorch.movedim() do?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is used to move the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?": {
        "answer": "Alias fortorch.movedim()",
        "question": "What is used to move the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?",
        "context": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What version ofinputtensor is Alias fortorch.movedim()?": {
        "answer": "narrowed",
        "question": "What version ofinputtensor is Alias fortorch.movedim()?",
        "context": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the index that returns a new tensor that indexes theinputtensor along dimensiondimusing": {
        "answer": "aLongTensor",
        "question": "What is the name of the index that returns a new tensor that indexes theinputtensor along dimensiondimusing",
        "context": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new 1-D tensor which indexes theinputtensor according to what boolean maskmask": {
        "answer": "aBoolTensor",
        "question": "Returns a new 1-D tensor which indexes theinputtensor according to what boolean maskmask",
        "context": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the new tensor that indexes theinputtensor according to the boolean maskmask": {
        "answer": "1-D",
        "question": "What is the name of the new tensor that indexes theinputtensor according to the boolean maskmask",
        "context": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that indexes theinputtensor according to the boolean maskmask?": {
        "answer": "Alias oftorch.vstack()",
        "question": "What is the name of the tensor that indexes theinputtensor according to the boolean maskmask?",
        "context": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the version oftorch.Tensor.scatter_()?": {
        "answer": "Out-of-place",
        "question": "What is the name of the version oftorch.Tensor.scatter_()?",
        "context": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does move the dimension(s) ofinput?": {
        "answer": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination",
        "question": "What does move the dimension(s) ofinput?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What method returns a new tensor that is a narrowed version ofinputtensor?": {
        "answer": "Alias fortorch.movedim()",
        "question": "What method returns a new tensor that is a narrowed version ofinputtensor?",
        "context": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor with the same data and number of elements asinput, but with what?": {
        "answer": "specified shape",
        "question": "Returns a tensor with the same data and number of elements asinput, but with what?",
        "context": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that returns a tensor with the same data and number of elements as input?": {
        "answer": "Alias oftorch.vstack()",
        "question": "What is the name of the function that returns a tensor with the same data and number of elements as input?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.movedim() do?": {
        "answer": "Alias oftorch.vstack()",
        "question": "What does Alias fortorch.movedim() do?",
        "context": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor have with the same data and number of elements asinput?": {
        "answer": "the specified shape",
        "question": "What does a tensor have with the same data and number of elements asinput?",
        "context": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tensor split into?": {
        "answer": "chunks",
        "question": "What is the tensor split into?",
        "context": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that moves the dimension(s) ofinput to the position(s) indestination?": {
        "answer": "Alias fortorch.movedim()",
        "question": "What is the name of the function that moves the dimension(s) ofinput to the position(s) indestination?",
        "context": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor with the same data and number of elements as input, but with what?": {
        "answer": "specified shape",
        "question": "Returns a tensor with the same data and number of elements as input, but with what?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Out-of-place version oftorch.Tensor.scatter_add_() Splits the tensor into": {
        "answer": "chunks",
        "question": "Out-of-place version oftorch.Tensor.scatter_add_() Splits the tensor into",
        "context": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.movedim() have?": {
        "answer": "the specified shape",
        "question": "What does Alias fortorch.movedim() have?",
        "context": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor with all the dimensions ofinput?": {
        "answer": "size1removed",
        "question": "What is a tensor with all the dimensions ofinput?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What happens to a sequence of tensors?": {
        "answer": "Concatenates a sequence of tensors along a new dimension",
        "question": "What happens to a sequence of tensors?",
        "context": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What function returns a new tensor that is a narrowed version ofinputtensor?": {
        "answer": "Alias fortorch.movedim()",
        "question": "What function returns a new tensor that is a narrowed version ofinputtensor?",
        "context": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned with the same data and number of elements as input but with the specified shape?": {
        "answer": "a tensor",
        "question": "What is returned with the same data and number of elements as input but with the specified shape?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor with what?": {
        "answer": "all the dimensions ofinputof size1removed",
        "question": "Returns a tensor with what?",
        "context": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What happens to a sequence of tensors along a new dimension?": {
        "answer": "Concatenates",
        "question": "What happens to a sequence of tensors along a new dimension?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of version ofinputtensor is a new tensor?": {
        "answer": "narrowed",
        "question": "What type of version ofinputtensor is a new tensor?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a new tensor have?": {
        "answer": "arcsine",
        "question": "What does a new tensor have?",
        "context": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that converts a sequence of tensors along a new dimension?": {
        "answer": "Alias fortorch.transpose()",
        "question": "What is the name of the function that converts a sequence of tensors along a new dimension?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that returns a tensor with the same data and number of elements asinput?": {
        "answer": "Alias oftorch.vstack()",
        "question": "What is the name of the tensor that returns a tensor with the same data and number of elements asinput?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor concatenate?": {
        "answer": "a sequence of tensors along a new dimension",
        "question": "What does a tensor concatenate?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.transpose() do?": {
        "answer": "Alias fortorch.transpose()",
        "question": "What does Alias fortorch.transpose() do?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What splits the tensor into chunks?": {
        "answer": "Out-of-place version oftorch.Tensor.scatter_()",
        "question": "What splits the tensor into chunks?",
        "context": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor with what data removed?": {
        "answer": "all the dimensions",
        "question": "Returns a tensor with what data removed?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is another name for fortorch.transpose()?": {
        "answer": "Alias fortorch.transpose()",
        "question": "What is another name for fortorch.transpose()?",
        "context": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the generator object generate?": {
        "answer": "pseudo random numbers",
        "question": "What does the generator object generate?",
        "context": "  Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the generator object produce?": {
        "answer": "pseudo random numbers",
        "question": "What does the generator object produce?",
        "context": "  Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of random number is the seed for generating random numbers?": {
        "answer": "non-deterministic",
        "question": "What type of random number is the seed for generating random numbers?",
        "context": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What sets the seed for generating random numbers to a non-deterministic random number?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What sets the seed for generating random numbers to a non-deterministic random number?",
        "context": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the random number generator state?": {
        "answer": "Draws binary random numbers (0 or 1) from a Bernoulli distribution",
        "question": "What is the random number generator state?",
        "context": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does atorch.ByteTensor set?": {
        "answer": "random number generator state",
        "question": "What does atorch.ByteTensor set?",
        "context": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does atorch.ByteTensor mean?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What does atorch.ByteTensor mean?",
        "context": "Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located": {
        "answer": "Draws binary random numbers (0 or 1) from a Bernoulli distribution",
        "question": "What is a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located",
        "context": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does each row containnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ": {
        "answer": "tensor",
        "question": "What does each row containnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ",
        "context": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is atorch.ByteTensor?": {
        "answer": "random number generator state",
        "question": "What is atorch.ByteTensor?",
        "context": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns the random number generator state as what?": {
        "answer": "atorch.ByteTensor",
        "question": "Returns the random number generator state as what?",
        "context": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are given to a tensor of random numbers drawn from separate normal distributions?": {
        "answer": "mean and standard deviation",
        "question": "What are given to a tensor of random numbers drawn from separate normal distributions?",
        "context": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a what of random numbers drawn from separate normal distributions whose mean and standard deviation are given?": {
        "answer": "tensor",
        "question": "Returns a what of random numbers drawn from separate normal distributions whose mean and standard deviation are given?",
        "context": "  Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the state that draws binary random numbers from a Bernoulli distribution?": {
        "answer": "Sets the random number generator state",
        "question": "What is the name of the state that draws binary random numbers from a Bernoulli distribution?",
        "context": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the random number generator state do?": {
        "answer": "Draws binary random numbers (0 or 1) from a Bernoulli distribution",
        "question": "What does the random number generator state do?",
        "context": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a Bernoulli distribution draw?": {
        "answer": "Draws binary random numbers",
        "question": "What does a Bernoulli distribution draw?",
        "context": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What parameter is given by the corresponding element ininputi?": {
        "answer": "rate parameter given by the corresponding element ininputi",
        "question": "What parameter is given by the corresponding element ininputi?",
        "context": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor of random numbers drawn from separate normal distributions what are given?": {
        "answer": "whose mean and standard deviation",
        "question": "Returns a tensor of random numbers drawn from separate normal distributions what are given?",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "The tensor returns a tensor of the same size as input with each element sampled from what distribution?": {
        "answer": "Poisson",
        "question": "The tensor returns a tensor of the same size as input with each element sampled from what distribution?",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What distribution is each element sampled from?": {
        "answer": "Poisson",
        "question": "What distribution is each element sampled from?",
        "context": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor return with each element sampled from a Poisson distribution with rate parameter given by the corresponding element inin": {
        "answer": "a tensor of the same size asinput",
        "question": "What does a tensor return with each element sampled from a Poisson distribution with rate parameter given by the corresponding element inin",
        "context": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with what from a uniform distribution on the interval?": {
        "answer": "random numbers",
        "question": "Returns a tensor filled with what from a uniform distribution on the interval?",
        "context": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Where is a tensor filled with random numbers?": {
        "answer": "uniform distribution",
        "question": "Where is a tensor filled with random numbers?",
        "context": "Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Where is a tensor filled with random integers generated?": {
        "answer": "betweenlow(inclusive) andhigh(exclusive)",
        "question": "Where is a tensor filled with random integers generated?",
        "context": "Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1) Returns a ": {
        "answer": "a tensor",
        "question": "What is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1) Returns a ",
        "context": "Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?": {
        "answer": "exclusive",
        "question": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "context": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned with the same shape as Tensorinput?": {
        "answer": "tensor",
        "question": "What is returned with the same shape as Tensorinput?",
        "context": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor with the same size asinput filled with random numbers?": {
        "answer": "uniform distribution",
        "question": "What is a tensor with the same size asinput filled with random numbers?",
        "context": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh?": {
        "answer": "exclusive",
        "question": "What is the name of the tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh?",
        "context": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1]?": {
        "answer": "a tensor",
        "question": "What is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1]?",
        "context": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?": {
        "answer": "tensor",
        "question": "What is a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "context": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?": {
        "answer": "same shape",
        "question": "What is the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "context": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the standard normal distribution?": {
        "answer": "mean0and variance1",
        "question": "What is the standard normal distribution?",
        "context": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the same shape as Tensorinputfilled with random integers generated?": {
        "answer": "uniformly",
        "question": "What is the same shape as Tensorinputfilled with random integers generated?",
        "context": "Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a random permutation of integers from0ton-1?": {
        "answer": "mean 0 and variance 1.",
        "question": "What is a random permutation of integers from0ton-1?",
        "context": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor filled with random numbers from a normal distribution with mean0and variance1?": {
        "answer": "random permutation of integers from0ton-1",
        "question": "What is a tensor filled with random numbers from a normal distribution with mean0and variance1?",
        "context": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a random permutation of integers from0ton-1 mean?": {
        "answer": "mean 0 and variance 1.",
        "question": "What does a random permutation of integers from0ton-1 mean?",
        "context": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a random permutation of integers?": {
        "answer": "from0ton-1",
        "question": "What is a random permutation of integers?",
        "context": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are some more in-place random sampling functions defined on?": {
        "answer": "Tensors",
        "question": "What are some more in-place random sampling functions defined on?",
        "context": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is another name for a normal distribution with mean0and variance1?": {
        "answer": "standard normal distribution",
        "question": "What is another name for a normal distribution with mean0and variance1?",
        "context": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned when a tensor is filled with random numbers from a normal distribution with mean 0 and variance 1?": {
        "answer": "a tensor with the same size asinput",
        "question": "What is returned when a tensor is filled with random numbers from a normal distribution with mean 0 and variance 1?",
        "context": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are there a few more defined on Tensors?": {
        "answer": "in-place random sampling functions",
        "question": "What are there a few more defined on Tensors?",
        "context": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Where are there more in-place random sampling functions defined?": {
        "answer": "Tensors",
        "question": "Where are there more in-place random sampling functions defined?",
        "context": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of distribution does tensor.log_normal_() originate from?": {
        "answer": "log-normal distribution",
        "question": "What type of distribution does tensor.log_normal_() originate from?",
        "context": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What distribution does Tensor.log_normal_() sample from?": {
        "answer": "log-normal",
        "question": "What distribution does Tensor.log_normal_() sample from?",
        "context": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of distribution did the numbers sample from?": {
        "answer": "discrete uniform distribution",
        "question": "What type of distribution did the numbers sample from?",
        "context": "torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are numbers sampled from the discrete uniform distribution torch?": {
        "answer": "Tensor.random_()",
        "question": "What are numbers sampled from the discrete uniform distribution torch?",
        "context": "torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of distribution are the numbers sampled from?": {
        "answer": "discrete uniform distribution",
        "question": "What type of distribution are the numbers sampled from?",
        "context": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is an engine for generating (scrambled) Sobol sequences?": {
        "answer": "SobolEngine Thetorch.quasirandom",
        "question": "What is an engine for generating (scrambled) Sobol sequences?",
        "context": "quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of sequence is generated by the SobolEngine Thetorch?": {
        "answer": "quasirandom",
        "question": "What type of sequence is generated by the SobolEngine Thetorch?",
        "context": "quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is saved to a disk file?": {
        "answer": "an object",
        "question": "What is saved to a disk file?",
        "context": "  Saves an object to a disk file.   Loads an object saved withtorch.save()from a file. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Loads an object saved from a file?": {
        "answer": "withtorch.save()",
        "question": "Loads an object saved from a file?",
        "context": "  Saves an object to a disk file.   Loads an object saved withtorch.save()from a file. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it do when an object is saved to a disk file?": {
        "answer": "Saves an object to a disk file",
        "question": "What does it do when an object is saved to a disk file?",
        "context": "  Saves an object to a disk file.   Loads an object saved withtorch.save()from a file. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Loads an object saved what from a file?": {
        "answer": "withtorch.save()",
        "question": "Loads an object saved what from a file?",
        "context": "  Saves an object to a disk file.   Loads an object saved withtorch.save()from a file. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What sets the number of threads used for intraop parallelism on CPU?": {
        "answer": "the number of threads used for parallelizing CPU operations",
        "question": "What sets the number of threads used for intraop parallelism on CPU?",
        "context": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the number of threads used for inter-op parallelism on CPU?": {
        "answer": "the number of threads used for interop parallelism",
        "question": "What is the number of threads used for inter-op parallelism on CPU?",
        "context": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the number of threads used for parallelizing CPU operations?": {
        "answer": "Returns the number of threads used for parallelizing CPU operations",
        "question": "What returns the number of threads used for parallelizing CPU operations?",
        "context": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns the number of threads used for what parallelism on CPU?": {
        "answer": "inter-op",
        "question": "Returns the number of threads used for what parallelism on CPU?",
        "context": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Returns the number of threads used for inter-op parallelism on CPU?": {
        "answer": "Sets the number of threads used for interop parallelism",
        "question": "What does Returns the number of threads used for inter-op parallelism on CPU?",
        "context": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a Context-manager?": {
        "answer": "disabled gradient calculation",
        "question": "What is a Context-manager?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Context-manager enable?": {
        "answer": "gradient calculation",
        "question": "What does Context-manager enable?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Context-manager set gradient calculation to?": {
        "answer": "on or off",
        "question": "What does Context-manager set gradient calculation to?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does grad mode return if grad mode is currently enabled?": {
        "answer": "True",
        "question": "What does grad mode return if grad mode is currently enabled?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does inference mode return if inference mode is currently enabled?": {
        "answer": "True",
        "question": "What does inference mode return if inference mode is currently enabled?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the manager that disabled gradient calculation?": {
        "answer": "Context-manager",
        "question": "What is the name of the manager that disabled gradient calculation?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a Context-manager do?": {
        "answer": "enables gradient calculation",
        "question": "What does a Context-manager do?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a Context-manager set gradient calculation to?": {
        "answer": "on or off",
        "question": "What does a Context-manager set gradient calculation to?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is currently enabled by the Context-manager?": {
        "answer": "grad mode",
        "question": "What is currently enabled by the Context-manager?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What mode does a Context-manager enable or disable?": {
        "answer": "inference mode",
        "question": "What mode does a Context-manager enable or disable?",
        "context": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.abs() compute?": {
        "answer": "inverse cosine",
        "question": "What does Alias fortorch.abs() compute?",
        "context": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Who returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?": {
        "answer": "Alias fortorch.acosh()",
        "question": "Who returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?",
        "context": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.abs() return a new tensor?": {
        "answer": "inverse hyperbolic cosine",
        "question": "What does Alias fortorch.abs() return a new tensor?",
        "context": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the scalarotherto each element of inputinput return a new resulting tensor?": {
        "answer": "Alias fortorch.acosh()",
        "question": "What does the scalarotherto each element of inputinput return a new resulting tensor?",
        "context": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.acosh() do?": {
        "answer": "inverse hyperbolic cosine",
        "question": "What does Alias fortorch.acosh() do?",
        "context": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the element-wise division oftensor1bytensor2 do?": {
        "answer": "multiply the result by the scalarvalueand add it toinput",
        "question": "What does the element-wise division oftensor1bytensor2 do?",
        "context": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What computes the inverse cosine of each element ininput?": {
        "answer": "Alias fortorch.abs",
        "question": "What computes the inverse cosine of each element ininput?",
        "context": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What function returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?": {
        "answer": "Alias fortorch.acosh()",
        "question": "What function returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?",
        "context": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Alias fortorch.acos() returns a new tensor with what cosine of the elements of input?": {
        "answer": "inverse hyperbolic",
        "question": "Alias fortorch.acos() returns a new tensor with what cosine of the elements of input?",
        "context": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?": {
        "answer": "Alias fortorch.acos()",
        "question": "What returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?",
        "context": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the element-wise division performed by Alias fortorch.acosh?": {
        "answer": "oftensor1bytensor2",
        "question": "What is the element-wise division performed by Alias fortorch.acosh?",
        "context": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.abs() compute the inverse cosine of each element ininput?": {
        "answer": "Alias fortorch.acos()",
        "question": "What does Alias fortorch.abs() compute the inverse cosine of each element ininput?",
        "context": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.acosh() add to each element of inputinput?": {
        "answer": "scalarotherto",
        "question": "What does Alias fortorch.acosh() add to each element of inputinput?",
        "context": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that returns a new tensor with the inverse hyperbolic cosine of the elements of input": {
        "answer": "Alias fortorch.acosh()",
        "question": "What is the name of the function that returns a new tensor with the inverse hyperbolic cosine of the elements of input",
        "context": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the element-wise division performed by Alias fortorch.acosh()?": {
        "answer": "oftensor1bytensor2",
        "question": "What is the element-wise division performed by Alias fortorch.acosh()?",
        "context": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.acos() compute?": {
        "answer": "inverse cosine",
        "question": "What does Alias fortorch.acos() compute?",
        "context": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of cosine does Alias fortorch.acosh() return a new tensor?": {
        "answer": "hyperbolic",
        "question": "What type of cosine does Alias fortorch.acosh() return a new tensor?",
        "context": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the cosine of each element in input?": {
        "answer": "inverse",
        "question": "What is the cosine of each element in input?",
        "context": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new tensor with what cosine of the elements of input?": {
        "answer": "inverse hyperbolic",
        "question": "Returns a new tensor with what cosine of the elements of input?",
        "context": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that returns a new tensor with the inverse hyperbolic cosine of the elements ofin": {
        "answer": "Alias fortorch.acosh()",
        "question": "What is the name of the function that returns a new tensor with the inverse hyperbolic cosine of the elements ofin",
        "context": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the cosine of elements ofinput?": {
        "answer": "hyperbolic",
        "question": "What is the cosine of elements ofinput?",
        "context": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.acosh() perform?": {
        "answer": "the element-wise multiplication",
        "question": "What does Alias fortorch.acosh() perform?",
        "context": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of cosine does Alias fortorch.acos() return?": {
        "answer": "inverse hyperbolic",
        "question": "What type of cosine does Alias fortorch.acos() return?",
        "context": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the element-wise multiplication performed by Alias fortorch.acosh?": {
        "answer": "oftensor1bytensor2",
        "question": "What is the name of the element-wise multiplication performed by Alias fortorch.acosh?",
        "context": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the inverse of the elements ofinput?": {
        "answer": "hyperbolic sine",
        "question": "What is the inverse of the elements ofinput?",
        "context": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does add the scalarotherto each element of the inputinputand returns a new resulting tensor?": {
        "answer": "Alias fortorch.acosh()",
        "question": "What does add the scalarotherto each element of the inputinputand returns a new resulting tensor?",
        "context": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.acosh() add?": {
        "answer": "scalarotherto each element of the inputinput",
        "question": "What does Alias fortorch.acosh() add?",
        "context": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the scalarotherto multiply the result by the scalarvalueand add it toinput?": {
        "answer": "element-wise division",
        "question": "What does the scalarotherto multiply the result by the scalarvalueand add it toinput?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the element-wise division oftensor1bytensor2 perform?": {
        "answer": "the element-wise multiplication",
        "question": "What does the element-wise division oftensor1bytensor2 perform?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the element-wise angle of the giveninputtensor?": {
        "answer": "radians",
        "question": "What is the element-wise angle of the giveninputtensor?",
        "context": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the cosine of the elements of input?": {
        "answer": "inverse hyperbolic",
        "question": "What is the cosine of the elements of input?",
        "context": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the element-wise division?": {
        "answer": "oftensor1bytensor2",
        "question": "What is the name of the element-wise division?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the element-wise multiplication?": {
        "answer": "oftensor1bytensor2",
        "question": "What is the name of the element-wise multiplication?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes what of the given inputtensor?": {
        "answer": "element-wise angle",
        "question": "Computes what of the given inputtensor?",
        "context": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the elements ofinput?": {
        "answer": "arcsine",
        "question": "What is the name of the elements ofinput?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that adds the scalarotherto each element of the input?": {
        "answer": "Alias fortorch.acosh()",
        "question": "What is the name of the function that adds the scalarotherto each element of the input?",
        "context": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the element-wise division performed by Alias fortorch.acosh?": {
        "answer": "oftensor1bytensor2",
        "question": "What is the name of the element-wise division performed by Alias fortorch.acosh?",
        "context": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new tensor with what of the elements of input?": {
        "answer": "arcsine",
        "question": "Returns a new tensor with what of the elements of input?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does each element of inputinput return a new resulting tensor?": {
        "answer": "scalarotherto",
        "question": "What does each element of inputinput return a new resulting tensor?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Who returns a new tensor with the arcsine of the elements ofinput?": {
        "answer": "Alias fortorch.asin()",
        "question": "Who returns a new tensor with the arcsine of the elements ofinput?",
        "context": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns a new resulting tensor?": {
        "answer": "Adds the scalarotherto each element of the inputinput",
        "question": "What returns a new resulting tensor?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that returns a new tensor with the arcsine of the elements ofinput?": {
        "answer": "Alias fortorch.asin()",
        "question": "What is the name of the function that returns a new tensor with the arcsine of the elements ofinput?",
        "context": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Performs what division of oftensor1bytensor2?": {
        "answer": "element-wise division",
        "question": "Performs what division of oftensor1bytensor2?",
        "context": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the element-wise division oftensor1bytensor2 multiply the result by?": {
        "answer": "scalarvalue",
        "question": "What does the element-wise division oftensor1bytensor2 multiply the result by?",
        "context": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.asin() have?": {
        "answer": "inverse hyperbolic sine",
        "question": "What does Alias fortorch.asin() have?",
        "context": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Who returns a new tensor with the inverse hyperbolic sine of the elements ofinput?": {
        "answer": "Alias fortorch.asin()",
        "question": "Who returns a new tensor with the inverse hyperbolic sine of the elements ofinput?",
        "context": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the element-wise division?": {
        "answer": "oftensor1bytensor2",
        "question": "What is the element-wise division?",
        "context": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does oftensor1bytensor2 perform?": {
        "answer": "element-wise multiplication",
        "question": "What does oftensor1bytensor2 perform?",
        "context": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new tensor with what of the elements ofinput?": {
        "answer": "cosine",
        "question": "Returns a new tensor with what of the elements ofinput?",
        "context": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns a new tensor with the inverse hyperbolic sine of the elements ofinput?": {
        "answer": "Alias fortorch.asinh()",
        "question": "What returns a new tensor with the inverse hyperbolic sine of the elements ofinput?",
        "context": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does oftensor1bytensor2 multiply the result by?": {
        "answer": "scalarvalue",
        "question": "What does oftensor1bytensor2 multiply the result by?",
        "context": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the element-wise angle of the elements ofinput?": {
        "answer": "arcsine",
        "question": "What is the element-wise angle of the elements ofinput?",
        "context": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Who returns a new tensor with the arctangent of the elements ofinput?": {
        "answer": "Alias fortorch.asinh()",
        "question": "Who returns a new tensor with the arctangent of the elements ofinput?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.atan() have?": {
        "answer": "arctangent",
        "question": "What does Alias fortorch.atan() have?",
        "context": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the new tensor with the arctangent of the elements ofinput?": {
        "answer": "Alias fortorch.asinh()",
        "question": "What is the name of the new tensor with the arctangent of the elements ofinput?",
        "context": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the element-wise multiplication?": {
        "answer": "oftensor1bytensor2",
        "question": "What is the element-wise multiplication?",
        "context": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the element-wise angle of the given inputtensor?": {
        "answer": "radians",
        "question": "What is the element-wise angle of the given inputtensor?",
        "context": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.asin() return a new tensor with?": {
        "answer": "inverse hyperbolic sine",
        "question": "What does Alias fortorch.asin() return a new tensor with?",
        "context": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns a new tensor with the arctangent of the elements ofinput?": {
        "answer": "Alias fortorch.asinh()",
        "question": "What returns a new tensor with the arctangent of the elements ofinput?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.    ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Alias fortorch.asinh(). Returns a new tensor with what of the elements ofinput": {
        "answer": "arctangent",
        "question": "Alias fortorch.asinh(). Returns a new tensor with what of the elements ofinput",
        "context": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Who returns a new tensor with the arcsine of elements ofinput?": {
        "answer": "Alias fortorch.asin",
        "question": "Who returns a new tensor with the arcsine of elements ofinput?",
        "context": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the inverse hyperbolic sine of elements ofinput?": {
        "answer": "arctangent",
        "question": "What is the inverse hyperbolic sine of elements ofinput?",
        "context": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that returns a new tensor with the inverse hyperbolic sine of the elements of": {
        "answer": "Alias fortorch.atanh()",
        "question": "What is the name of the tensor that returns a new tensor with the inverse hyperbolic sine of the elements of",
        "context": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the sine of the elements of input?": {
        "answer": "inverse hyperbolic",
        "question": "What is the sine of the elements of input?",
        "context": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the inverse hyperbolic tangent of the elements ofinput?": {
        "answer": "arctangent",
        "question": "What is the inverse hyperbolic tangent of the elements ofinput?",
        "context": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns a new tensor with the inverse hyperbolic tangent of the elements ofinput?": {
        "answer": "Alias fortorch.atan()",
        "question": "What returns a new tensor with the inverse hyperbolic tangent of the elements ofinput?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.atan() return a new tensor with?": {
        "answer": "inverse hyperbolic tangent",
        "question": "What does Alias fortorch.atan() return a new tensor with?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.    ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the element ofinput that returns a new tensor?": {
        "answer": "arcsine",
        "question": "What is the name of the element ofinput that returns a new tensor?",
        "context": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Who returns a new tensor with the inverse hyperbolic tangent of the elements ofinput?": {
        "answer": "Alias fortorch.atan",
        "question": "Who returns a new tensor with the inverse hyperbolic tangent of the elements ofinput?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that returns a new tensor with the arctangent of the elements ofinput": {
        "answer": "Alias fortorch.atan",
        "question": "What is the name of the tensor that returns a new tensor with the arctangent of the elements ofinput",
        "context": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of sine does Alias fortorch.asin() return?": {
        "answer": "inverse hyperbolic",
        "question": "What type of sine does Alias fortorch.asin() return?",
        "context": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What element of input does Alias fortorch.asinh() return?": {
        "answer": "arctangent",
        "question": "What element of input does Alias fortorch.asinh() return?",
        "context": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the new tensor with the inverse hyperbolic tangent of the elements ofinput?": {
        "answer": "Alias fortorch.atanh",
        "question": "What is the name of the new tensor with the inverse hyperbolic tangent of the elements ofinput?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.atanh() have?": {
        "answer": "inverse hyperbolic tangent",
        "question": "What does Alias fortorch.atanh() have?",
        "context": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the element-wise arctangent ofinputi/otheritextinput_i ": {
        "answer": "Alias fortorch.atanh",
        "question": "What is the name of the element-wise arctangent ofinputi/otheritextinput_i ",
        "context": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the element-wise ofinputi/otheritextinput_i / textother": {
        "answer": "arctangent",
        "question": "What is the element-wise ofinputi/otheritextinput_i / textother",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that returns a new tensor with the inverse hyperbolic sine of the elements ofinput?": {
        "answer": "Alias fortorch.atanh()",
        "question": "What is the name of the function that returns a new tensor with the inverse hyperbolic sine of the elements ofinput?",
        "context": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "With consideration of what is the arctangent ofinputi/otheri / textother_iinput": {
        "answer": "the quadrant",
        "question": "With consideration of what is the arctangent ofinputi/otheri / textother_iinput",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes what of the given input tensor?": {
        "answer": "bitwise NOT",
        "question": "Computes what of the given input tensor?",
        "context": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new tensor with what sine of the elements of input?": {
        "answer": "inverse hyperbolic",
        "question": "Returns a new tensor with what sine of the elements of input?",
        "context": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.asinh() return a new tensor with?": {
        "answer": "arctangent",
        "question": "What does Alias fortorch.asinh() return a new tensor with?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tangent of elements ofinput?": {
        "answer": "hyperbolic",
        "question": "What is the tangent of elements ofinput?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.atanh() do?": {
        "answer": "Computes the bitwise NOT of the given input tensor",
        "question": "What does Alias fortorch.atanh() do?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.asinh() do?": {
        "answer": "Computes the bitwise OR ofinputandother",
        "question": "What does Alias fortorch.asinh() do?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns a new tensor with the arctangent of the elements of input?": {
        "answer": "Alias fortorch.asinh()",
        "question": "What returns a new tensor with the arctangent of the elements of input?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns a new tensor with the inverse hyperbolic tangent of the elements of input?": {
        "answer": "Alias fortorch.atan()",
        "question": "What returns a new tensor with the inverse hyperbolic tangent of the elements of input?",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that returns a new tensor with the inverse hyperbolic tangent of the elements of input": {
        "answer": "Alias fortorch.atanh()",
        "question": "What is the name of the function that returns a new tensor with the inverse hyperbolic tangent of the elements of input",
        "context": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the indices of the maximum value of all elements in?": {
        "answer": "theinputtensor",
        "question": "What is the indices of the maximum value of all elements in?",
        "context": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the minimum value of a flattened tensor?": {
        "answer": "the flattened tensor or along a dimension",
        "question": "What is the minimum value of a flattened tensor?",
        "context": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the maximum value of each slice of theinputtensor in the given dimension(s)dim?": {
        "answer": "the indices of the minimum value(s) of the flattened tensor or along a dimension",
        "question": "What returns the maximum value of each slice of theinputtensor in the given dimension(s)dim?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What happens if all elements ininputevaluate toTrue?": {
        "answer": "Tests",
        "question": "What happens if all elements ininputevaluate toTrue?",
        "context": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the maximum value of all elements in theinputtensor?": {
        "answer": "minimum value",
        "question": "What is the maximum value of all elements in theinputtensor?",
        "context": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the input tensor return?": {
        "answer": "unique elements",
        "question": "What does the input tensor return?",
        "context": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the indices of the maximum value of all elements in the inputtensor?": {
        "answer": "Returns the indices of the maximum value of all elements in theinputtensor",
        "question": "What returns the indices of the maximum value of all elements in the inputtensor?",
        "context": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what value of the inputtensor in the given dimension(s)dim?": {
        "answer": "the minimum value of each slice",
        "question": "Returns what value of the inputtensor in the given dimension(s)dim?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns the maximum value of all elements ininputtensor.": {
        "answer": "Tests if all elements ininputevaluate toTrue",
        "question": "Returns the maximum value of all elements ininputtensor.",
        "context": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Which element returns the maximum value of all elements ininputtensor?": {
        "answer": "input tensor",
        "question": "Which element returns the maximum value of all elements ininputtensor?",
        "context": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what value of all elements in the inputtensor?": {
        "answer": "minimum value",
        "question": "Returns what value of all elements in the inputtensor?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What returns the indices of the minimum value of each slice of theinputtensor in the given dimension(s)dim?": {
        "answer": "the minimum value of each slice of theinputtensor in the given dimension(s)dim",
        "question": "What returns the indices of the minimum value of each slice of theinputtensor in the given dimension(s)dim?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the minimum value of all elements in theinputtensor?": {
        "answer": "Returns the minimum value of all elements in theinputtensor",
        "question": "What returns the minimum value of all elements in theinputtensor?",
        "context": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Where is the minimum value of all elements in theinputtensor?": {
        "answer": "theinputtensor",
        "question": "Where is the minimum value of all elements in theinputtensor?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the return of (input-other) elements in theinputtensor?": {
        "answer": "p-norm",
        "question": "What is the return of (input-other) elements in theinputtensor?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the maximum value of each slice of the inputtensor in the given dimension(s)dim?": {
        "answer": "the indices of the minimum value(s) of the flattened tensor or along a dimension",
        "question": "What returns the maximum value of each slice of the inputtensor in the given dimension(s)dim?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it do when all elements ininputevaluate toTrue?": {
        "answer": "Tests if all elements ininputevaluate toTrue",
        "question": "What does it do when all elements ininputevaluate toTrue?",
        "context": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Which element returns the maximum value of all elements in theinputtensor?": {
        "answer": "input tensor",
        "question": "Which element returns the maximum value of all elements in theinputtensor?",
        "context": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the minimum value of all elements in the inputtensor?": {
        "answer": "Returns the minimum value of all elements in theinputtensor",
        "question": "What returns the minimum value of all elements in the inputtensor?",
        "context": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns the what of (input-other)?": {
        "answer": "p-norm",
        "question": "Returns the what of (input-other)?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the minimum value of each slice of theinputtensor in the given dimension(s)dim?": {
        "answer": "the minimum value of each slice of theinputtensor in the given dimension(s)dim",
        "question": "What returns the minimum value of each slice of theinputtensor in the given dimension(s)dim?",
        "context": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the return of the maximum value of each slice of theinputtensor in the given dimension(s)dim?": {
        "answer": "the minimum value of each slice of theinputtensor in the given dimension(s)dim",
        "question": "What is the return of the maximum value of each slice of theinputtensor in the given dimension(s)dim?",
        "context": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the input tensor test if all elements ininputevaluate to?": {
        "answer": "Tests if all elements ininputevaluate toTrue",
        "question": "What does the input tensor test if all elements ininputevaluate to?",
        "context": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the maximum value of all elements in theinputtensor?": {
        "answer": "Returns the maximum value of all elements in theinputtensor",
        "question": "What returns the maximum value of all elements in theinputtensor?",
        "context": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the log of summed exponentials of each row of theinputtensor in the given dimensiondim?": {
        "answer": "the log of summed exponentials",
        "question": "What is the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what value of each slice of theinputtensor in the given dimension(s)dim?": {
        "answer": "minimum value",
        "question": "Returns what value of each slice of theinputtensor in the given dimension(s)dim?",
        "context": "  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned for each row of the inputtensor in the given dimensiondim?": {
        "answer": "the p-norm of (input-other) Returns the log of summed exponentials",
        "question": "What is returned for each row of the inputtensor in the given dimensiondim?",
        "context": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the minimum value of each slice of theinputtensor in the given dimension(s)dim?": {
        "answer": "the flattened tensor or along a dimension",
        "question": "What is the minimum value of each slice of theinputtensor in the given dimension(s)dim?",
        "context": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the minimum value of all elements in theinputtensor?": {
        "answer": "maximum value",
        "question": "What is the minimum value of all elements in theinputtensor?",
        "context": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the minimum value of all elements in?": {
        "answer": "theinputtensor",
        "question": "What is the minimum value of all elements in?",
        "context": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the value of all elements in theinputtensor?": {
        "answer": "mean value",
        "question": "What is the value of all elements in theinputtensor?",
        "context": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what of each row of the inputtensor in the given dimensiondim?": {
        "answer": "the p-norm of (input-other) Returns the log of summed exponentials",
        "question": "Returns what of each row of the inputtensor in the given dimensiondim?",
        "context": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How do all elements ininputevaluate toTrue?": {
        "answer": "Tests",
        "question": "How do all elements ininputevaluate toTrue?",
        "context": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the p-norm of (input-other) return?": {
        "answer": "the log of summed exponentials of each row of theinputtensor in the given dimensiondim",
        "question": "What does the p-norm of (input-other) return?",
        "context": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the mean value of all elements in theinputtensor?": {
        "answer": "median",
        "question": "What is the mean value of all elements in theinputtensor?",
        "context": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does ignoringNaNvalues mean?": {
        "answer": "the median of the values ininput",
        "question": "What does ignoringNaNvalues mean?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it test if all elements ininputevaluate toTrue?": {
        "answer": "Tests if all elements ininputevaluate toTrue",
        "question": "What does it test if all elements ininputevaluate toTrue?",
        "context": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is used to test if all elements ininputevaluate toTrue?": {
        "answer": "input tensor",
        "question": "What is used to test if all elements ininputevaluate toTrue?",
        "context": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what value of the values ininput?": {
        "answer": "median",
        "question": "Returns what value of the values ininput?",
        "context": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what value ignoringNaNvalues?": {
        "answer": "the median of the values ininput",
        "question": "Returns what value ignoringNaNvalues?",
        "context": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does theinputtensor return?": {
        "answer": "the product of all elements in theinputtensor",
        "question": "What does theinputtensor return?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the input tensor ignore?": {
        "answer": "the median of the values ininput",
        "question": "What does the input tensor ignore?",
        "context": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the maximum value of all elements in the inputtensor?": {
        "answer": "input tensor",
        "question": "What returns the maximum value of all elements in the inputtensor?",
        "context": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the inputtensor return?": {
        "answer": "the p-norm of (input-other)",
        "question": "What does the inputtensor return?",
        "context": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the input tensor return ignoringNaNvalues?": {
        "answer": "the median of the values ininput",
        "question": "What does the input tensor return ignoringNaNvalues?",
        "context": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what value of all elements in theinputtensor?": {
        "answer": "mean value",
        "question": "Returns what value of all elements in theinputtensor?",
        "context": "  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns the median of the values ininput, ignoring what?": {
        "answer": "NaNvalues",
        "question": "Returns the median of the values ininput, ignoring what?",
        "context": "Returns the maximum value of all elements in theinputtensor. Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim?": {
        "answer": "the p-norm of (input-other) Returns the log of summed exponentials",
        "question": "What returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returdtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the return of the median of the values ininput?": {
        "answer": "the mean value of all elements in theinputtensor",
        "question": "What is the return of the median of the values ininput?",
        "context": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the mode value of each row of theinputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the mode value of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what of the values ininput, ignoringNaNvalues?": {
        "answer": "the median",
        "question": "Returns what of the values ininput, ignoringNaNvalues?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the mode value of each row of theinputtensor in the given dimensiondim?": {
        "answer": "namedtuple",
        "question": "What returns the mode value of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the mode value of a given tensor?": {
        "answer": "matrix norm or vector norm",
        "question": "What is the mode value of a given tensor?",
        "context": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does return the log of summed exponentials of each row of theinputtensor in the given dimensiondim?": {
        "answer": "the p-norm",
        "question": "What does return the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what of the values ininput?": {
        "answer": "median",
        "question": "Returns what of the values ininput?",
        "context": "  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what of a given tensor?": {
        "answer": "matrix norm or vector norm",
        "question": "Returns what of a given tensor?",
        "context": "  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the return of the log of summed exponentials of each row of theinputtensor in the given dimensiondim?": {
        "answer": "the mean value of all elements in theinputtensor",
        "question": "What is the return of the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what of each row of theinputtensor in the given dimensiondim?": {
        "answer": "the log of summed exponentials",
        "question": "Returns what of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what value in the inputtensor?": {
        "answer": "mean value of all elements",
        "question": "Returns what value in the inputtensor?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the mean value of all elements in?": {
        "answer": "theinputtensor",
        "question": "What is the mean value of all elements in?",
        "context": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does ignoringNaNvalues mean value in theinputtensor?": {
        "answer": "the median of the values ininput",
        "question": "What does ignoringNaNvalues mean value in theinputtensor?",
        "context": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of a value in a given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the name of a value in a given dimensiondim?",
        "context": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a given tensor return?": {
        "answer": "matrix norm or vector norm",
        "question": "What does a given tensor return?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Not a Numbers (NaNs) is treated as what?": {
        "answer": "zero",
        "question": "Not a Numbers (NaNs) is treated as what?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does not a Numbers return?": {
        "answer": "the product of all elements in theinputtensor",
        "question": "What does not a Numbers return?",
        "context": "Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the mode value of each row of the inputtensor in the given dimensiondim?": {
        "answer": "namedtuple",
        "question": "What returns the mode value of each row of the inputtensor in the given dimensiondim?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the return of the values ininput?": {
        "answer": "median",
        "question": "What is the return of the values ininput?",
        "context": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of a given tensor?": {
        "answer": "matrix norm or vector norm",
        "question": "What is the name of a given tensor?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the quantile of each row of theinputtensor?": {
        "answer": "q-th",
        "question": "What is the quantile of each row of theinputtensor?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns the median of the values ininput, what?": {
        "answer": "ignoringNaNvalues",
        "question": "Returns the median of the values ininput, what?",
        "context": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns the what of a given tensor?": {
        "answer": "matrix norm or vector norm",
        "question": "Returns the what of a given tensor?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are the quantiles of each row of theinputtensor along the dimensiondim?": {
        "answer": "q-th",
        "question": "What are the quantiles of each row of theinputtensor along the dimensiondim?",
        "context": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "In what order does the indices sort a tensor along a given dimension?": {
        "answer": "ascending order by value",
        "question": "In what order does the indices sort a tensor along a given dimension?",
        "context": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Computes element-wise equality?": {
        "answer": "Trueif two tensors have the same size and elements",
        "question": "What does Computes element-wise equality?",
        "context": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of a tensor?": {
        "answer": "Computesinput",
        "question": "What is the name of a tensor?",
        "context": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the name of the tensor?": {
        "answer": "Alias fortorch.gt()",
        "question": "What is the name of the name of the tensor?",
        "context": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How do two tensors have the same size and elements?": {
        "answer": "Trueif two tensors have the same size and elements",
        "question": "How do two tensors have the same size and elements?",
        "context": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that has the same size and elements?": {
        "answer": "Alias fortorch.ge()",
        "question": "What is the name of the tensor that has the same size and elements?",
        "context": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.ge() use to return a new tensor with boolean elements representing if": {
        "answer": "Computesinput",
        "question": "What does Alias fortorch.ge() use to return a new tensor with boolean elements representing if",
        "context": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Who returns a new tensor with boolean elements representing if each element isfiniteor not?": {
        "answer": "Alias fortorch.gt()",
        "question": "Who returns a new tensor with boolean elements representing if each element isfiniteor not?",
        "context": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What represents if each element ofinputis close to the corresponding element ofother?": {
        "answer": "boolean elements",
        "question": "What represents if each element ofinputis close to the corresponding element ofother?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of a new tensor with boolean elements representing if each element isfiniteor not?": {
        "answer": "Alias fortorch.le()",
        "question": "What is the name of a new tensor with boolean elements representing if each element isfiniteor not?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of Alias fortorch.ge()?": {
        "answer": "Computesinput",
        "question": "What is the name of Alias fortorch.ge()?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a new tensor with boolean elements representing if each element ofinputis close to the corresponding element of": {
        "answer": "Alias fortorch.gt()",
        "question": "What is a new tensor with boolean elements representing if each element ofinputis close to the corresponding element of",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a test if each element ofinputis positive or negative infinity?": {
        "answer": "infinite",
        "question": "What is a test if each element ofinputis positive or negative infinity?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the test if each element ofinputis infinite?": {
        "answer": "positive infinity",
        "question": "What is the test if each element ofinputis infinite?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of input does Alias fortorch.gt() use?": {
        "answer": "Computesinput",
        "question": "What type of input does Alias fortorch.gt() use?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a new tensor with boolean elements representing if each element isfiniteor not?": {
        "answer": "Alias fortorch.gt()",
        "question": "What is a new tensor with boolean elements representing if each element isfiniteor not?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding": {
        "answer": "Alias fortorch.gt()",
        "question": "What is a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding",
        "context": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What represents if each element ofinputis \u201cclose\u201d to the corresponding element ofother?": {
        "answer": "boolean elements",
        "question": "What represents if each element ofinputis \u201cclose\u201d to the corresponding element ofother?",
        "context": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the test if each element ofinputis infinite or not?": {
        "answer": "positive infinity",
        "question": "What is the test if each element ofinputis infinite or not?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What test does each element ofinputis negative infinity or not?": {
        "answer": "Tests",
        "question": "What test does each element ofinputis negative infinity or not?",
        "context": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the function that returns a new tensor with boolean elements?": {
        "answer": "Alias fortorch.gt()",
        "question": "What is the name of the function that returns a new tensor with boolean elements?",
        "context": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a new tensor with boolean elements represent?": {
        "answer": "if each element ofinputis \u201cclose\u201d to the corresponding element ofother",
        "question": "What does a new tensor with boolean elements represent?",
        "context": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Is each element ofinput positive or negative infinity or not?": {
        "answer": "infinite",
        "question": "Is each element ofinput positive or negative infinity or not?",
        "context": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Tests if each element ofinputis what?": {
        "answer": "infinite",
        "question": "Tests if each element ofinputis what?",
        "context": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Tests if each element ofinputis what infinity or not?": {
        "answer": "negative",
        "question": "Tests if each element ofinputis what infinity or not?",
        "context": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a positive or negative infinity test?": {
        "answer": "infinite",
        "question": "What is a positive or negative infinity test?",
        "context": "Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a new tensor with boolean elements represent if each element ofinputis negative infinity or not?": {
        "answer": "Tests",
        "question": "What does a new tensor with boolean elements represent if each element ofinputis negative infinity or not?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What represents if each element ofinputis NaN or not?": {
        "answer": "boolean elements",
        "question": "What represents if each element ofinputis NaN or not?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What represents if each element ofinputis real-valued or not?": {
        "answer": "boolean elements",
        "question": "What represents if each element ofinputis real-valued or not?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new tensor with what elements?": {
        "answer": "boolean",
        "question": "Returns a new tensor with what elements?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the value of each element ofinput?": {
        "answer": "infinite",
        "question": "What is the value of each element ofinput?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Tests if each element ofinputis what or not?": {
        "answer": "positive infinity",
        "question": "Tests if each element ofinputis what or not?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the infinity of each element ofinput?": {
        "answer": "negative",
        "question": "What is the infinity of each element ofinput?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new tensor with boolean elements representing if each element ofinputis what?": {
        "answer": "NaN",
        "question": "Returns a new tensor with boolean elements representing if each element ofinputis what?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new tensor with boolean elements representing if each element ofinput is what?": {
        "answer": "real-valued",
        "question": "Returns a new tensor with boolean elements representing if each element ofinput is what?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a new tensor with boolean elements represent if each element ofinputis real-valued or not?": {
        "answer": "Tests",
        "question": "What does a new tensor with boolean elements represent if each element ofinputis real-valued or not?",
        "context": "Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the value of each element of input?": {
        "answer": "infinite",
        "question": "What is the value of each element of input?",
        "context": "Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a new tensor with what elements representing if each element ofinputis NaN or not?": {
        "answer": "boolean",
        "question": "Returns a new tensor with what elements representing if each element ofinputis NaN or not?",
        "context": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Tests if each element ofinputis is what?": {
        "answer": "positive infinity",
        "question": "Tests if each element ofinputis is what?",
        "context": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the test if each element ofinputis positive infinity or not?": {
        "answer": "negative infinity",
        "question": "What is the test if each element ofinputis positive infinity or not?",
        "context": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is thekth smallest element of each row of theinputtensor in the given dimensiondim?": {
        "answer": "a namedtuple(values,indices)",
        "question": "What is thekth smallest element of each row of theinputtensor in the given dimensiondim?",
        "context": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor wherevaluesis thekth smallest element of each row of the inputtensor in the": {
        "answer": "namedtuple",
        "question": "What is the name of the tensor wherevaluesis thekth smallest element of each row of the inputtensor in the",
        "context": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What happens if each element ofinputis negative infinity or not?": {
        "answer": "Tests",
        "question": "What happens if each element ofinputis negative infinity or not?",
        "context": "Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a namedtuple?": {
        "answer": "thekth smallest element of each row of theinputtensor in the given dimensiondim",
        "question": "What is a namedtuple?",
        "context": "Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the tensor that returns a new tensor with boolean elements representing if each element of": {
        "answer": "Alias fortorch.gt()",
        "question": "What is the name of the tensor that returns a new tensor with boolean elements representing if each element of",
        "context": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is STFT?": {
        "answer": "Short-time Fourier transform",
        "question": "What is STFT?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the short time Fourier Transform?": {
        "answer": "Inverse",
        "question": "What is the name of the short time Fourier Transform?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the window function?": {
        "answer": "Bartlett",
        "question": "What is the name of the window function?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of window function is Hamming window function?": {
        "answer": "Blackman",
        "question": "What type of window function is Hamming window function?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the blackman window function?": {
        "answer": "Hamming",
        "question": "What is the name of the blackman window function?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the window function that computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta?": {
        "answer": "Hann",
        "question": "What is the name of the window function that computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the Kaiser window have?": {
        "answer": "window lengthwindow_lengthand shape parameterbeta",
        "question": "What does the Kaiser window have?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is another name for Short-time Fourier transform?": {
        "answer": "Inverse short time Fourier Transform",
        "question": "What is another name for Short-time Fourier transform?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Which window function computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta?": {
        "answer": "Hann",
        "question": "Which window function computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta?",
        "context": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a view of each input tensor with zero dimensions?": {
        "answer": "3-dimensional",
        "question": "What is a view of each input tensor with zero dimensions?",
        "context": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the view of each input tensor with zero dimensions?": {
        "answer": "3-dimensional",
        "question": "What is the view of each input tensor with zero dimensions?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the count of each value in an array of non-negative ints?": {
        "answer": "frequency",
        "question": "What is the count of each value in an array of non-negative ints?",
        "context": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a block diagonal matrix from provided tensors?": {
        "answer": "Create a block diagonal matrix",
        "question": "What is a block diagonal matrix from provided tensors?",
        "context": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Broadcasts the given tensors according to what?": {
        "answer": "Broadcasting semantics",
        "question": "Broadcasts the given tensors according to what?",
        "context": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Broadcastsinputto what?": {
        "answer": "shapeshape",
        "question": "Broadcastsinputto what?",
        "context": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Broadcastsinputto the shapeshape is similar tobroadcast_tensors() but for what?": {
        "answer": "shapes",
        "question": "Broadcastsinputto the shapeshape is similar tobroadcast_tensors() but for what?",
        "context": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How many dimensions does a 3-dimensional view of each input tensor have?": {
        "answer": "zero",
        "question": "How many dimensions does a 3-dimensional view of each input tensor have?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Count each value in an array of non-negative ints?": {
        "answer": "frequency",
        "question": "What does Count each value in an array of non-negative ints?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of a block diagonal matrix from provided tensors?": {
        "answer": "Create a block diagonal matrix",
        "question": "What is the name of a block diagonal matrix from provided tensors?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the same asbroadcast_tensors()?": {
        "answer": "shapes",
        "question": "What is the same asbroadcast_tensors()?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are the indices of the buckets to which each value in theinputbelongs?": {
        "answer": "the boundaries of the buckets are set byboundaries",
        "question": "What are the indices of the buckets to which each value in theinputbelongs?",
        "context": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What doesbroadcast_tensors() look for?": {
        "answer": "shapes",
        "question": "What doesbroadcast_tensors() look for?",
        "context": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the given sequence of tensors?": {
        "answer": "Do cartesian product",
        "question": "What is the name of the given sequence of tensors?",
        "context": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What distance does a cartesian product batched between each pair of two collections of row vectors?": {
        "answer": "p-norm distance",
        "question": "What distance does a cartesian product batched between each pair of two collections of row vectors?",
        "context": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a cartesian product of the given sequence of tensors return?": {
        "answer": "copy ofinput",
        "question": "What does a cartesian product of the given sequence of tensors return?",
        "context": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the input to the shapeshape?": {
        "answer": "Broadcastsinput",
        "question": "What is the name of the input to the shapeshape?",
        "context": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Broadcastsinputto what shape?": {
        "answer": "shapes",
        "question": "Broadcastsinputto what shape?",
        "context": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Broadcastsinput return?": {
        "answer": "the indices of the buckets",
        "question": "What does Broadcastsinput return?",
        "context": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned by Broadcastsinput to the shapeshape?": {
        "answer": "Do cartesian product of the given sequence of tensors",
        "question": "What is returned by Broadcastsinput to the shapeshape?",
        "context": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes batched what?": {
        "answer": "p-norm distance between each pair of the two collections of row vectors",
        "question": "Computes batched what?",
        "context": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned when the p-norm distance between each pair of row vectors is computed?": {
        "answer": "a copy ofinput",
        "question": "What is returned when the p-norm distance between each pair of row vectors is computed?",
        "context": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it do to return a copy ofinput?": {
        "answer": "Compute combinations of lengthrrrof the given tensor",
        "question": "What does it do to return a copy ofinput?",
        "context": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the product of the given sequence of tensors?": {
        "answer": "Do cartesian",
        "question": "What is the product of the given sequence of tensors?",
        "context": "Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) ofinputandother.   Computes the histogram of a tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the function that returns the cross product of vectors in dimensiondimofinputandother?": {
        "answer": "Compute combinations of lengthrrrof the given tensor",
        "question": "What is the function that returns the cross product of vectors in dimensiondimofinputandother?",
        "context": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does dimensiondimofinputandother return?": {
        "answer": "the cross product of vectors",
        "question": "What does dimensiondimofinputandother return?",
        "context": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the cumulative maximum of elements ofinputin the dimensiondim?": {
        "answer": "namedtuple",
        "question": "What is the cumulative maximum of elements ofinputin the dimensiondim?",
        "context": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the cumulative maximum of elements ofinputin the dimensiondim?": {
        "answer": "a namedtuple(values,indices)",
        "question": "What returns the cumulative maximum of elements ofinputin the dimensiondim?",
        "context": "Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) ofinputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) ofinputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements ofinputin the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the cumulative minimum of elements ofinputin the dimensiondim?": {
        "answer": "a namedtuple(values,indices)",
        "question": "What returns the cumulative minimum of elements ofinputin the dimensiondim?",
        "context": "Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) ofinputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) ofinputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements ofinputin the dimensiondim.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned when a namedtuple(values,indices) returns the cumulative minimum of elements ofinputin the dimensiondim": {
        "answer": "cumulative product of elements ofinputin the dimensiondim",
        "question": "What is returned when a namedtuple(values,indices) returns the cumulative minimum of elements ofinputin the dimensiondim",
        "context": "Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Returns the cumulative product of elements ofinputin the dimensiondim?": {
        "answer": "the cumulative sum of elements ofinputin the dimensiondim",
        "question": "What does Returns the cumulative product of elements ofinputin the dimensiondim?",
        "context": "Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the cumulative minimum of elements ofinputin the dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the cumulative minimum of elements ofinputin the dimensiondim?",
        "context": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the cumulative sum of elements ofinputin the dimensiondim?": {
        "answer": "cumulative product",
        "question": "What is the cumulative sum of elements ofinputin the dimensiondim?",
        "context": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a namedtuple return?": {
        "answer": "cumulative sum of elements ofinputin the dimensiondim",
        "question": "What does a namedtuple return?",
        "context": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a reduced matrix-matrix product of matrices stored inbatch1andbatch2?": {
        "answer": "add step",
        "question": "What is a reduced matrix-matrix product of matrices stored inbatch1andbatch2?",
        "context": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the batch matrix-matrix product of matrices stored inbatch1andbatch2 perform?": {
        "answer": "matrix multiplication",
        "question": "What does the batch matrix-matrix product of matrices stored inbatch1andbatch2 perform?",
        "context": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the vectorvec perform?": {
        "answer": "matrix-vector product",
        "question": "What does the vectorvec perform?",
        "context": "Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the matrix-vector product perform?": {
        "answer": "batch matrix-matrix product of matrices inbatch1andbatch2",
        "question": "What does the matrix-vector product perform?",
        "context": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the matrices inbatch1andbatch2?": {
        "answer": "batch matrix-matrix product",
        "question": "What is the name of the matrices inbatch1andbatch2?",
        "context": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What happens to all matrix multiplications along the first dimension?": {
        "answer": "all matrix multiplications get accumulated along the first dimension",
        "question": "What happens to all matrix multiplications along the first dimension?",
        "context": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the matrix multiplication of matrices perform?": {
        "answer": "matricesmat1andmat2",
        "question": "What does the matrix multiplication of matrices perform?",
        "context": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What product of the matrixmatand the vectorvec?": {
        "answer": "matrix-vector product",
        "question": "What product of the matrixmatand the vectorvec?",
        "context": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What happens to the outer-product of vectorsvec1andvec2?": {
        "answer": "adds it to the matrixinput",
        "question": "What happens to the outer-product of vectorsvec1andvec2?",
        "context": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the matrices that performs a batch matrix-matrix product?": {
        "answer": "inbatch1andbatch2",
        "question": "What is the name of the matrices that performs a batch matrix-matrix product?",
        "context": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the matricesmat1andmat2 perform?": {
        "answer": "matrix multiplication",
        "question": "What does the matricesmat1andmat2 perform?",
        "context": "Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the matrix-vector product of vectorsvec1andvec2?": {
        "answer": "outer-product",
        "question": "What is the matrix-vector product of vectorsvec1andvec2?",
        "context": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the batch matrix-matrix product of matrices stored ininputandmat2?": {
        "answer": "Alias",
        "question": "What is the name of the batch matrix-matrix product of matrices stored ininputandmat2?",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a batch matrix-matrix product of matrices stored?": {
        "answer": "ininputandmat2",
        "question": "What is a batch matrix-matrix product of matrices stored?",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the matrix product of?": {
        "answer": "NNN2-D",
        "question": "What is the matrix product of?",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?": {
        "answer": "Cholesky",
        "question": "What decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?",
        "context": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the outer-product of?": {
        "answer": "vectorsvec1andvec2",
        "question": "What is the outer-product of?",
        "context": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a batch matrix-matrix product of matrices inbatch1andbatch2 perform?": {
        "answer": "batch matrix-matrix product of matrices inbatch1andbatch2",
        "question": "What does a batch matrix-matrix product of matrices inbatch1andbatch2 perform?",
        "context": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?": {
        "answer": "Cholesky",
        "question": "What is the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?": {
        "answer": "Cholesky factor matrixuuu",
        "question": "What is the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?",
        "context": " Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuuu": {
        "answer": "Solves",
        "question": "What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuuu",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is used to perform a batch matrix-matrix product?": {
        "answer": "matrices stored ininputandmat2",
        "question": "What is used to perform a batch matrix-matrix product?",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What returns the matrix product of theNNN2-D tensors?": {
        "answer": "Returns the matrix product of theNNN2-D tensors",
        "question": "What returns the matrix product of theNNN2-D tensors?",
        "context": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices": {
        "answer": "Cholesky",
        "question": "Computes the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes the inverse of a symmetric positive-definite matrixAAAusing what?": {
        "answer": "Cholesky factoruuu",
        "question": "Computes the inverse of a symmetric positive-definite matrixAAAusing what?",
        "context": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu(). ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Solves what with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?": {
        "answer": "a linear system of equations",
        "question": "Solves what with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?",
        "context": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What matrix product does the Cholesky decomposition of a symmetric positive-definite matrixAAAor return?": {
        "answer": "NNN2-D tensors",
        "question": "What matrix product does the Cholesky decomposition of a symmetric positive-definite matrixAAAor return?",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?": {
        "answer": "Solves",
        "question": "What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?",
        "context": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?": {
        "answer": "dot product of two 1D tensors",
        "question": "What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what of theNNN2-D tensors?": {
        "answer": "matrix product",
        "question": "Returns what of theNNN2-D tensors?",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu return?": {
        "answer": "matrixinv",
        "question": "What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu return?",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the Cholesky decomposition of for batches of symmetric positive-definite matrices?": {
        "answer": "symmetric positive-definite matrixAAAor",
        "question": "What does the Cholesky decomposition of for batches of symmetric positive-definite matrices?",
        "context": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the inverse of a symmetric positive-definite matrixAAAor?": {
        "answer": "returns matrixinv",
        "question": "What is the inverse of a symmetric positive-definite matrixAAAor?",
        "context": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the Cholesky factor matrixuuu?": {
        "answer": "dot product of two 1D tensors",
        "question": "What is the Cholesky factor matrixuuu?",
        "context": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a real square matrix?": {
        "answer": "eigenvalues and eigenvectors",
        "question": "What is a real square matrix?",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned by Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?": {
        "answer": "matrixinv",
        "question": "What is returned by Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?",
        "context": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes what of a real square matrix?": {
        "answer": "eigenvalues and eigenvectors",
        "question": "Computes what of a real square matrix?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does AAAusing's Cholesky factor matrixuuu compute?": {
        "answer": "dot product of two 1D tensors",
        "question": "What does AAAusing's Cholesky factor matrixuuu compute?",
        "context": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a real square matrix have?": {
        "answer": "eigenvalues and eigenvectors",
        "question": "What does a real square matrix have?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a low-level function for calling directly?": {
        "answer": "LAPACK\u2019s geqrf",
        "question": "What is a low-level function for calling directly?",
        "context": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a low-level function for calling LAPACK's geqrf directly?": {
        "answer": "Alias oftorch.outer()",
        "question": "What is a low-level function for calling LAPACK's geqrf directly?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias oftorch.outer() compute the dot product for?": {
        "answer": "1D tensors",
        "question": "What does Alias oftorch.outer() compute the dot product for?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the matrixuuu of a positive semidefinite matrix?": {
        "answer": "Cholesky factor",
        "question": "What is the matrixuuu of a positive semidefinite matrix?",
        "context": " Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the dot product of two 1D tensors?": {
        "answer": "dot product of two 1D tensors",
        "question": "What is the dot product of two 1D tensors?",
        "context": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is this function for calling LAPACK's geqrf directly?": {
        "answer": "low-level function",
        "question": "What is this function for calling LAPACK's geqrf directly?",
        "context": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the low-level function for calling LAPACK's geqrf directly?": {
        "answer": "Alias oftorch.outer()",
        "question": "What is the low-level function for calling LAPACK's geqrf directly?",
        "context": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes the dot product for what?": {
        "answer": "1D tensors",
        "question": "Computes the dot product for what?",
        "context": "Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a positive semidefinite matrix to be inverted?": {
        "answer": "Cholesky factor matrixuuu",
        "question": "What is a positive semidefinite matrix to be inverted?",
        "context": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes what dot product of two 1D tensors?": {
        "answer": "dot product of two 1D tensors",
        "question": "Computes what dot product of two 1D tensors?",
        "context": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes the eigenvalues and eigenvectors of what?": {
        "answer": "real square matrix",
        "question": "Computes the eigenvalues and eigenvectors of what?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a low-level function for calling LAPACK's geqrf?": {
        "answer": "Alias oftorch.outer()",
        "question": "What is a low-level function for calling LAPACK's geqrf?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of Alias fortorch.linalg.det()?": {
        "answer": "Alias fortorch.linalg.inv",
        "question": "What is the name of Alias fortorch.linalg.det()?",
        "context": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "A linear system of equations with a positive semidefinite matrix to be inverted given what matrix?": {
        "answer": "Cholesky factor matrix",
        "question": "A linear system of equations with a positive semidefinite matrix to be inverted given what matrix?",
        "context": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the low-level function for calling LAPACK\u2019s geqrf directly?": {
        "answer": "Alias oftorch.outer()",
        "question": "What is the low-level function for calling LAPACK\u2019s geqrf directly?",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a low-level function for calling LAPACK\u2019s geqrf directly?": {
        "answer": "Computes the dot product of two 1D tensors",
        "question": "What is a low-level function for calling LAPACK\u2019s geqrf directly?",
        "context": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is this function for calling LAPACK\u2019s geqrf directly?": {
        "answer": "low-level function",
        "question": "What is this function for calling LAPACK\u2019s geqrf directly?",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.linalg.det() call?": {
        "answer": "Alias fortorch.linalg.inv()",
        "question": "What does Alias fortorch.linalg.det() call?",
        "context": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes the eigenvalues and eigenvectors of a real square matrix?": {
        "answer": "dot product",
        "question": "Computes the eigenvalues and eigenvectors of a real square matrix?",
        "context": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are the numbers of a real square matrix?": {
        "answer": "eigenvalues and eigenvectors",
        "question": "What are the numbers of a real square matrix?",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does this function do for calling LAPACK's geqrf directly?": {
        "answer": "low-level function",
        "question": "What does this function do for calling LAPACK's geqrf directly?",
        "context": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias fortorch.linalg.det() calculate?": {
        "answer": "log determinant of a square matrix or batches of square matrices",
        "question": "What does Alias fortorch.linalg.det() calculate?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Who calculates log determinant of a square matrix or batches of square matrices?": {
        "answer": "Alias fortorch.linalg.slogdet",
        "question": "Who calculates log determinant of a square matrix or batches of square matrices?",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What calculates the log determinant of a square matrix or batches of square matrices?": {
        "answer": "Alias fortorch.linalg.slogdet()",
        "question": "What calculates the log determinant of a square matrix or batches of square matrices?",
        "context": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the eigenvalues and eigenvectors of a real square matrix?": {
        "answer": "low-level function",
        "question": "What is the eigenvalues and eigenvectors of a real square matrix?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes the dot product for 1D tensors?": {
        "answer": "Alias oftorch.outer()",
        "question": "Computes the dot product for 1D tensors?",
        "context": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Alias oftorch.outer() do?": {
        "answer": "Computes the dot product for 1D tensors",
        "question": "What does Alias oftorch.outer() do?",
        "context": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What problems does Alias fortorch.linalg.slogdet() solve?": {
        "answer": "least squares and least norm problems",
        "question": "What problems does Alias fortorch.linalg.slogdet() solve?",
        "context": "  Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What computes the dot product for 1D tensors?": {
        "answer": "Alias oftorch.outer()",
        "question": "What computes the dot product for 1D tensors?",
        "context": "  Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Computes what of a matrix or batches of matricesA?": {
        "answer": "LU factorization",
        "question": "Computes what of a matrix or batches of matricesA?",
        "context": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tensor filled with?": {
        "answer": "scalar value0",
        "question": "What is the tensor filled with?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "As of 0.4, this function does not support what?": {
        "answer": "anoutkeyword",
        "question": "As of 0.4, this function does not support what?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is equivalent totorch.zeros?": {
        "answer": "oldtorch.zeros_like",
        "question": "What is equivalent totorch.zeros?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the size ofinput that determines the size of the output tensor?": {
        "answer": "input(Tensor)",
        "question": "What is the size ofinput that determines the size of the output tensor?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What returns a tensor filled with the scalar value0?": {
        "answer": "a tensor filled with the scalar value0",
        "question": "What returns a tensor filled with the scalar value0?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is equivalent to totorch.zeros(input.size(),out=output)?": {
        "answer": "oldtorch.zeros_like",
        "question": "What is equivalent to totorch.zeros(input.size(),out=output)?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What determines the size of the output tensor?": {
        "answer": "input(Tensor)",
        "question": "What determines the size of the output tensor?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "Warning As of 0.4, this function does not support what?": {
        "answer": "anoutkeyword",
        "question": "Warning As of 0.4, this function does not support what?",
        "context": "Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the oldtorch.zeros_like equivalent?": {
        "answer": "totorch.zeros",
        "question": "What is the oldtorch.zeros_like equivalent?",
        "context": "As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the size ofinput?": {
        "answer": "input(Tensor)",
        "question": "What is the size ofinput?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the default to the dtype ofinput?": {
        "answer": "ifNone",
        "question": "What is the default to the dtype ofinput?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the desired layout of tensor?": {
        "answer": "layout",
        "question": "What is the desired layout of tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the default to the layout ofinput?": {
        "answer": "ifNone",
        "question": "What is the default to the layout ofinput?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "As of what date does this function not support anoutkeyword?": {
        "answer": "0.4",
        "question": "As of what date does this function not support anoutkeyword?",
        "context": "As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is equivalent totorch.zeros(input.size(),out=output)?": {
        "answer": "oldtorch.zeros_like",
        "question": "What is equivalent totorch.zeros(input.size(),out=output)?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What defaults to the dtype of input?": {
        "answer": "ifNone",
        "question": "What defaults to the dtype of input?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the desired layout of returned tensor?": {
        "answer": "layout",
        "question": "What is the desired layout of returned tensor?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What defaults to the layout of input?": {
        "answer": "ifNone",
        "question": "What defaults to the layout of input?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the desired device of returned tensor?": {
        "answer": "device(torch.device, optional)",
        "question": "What is the desired device of returned tensor?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the default to the device ofinput?": {
        "answer": "ifNone",
        "question": "What is the default to the device ofinput?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What determines size of output tensor?": {
        "answer": "input(Tensor)",
        "question": "What determines size of output tensor?",
        "context": "input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What does ifNone default to?": {
        "answer": "device ofinput",
        "question": "What does ifNone default to?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What should record operations on the returned tensor?": {
        "answer": "autograd",
        "question": "What should record operations on the returned tensor?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the default for a tensor?": {
        "answer": "False",
        "question": "What is the default for a tensor?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What defaults to the device of input?": {
        "answer": "ifNone",
        "question": "What defaults to the device of input?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the default setting for autograd to record operations on the returned tensor?": {
        "answer": "False",
        "question": "What is the default setting for autograd to record operations on the returned tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    "What is the name of the object that gets the current device of a Generator?": {
        "answer": "torch.Generator object",
        "question": "What is the name of the object that gets the current device of a Generator?",
        "context": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "Generator.device -> device Gets what of the generator?": {
        "answer": "current device",
        "question": "Generator.device -> device Gets what of the generator?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does atorch.Generator object do?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What does atorch.Generator object do?",
        "context": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What are remapped to positive values with the formula0xfff_fff_fff_fff + seed?": {
        "answer": "Negative inputs",
        "question": "What are remapped to positive values with the formula0xfff_fff_fff_fff + seed?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is an example of a generator that gets a non-deterministic random number from std::random_device?": {
        "answer": "torch.Generator object",
        "question": "What is an example of a generator that gets a non-deterministic random number from std::random_device?",
        "context": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does a generator get a non-deterministic random number from?": {
        "answer": "std::random_device or the current time",
        "question": "What does a generator get a non-deterministic random number from?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does the generator do?": {
        "answer": "Sets the Generator state",
        "question": "What does the generator do?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What object is used to seed a Generator?": {
        "answer": "torch.Generator",
        "question": "What object is used to seed a Generator?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does it do when a generator is seeded?": {
        "answer": "Sets the Generator state",
        "question": "What does it do when a generator is seeded?",
        "context": "Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does generator.device -> device get?": {
        "answer": "current device",
        "question": "What does generator.device -> device get?",
        "context": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What object is used to get a non-deterministic random number from std::random_device or the current time?": {
        "answer": "torch.Generator",
        "question": "What object is used to get a non-deterministic random number from std::random_device or the current time?",
        "context": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does a Generator get a non-deterministic random number from?": {
        "answer": "std::random_device or the current time",
        "question": "What does a Generator get a non-deterministic random number from?",
        "context": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the desired state?": {
        "answer": "new_state",
        "question": "What is the desired state?",
        "context": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no": {
        "answer": "non_blocking(bool)",
        "question": "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does this storage cast to?": {
        "answer": "float type",
        "question": "What type of storage does this storage cast to?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Everytorch.Tensorcasts this storage to what type of type?": {
        "answer": "bfloat16",
        "question": "Everytorch.Tensorcasts this storage to what type of type?",
        "context": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does everytorch.Tensorcast?": {
        "answer": "float type",
        "question": "What type of storage does everytorch.Tensorcast?",
        "context": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Casts this storage to what type Casts this storage to bool type Casts this storage to bool type Casts this storage to ": {
        "answer": "bfloat16",
        "question": "Casts this storage to what type Casts this storage to bool type Casts this storage to bool type Casts this storage to ",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Casts this storage to bool type Casts this storage to char type Returns?": {
        "answer": "a copy",
        "question": "What type of storage does Casts this storage to bool type Casts this storage to char type Returns?",
        "context": "Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfsharedisTrue, what happens?": {
        "answer": "memory is shared between all processes",
        "question": "IfsharedisTrue, what happens?",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfsharedisTrue, then memory is shared between all processes.": {
        "answer": "All changes are written to the file",
        "question": "IfsharedisTrue, then memory is shared between all processes.",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does not affect the file?": {
        "answer": "IfsharedisFalse",
        "question": "What type of storage does not affect the file?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does Casts this storage to char type Return?": {
        "answer": "a copy",
        "question": "What does Casts this storage to char type Return?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does Casts this storage to char type return?": {
        "answer": "a copy",
        "question": "What does Casts this storage to char type return?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Where is a copy of this storage stored?": {
        "answer": "CUDA memory",
        "question": "Where is a copy of this storage stored?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfTrue and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no": {
        "answer": "non_blocking(bool)",
        "question": "IfTrue and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of memory is shared between all processes?": {
        "answer": "IfsharedisTrue",
        "question": "What type of memory is shared between all processes?",
        "context": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does the changes on the storage not affect the file?": {
        "answer": "IfsharedisFalse",
        "question": "What type of storage does the changes on the storage not affect the file?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the function that returns a copy of the object in CUDA memory?": {
        "answer": "non_blocking(bool)",
        "question": "What is the name of the function that returns a copy of the object in CUDA memory?",
        "context": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in what memory and on the correct device, then no copy is performed and the original object is returned?": {
        "answer": "CUDA memory",
        "question": "If the object is already in what memory and on the correct device, then no copy is performed and the original object is returned?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of argument has no effect?": {
        "answer": "non_blocking",
        "question": "What type of argument has no effect?",
        "context": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the argument that asynchronous with respect to the host?": {
        "answer": "non_blocking(bool)",
        "question": "What is the name of the argument that asynchronous with respect to the host?",
        "context": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens if sharedisTrue?": {
        "answer": "memory is shared between all processes",
        "question": "What happens if sharedisTrue?",
        "context": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the changes on the storage do not affect the file, what is the name of the file that must contain at leastsize * sizeof(Type)": {
        "answer": "IfsharedisFalse",
        "question": "If the changes on the storage do not affect the file, what is the name of the file that must contain at leastsize * sizeof(Type)",
        "context": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the file must contain at leastsize * sizeof(Type)bytes (Typeis the type of storage)?": {
        "answer": "IfsharedisFalse",
        "question": "If the file must contain at leastsize * sizeof(Type)bytes (Typeis the type of storage)?",
        "context": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does filename(str) mean?": {
        "answer": "filename(str) \u2013 file name to map shared",
        "question": "What does filename(str) mean?",
        "context": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does it do to the storage if it's not already pinned?": {
        "answer": "Moves the storage to shared memory",
        "question": "What does it do to the storage if it's not already pinned?",
        "context": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Casts this storage to?": {
        "answer": "float",
        "question": "What type of storage does Casts this storage to?",
        "context": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does the changes on the storage do not affect the file?": {
        "answer": "IfsharedisFalse",
        "question": "What type of storage does the changes on the storage do not affect the file?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Returns a list containing the elements of what storage?": {
        "answer": "self Casts this storage to short type",
        "question": "Returns a list containing the elements of what storage?",
        "context": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the file that must contain at leastsize * sizeof(Type)bytes?": {
        "answer": "IfsharedisFalse",
        "question": "What is the name of the file that must contain at leastsize * sizeof(Type)bytes?",
        "context": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What Casts this storage to short type Returns a list containing the elements of this storage?": {
        "answer": "self",
        "question": "What Casts this storage to short type Returns a list containing the elements of this storage?",
        "context": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. Ion GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "When do gradients with respect toUandV depend onS1?": {
        "answer": "when the matrix has small singular values",
        "question": "When do gradients with respect toUandV depend onS1?",
        "context": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What may be multiplied by for complex-valuedinput?": {
        "answer": "arbitrary phase factor",
        "question": "What may be multiplied by for complex-valuedinput?",
        "context": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What happens when inputhas multiple singular values?": {
        "answer": "repeated singular values",
        "question": "What happens when inputhas multiple singular values?",
        "context": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What may be multiplied byUandV?": {
        "answer": "arbitrary phase factor",
        "question": "What may be multiplied byUandV?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "When is the singular value decomposition not unique?": {
        "answer": "wheninputhas repeated singular values",
        "question": "When is the singular value decomposition not unique?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What setting controls whether to compute the reduced or full decomposition, and consequently, the shape of returnedUandV?": {
        "answer": "Default:True",
        "question": "What setting controls whether to compute the reduced or full decomposition, and consequently, the shape of returnedUandV?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What controls whether to computeUandV?": {
        "answer": "compute_uv",
        "question": "What controls whether to computeUandV?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the default value for compute_uv?": {
        "answer": "Default:True",
        "question": "What is the default value for compute_uv?",
        "context": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is a float tensor converted to?": {
        "answer": "quantized tensor",
        "question": "What is a float tensor converted to?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What has to be the desired data type of returned tensor?": {
        "answer": "one of the quantized dtypes",
        "question": "What has to be the desired data type of returned tensor?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "When inputis on CUDA,torch.eig()causes what?": {
        "answer": "host-device synchronization",
        "question": "When inputis on CUDA,torch.eig()causes what?",
        "context": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Where is each row an eigenvalue of input?": {
        "answer": "the first element is the real part and the second element is the imaginary part",
        "question": "Where is each row an eigenvalue of input?",
        "context": "input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Ifeigenvectors=False, what is it?": {
        "answer": "empty tensor",
        "question": "Ifeigenvectors=False, what is it?",
        "context": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What can shape(nn)(n times n)(nn) be used to compute?": {
        "answer": "normalized (unit length) eigenvectors",
        "question": "What can shape(nn)(n times n)(nn) be used to compute?",
        "context": "Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "If the correspondingeigenvalues[j]is a what?": {
        "answer": "real number",
        "question": "If the correspondingeigenvalues[j]is a what?",
        "context": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "If the correspondingeigenvalues[j]andeigenvalues[j + 1]form what?": {
        "answer": "a complex conjugate pair",
        "question": "If the correspondingeigenvalues[j]andeigenvalues[j + 1]form what?",
        "context": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What could a map-style dataset read from a folder on the disk?": {
        "answer": "theidx-th image and its corresponding label",
        "question": "What could a map-style dataset read from a folder on the disk?",
        "context": "At the heart of PyTorch data loading utility is thetorch.utils.data.DataLoaderclass.  It represents a Python iterable over a dataset, with support for map-style and iterable-style datasets, customizing data loading order, automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Who configures the options for map-style and iterable-style datasets?": {
        "answer": "the constructor arguments of aDataLoader",
        "question": "Who configures the options for map-style and iterable-style datasets?",
        "context": "map-style and iterable-style datasets, customizing data loading order, automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time). ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of a dataset that could read theidx-th image and its corresponding label from a folder on the disk?": {
        "answer": "SeeDataset",
        "question": "What is the name of a dataset that could read theidx-th image and its corresponding label from a folder on the disk?",
        "context": "For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Automatic batching, single- and multi-process data loading, and what other option are configured by the constructor arguments of aDataLoa": {
        "answer": "automatic memory pinning",
        "question": "Automatic batching, single- and multi-process data loading, and what other option are configured by the constructor arguments of aDataLoa",
        "context": "automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "How are the options configured?": {
        "answer": "the constructor arguments of aDataLoader",
        "question": "How are the options configured?",
        "context": "single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is an instance of a subclass ofIterableDataset that implements the__iter__()protocol?": {
        "answer": "iterable-style dataset",
        "question": "What is an instance of a subclass ofIterableDataset that implements the__iter__()protocol?",
        "context": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What are iterable-style datasets particularly suitable for?": {
        "answer": "cases where random reads are expensive or even improbable",
        "question": "What are iterable-style datasets particularly suitable for?",
        "context": "An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of a map-style dataset that could read theidx-th image and its corresponding label from a folder on": {
        "answer": "SeeDataset",
        "question": "What is the name of a map-style dataset that could read theidx-th image and its corresponding label from a folder on",
        "context": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "When using anIterableDatasetwith what?": {
        "answer": "multi-process data loading",
        "question": "When using anIterableDatasetwith what?",
        "context": "For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the dataset that implements the__iter__()protocol?": {
        "answer": "SeeDataset",
        "question": "What is the name of the dataset that implements the__iter__()protocol?",
        "context": "SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is anIterableDatasetwith?": {
        "answer": "multi-process data loading",
        "question": "What is anIterableDatasetwith?",
        "context": "The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time). ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What document describes how to avoid duplicated data when using multi-process data loading?": {
        "answer": "SeeIterableDatasetdocumentations",
        "question": "What document describes how to avoid duplicated data when using multi-process data loading?",
        "context": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "How is the data loading order controlled by the user-defined iterable?": {
        "answer": "yielding a batched sample at each time",
        "question": "How is the data loading order controlled by the user-defined iterable?",
        "context": "The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time). ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What are sampleclasses used to do?": {
        "answer": "represent iterable objects over the indices to datasets",
        "question": "What are sampleclasses used to do?",
        "context": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What can aSampler do in the common case with stochastic gradient decent?": {
        "answer": "randomly permute a list of indices and yield each one at a time",
        "question": "What can aSampler do in the common case with stochastic gradient decent?",
        "context": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "How will a sequential or shuffled sampler be constructed?": {
        "answer": "based on theshuffleargument to aDataLoader",
        "question": "How will a sequential or shuffled sampler be constructed?",
        "context": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Who supports automatically collating individual fetched data samples into batches?": {
        "answer": "DataLoader",
        "question": "Who supports automatically collating individual fetched data samples into batches?",
        "context": "A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What are batched samples?": {
        "answer": "containing Tensors",
        "question": "What are batched samples?",
        "context": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What yields batched samples instead of individual samples whenbatch_size(default1) is notNone?": {
        "answer": "data loader",
        "question": "What yields batched samples instead of individual samples whenbatch_size(default1) is notNone?",
        "context": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What type of datasets can users alternatively specifybatch_sampler?": {
        "answer": "map-style datasets",
        "question": "What type of datasets can users alternatively specifybatch_sampler?",
        "context": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "In some cases, users may want to handle batching what?": {
        "answer": "manually in dataset code",
        "question": "In some cases, users may want to handle batching what?",
        "context": "and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What type of data would it be cheaper to load manually?": {
        "answer": "batched data",
        "question": "What type of data would it be cheaper to load manually?",
        "context": "and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Who returns each member of thedatasetobject?": {
        "answer": "the data loader",
        "question": "Who returns each member of thedatasetobject?",
        "context": "After fetching a list of samples using the indices from sampler, the function\npassed as thecollate_fnargument is used to collate lists of samples\ninto batches. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "In certain cases, users may want to handle batching what?": {
        "answer": "manually",
        "question": "In certain cases, users may want to handle batching what?",
        "context": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is used to collate samples?": {
        "answer": "automatic batching",
        "question": "What is used to collate samples?",
        "context": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is disabled when bothbatch_sizeandbatch_samplerareNone(default value forbatch_samplerareN": {
        "answer": "automatic batching",
        "question": "What is disabled when bothbatch_sizeandbatch_samplerareNone(default value forbatch_samplerareN",
        "context": "In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Each sample obtained from thedatasetis processed with the function passed as what?": {
        "answer": "thecollate_fnargument",
        "question": "Each sample obtained from thedatasetis processed with the function passed as what?",
        "context": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What could be cheaper to handle batching manually in dataset code?": {
        "answer": "directly load batched data",
        "question": "What could be cheaper to handle batching manually in dataset code?",
        "context": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the function passed to each sample obtained from thedataset?": {
        "answer": "thecollate_fnargument",
        "question": "What is the function passed to each sample obtained from thedataset?",
        "context": "When fetching fromiterable-style datasetswithmulti-processing, thedrop_lastargument drops the last non-full batch of each worker\u2019s dataset replica. After fetching a list of samples using the indices from sampler, the function\npassed as thecollate_fnargument is used to collate lists of samples\ninto batches. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does the defaultcollate_fnsimply do when automatic batching is disabled?": {
        "answer": "defaultcollate_fnsimply converts NumPy arrays into PyTorch Tensors",
        "question": "What does the defaultcollate_fnsimply do when automatic batching is disabled?",
        "context": "When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is roughly equivalent to loading from when automatic batching is disabled?": {
        "answer": "map-style dataset",
        "question": "What is roughly equivalent to loading from when automatic batching is disabled?",
        "context": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "When automatic batching is disabled,collate_fnis called with what?": {
        "answer": "each individual data sample",
        "question": "When automatic batching is disabled,collate_fnis called with what?",
        "context": "When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is expected when automatic batching is enabled?": {
        "answer": "to collate the input samples into a batch",
        "question": "What is expected when automatic batching is enabled?",
        "context": "In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. For instance, if each data sample consists of a 3-channel image and an integral\nclass label, i.e., each element of the dataset returns a tuple(image,class_index), the defaultcollate_fncollates a list of\nsuch tuples into a single tuple of a batched image tensor and a batched class\nlabel Tensor. In particular, the defaultcollate_fnhas the following\nproperties: It always prepends a new dimension as the batch dimension.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "How many subprocesses to use for data loading?": {
        "answer": "how many subprocesses to use for data loading",
        "question": "How many subprocesses to use for data loading?",
        "context": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the main process that will be loaded in the main process?": {
        "answer": "default:0",
        "question": "What is the name of the main process that will be loaded in the main process?",
        "context": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What can be used to define the strategy to draw samples from the dataset?": {
        "answer": "anyIterablewith__len__implemented",
        "question": "What can be used to define the strategy to draw samples from the dataset?",
        "context": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What must not be specified if specified?": {
        "answer": "shuffle",
        "question": "What must not be specified if specified?",
        "context": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of how many subprocesses to use for data loading?": {
        "answer": "num_workers",
        "question": "What is the name of how many subprocesses to use for data loading?",
        "context": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the default value for data loading?": {
        "answer": "default:0",
        "question": "What is the default value for data loading?",
        "context": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the sampler that likesampler, but returns a batch of indices at a time?": {
        "answer": "batch_sampler",
        "question": "What is the name of the sampler that likesampler, but returns a batch of indices at a time?",
        "context": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the mutually exclusive feature of batch_sampler?": {
        "answer": "withbatch_size,shuffle,sampler, anddrop_last",
        "question": "What is the mutually exclusive feature of batch_sampler?",
        "context": "batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does collate_fn merge a list of samples to form?": {
        "answer": "a mini-batch of Tensor",
        "question": "What does collate_fn merge a list of samples to form?",
        "context": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is used when using a map-style dataset?": {
        "answer": "batched loading",
        "question": "What is used when using a map-style dataset?",
        "context": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is num_workers?": {
        "answer": "total number of workers",
        "question": "What is num_workers?",
        "context": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. When called in a worker, this returns an object guaranteed to have the\nfollowing attributes: id: the current worker id. num_workers: the total number of workers. seed: the random seed set for the current worker. This value is\ndetermined by main process RNG and the worker id. SeeDataLoader\u2019s documentation for more details. dataset: the copy of the dataset object inthisprocess. Note\nthat this will be a different object in a different process than the one\nin the main process. When called in the main process, this returnsNone. Note When used in aworker_init_fnpassed over toDataLoader, this method can be useful to\nset up each worker process differently, for instance, usingworker_idto configure thedatasetobject to only read a specific fraction of a\nsharded dataset, or useseedto seed other libraries used in dataset\ncode. Randomly split a dataset into non-overlapping new datasets of given lengths.\nOptionally fix the generator for reproducible results, e.g.: dataset(Dataset) \u2013 Dataset to be split ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What merges a list of samples to form a mini-batch of Tensor(s)?": {
        "answer": "collate_fn",
        "question": "What merges a list of samples to form a mini-batch of Tensor(s)?",
        "context": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What type of dataset is collate_fn used when using batched loading?": {
        "answer": "map-style dataset",
        "question": "What type of dataset is collate_fn used when using batched loading?",
        "context": "batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "When using batched loading from what type of dataset?": {
        "answer": "map-style dataset",
        "question": "When using batched loading from what type of dataset?",
        "context": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What merges a list of samples to form a mini-batch of Tensors?": {
        "answer": "collate_fn",
        "question": "What merges a list of samples to form a mini-batch of Tensors?",
        "context": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is collate_fn used when using?": {
        "answer": "batched loading",
        "question": "What is collate_fn used when using?",
        "context": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is used when using batched loading from a map-style dataset?": {
        "answer": "pin_memory",
        "question": "What is used when using batched loading from a map-style dataset?",
        "context": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "If your data elements are what?": {
        "answer": "a custom type",
        "question": "If your data elements are what?",
        "context": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What pinned memory does the data loader copy Tensors into?": {
        "answer": "CUDA",
        "question": "What pinned memory does the data loader copy Tensors into?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What type of data elements does yourcollate_fn return a batch that is a custom type?": {
        "answer": "a custom type",
        "question": "What type of data elements does yourcollate_fn return a batch that is a custom type?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the data loader that will copy Tensors into CUDA pinned memory before returning them?": {
        "answer": "pin_memory",
        "question": "What is the name of the data loader that will copy Tensors into CUDA pinned memory before returning them?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "If your data elements are a what type?": {
        "answer": "custom type",
        "question": "If your data elements are a what type?",
        "context": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is bool,optional?": {
        "answer": "pin_memory",
        "question": "What is bool,optional?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the size of the dataset not divisible by the batch size?": {
        "answer": "IfFalseand the size of dataset is not divisible by the batch size",
        "question": "What is the size of the dataset not divisible by the batch size?",
        "context": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the name of a batch that is not divisible by the batch size?": {
        "answer": "default:False",
        "question": "What is the name of the name of a batch that is not divisible by the batch size?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What type of data elements does yourcollate_fnreturns a batch that is a custom type?": {
        "answer": "a custom type",
        "question": "What type of data elements does yourcollate_fnreturns a batch that is a custom type?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is set toTrue to drop the last incomplete batch if the dataset size is not divisible by the batch size?": {
        "answer": "drop_last",
        "question": "What is set toTrue to drop the last incomplete batch if the dataset size is not divisible by the batch size?",
        "context": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "If the size of dataset is not divisible by the batch size, what happens?": {
        "answer": "the last batch will be smaller",
        "question": "If the size of dataset is not divisible by the batch size, what happens?",
        "context": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the default setting for the size of a dataset?": {
        "answer": "default:False",
        "question": "What is the default setting for the size of a dataset?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is set toTrueto drop the last incomplete batch?": {
        "answer": "drop_last",
        "question": "What is set toTrueto drop the last incomplete batch?",
        "context": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the timeout value for collecting a batch from workers?": {
        "answer": "timeout",
        "question": "What is the timeout value for collecting a batch from workers?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What should the timeout value for collecting a batch from workers always be?": {
        "answer": "non-negative",
        "question": "What should the timeout value for collecting a batch from workers always be?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the default timeout value for collecting a batch from workers?": {
        "answer": "0",
        "question": "What is the default timeout value for collecting a batch from workers?",
        "context": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What will be smaller if the dataset size is not divisible by the batch size?": {
        "answer": "the last batch",
        "question": "What will be smaller if the dataset size is not divisible by the batch size?",
        "context": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the default value for worker_init_fn?": {
        "answer": "default:None",
        "question": "What is the default value for worker_init_fn?",
        "context": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What will be called on each worker subprocess with the worker id (an int in[0,num_workers-1]) as": {
        "answer": "worker_init_fn",
        "question": "What will be called on each worker subprocess with the worker id (an int in[0,num_workers-1]) as",
        "context": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "If notNone, worker_init_fn(callable,optional) will be called on each worker subprocess with what input?": {
        "answer": "worker id",
        "question": "If notNone, worker_init_fn(callable,optional) will be called on each worker subprocess with what input?",
        "context": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Who will use this RNG to generate random indexes?": {
        "answer": "RandomSampler",
        "question": "Who will use this RNG to generate random indexes?",
        "context": "timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the default value of generator(torch.Generator,optional)?": {
        "answer": "default:None",
        "question": "What is the default value of generator(torch.Generator,optional)?",
        "context": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Who will use generator to generate random indexes and multiprocessing to generatebase_seedfor workers?": {
        "answer": "RandomSampler",
        "question": "Who will use generator to generate random indexes and multiprocessing to generatebase_seedfor workers?",
        "context": "generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is int,optional,keyword-only arg?": {
        "answer": "prefetch_factor",
        "question": "What is int,optional,keyword-only arg?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "How many num_workers samples will be prefetched across all workers?": {
        "answer": "2",
        "question": "How many num_workers samples will be prefetched across all workers?",
        "context": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "How many prefetch_factors are there?": {
        "answer": "2",
        "question": "How many prefetch_factors are there?",
        "context": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does the data loader not shutdown the worker processes after a dataset has been consumed once?": {
        "answer": "IfTrue",
        "question": "What does the data loader not shutdown the worker processes after a dataset has been consumed once?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does the data loader allow to keep alive after a dataset has been consumed once?": {
        "answer": "Datasetinstances",
        "question": "What does the data loader allow to keep alive after a dataset has been consumed once?",
        "context": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the word \"False\"?": {
        "answer": "Warning",
        "question": "What is the name of the word \"False\"?",
        "context": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the term for persistent_workers?": {
        "answer": "IfTrue",
        "question": "What is the term for persistent_workers?",
        "context": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is a worker_init_fncannot be an unpicklable object?": {
        "answer": "lambda function",
        "question": "What is a worker_init_fncannot be an unpicklable object?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Where is Multiprocessing best practiceson more details related to multiprocessing?": {
        "answer": "PyTorch",
        "question": "Where is Multiprocessing best practiceson more details related to multiprocessing?",
        "context": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does PyTorch have to do with multiprocessing best practices?": {
        "answer": "Warning",
        "question": "What does PyTorch have to do with multiprocessing best practices?",
        "context": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does persistent_workers allow to keep alive?": {
        "answer": "workersDatasetinstances",
        "question": "What does persistent_workers allow to keep alive?",
        "context": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Worker_init_fncannot be an unpicklable object, e.g., a lambda function.": {
        "answer": "thespawnstart method",
        "question": "Worker_init_fncannot be an unpicklable object, e.g., a lambda function.",
        "context": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Where can you find more details about multiprocessing?": {
        "answer": "PyTorch",
        "question": "Where can you find more details about multiprocessing?",
        "context": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does PyTorch do when it comes to multiprocessing?": {
        "answer": "Warning",
        "question": "What does PyTorch do when it comes to multiprocessing?",
        "context": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is an example of a function that is not unpicklable?": {
        "answer": "lambda function",
        "question": "What is an example of a function that is not unpicklable?",
        "context": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Where is the Multiprocessing best practiceson more details related to multiprocessing?": {
        "answer": "PyTorch",
        "question": "Where is the Multiprocessing best practiceson more details related to multiprocessing?",
        "context": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is an example of a spawnstart method?": {
        "answer": "Warning",
        "question": "What is an example of a spawnstart method?",
        "context": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Worker_init_fncannot be an unpicklable object, e.g., a lambda function?": {
        "answer": "thespawnstart method",
        "question": "Worker_init_fncannot be an unpicklable object, e.g., a lambda function?",
        "context": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Where can you find more information about multiprocessing?": {
        "answer": "PyTorch",
        "question": "Where can you find more information about multiprocessing?",
        "context": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does worker_init_fncannot be an unpicklable object?": {
        "answer": "lambda function",
        "question": "What does worker_init_fncannot be an unpicklable object?",
        "context": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the multiprocessing best practices?": {
        "answer": "PyTorch",
        "question": "What is the name of the multiprocessing best practices?",
        "context": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Worker_init_fncannot be an unpicklable object, e.g., what?": {
        "answer": "a lambda function",
        "question": "Worker_init_fncannot be an unpicklable object, e.g., what?",
        "context": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the warning len(dataloader)heuristic based on?": {
        "answer": "the length of the sampler used",
        "question": "What is the warning len(dataloader)heuristic based on?",
        "context": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Whendatasetis what, it instead returns an estimate based on onlen(dataset)/batch_size?": {
        "answer": "anIterableDataset",
        "question": "Whendatasetis what, it instead returns an estimate based on onlen(dataset)/batch_size?",
        "context": "len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does PyTorch trust in handling multi-process loading?": {
        "answer": "userdatasetcode",
        "question": "What does PyTorch trust in handling multi-process loading?",
        "context": "Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is len(dataloader)heuristic based on?": {
        "answer": "the length of the sampler used",
        "question": "What is len(dataloader)heuristic based on?",
        "context": "len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "When can more than one batch worth of samples be dropped?": {
        "answer": "whendrop_lastis set",
        "question": "When can more than one batch worth of samples be dropped?",
        "context": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Which type of dataset can not detect such cases in general?": {
        "answer": "PyTorch",
        "question": "Which type of dataset can not detect such cases in general?",
        "context": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What interacts with Multi-process data loading?": {
        "answer": "howIterableDataset",
        "question": "What interacts with Multi-process data loading?",
        "context": "Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What can happen to an otherwise complete batch?": {
        "answer": "broken into multiple ones",
        "question": "What can happen to an otherwise complete batch?",
        "context": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What program can not detect sharding cases in general?": {
        "answer": "PyTorch",
        "question": "What program can not detect sharding cases in general?",
        "context": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does howIterableDataset interact with?": {
        "answer": "Multi-process data loading",
        "question": "What does howIterableDataset interact with?",
        "context": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What interacts withMulti-process data loading?": {
        "answer": "howIterableDataset",
        "question": "What interacts withMulti-process data loading?",
        "context": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What type of numbers do My data loader workers return?": {
        "answer": "random",
        "question": "What type of numbers do My data loader workers return?",
        "context": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What class represents aDataset?": {
        "answer": "abstract class",
        "question": "What class represents aDataset?",
        "context": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does My data loader workers return identical random numbers?": {
        "answer": "Warning SeeReproducibility",
        "question": "What does My data loader workers return identical random numbers?",
        "context": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What do multi-process data loadingnotes return for random seed related questions?": {
        "answer": "Randomness",
        "question": "What do multi-process data loadingnotes return for random seed related questions?",
        "context": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does an abstract class represent?": {
        "answer": "aDataset",
        "question": "What does an abstract class represent?",
        "context": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does SeeReproducibility andMy data loader workers return?": {
        "answer": "random numbers",
        "question": "What does SeeReproducibility andMy data loader workers return?",
        "context": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Randomness in multi-process data loadingnotes for what?": {
        "answer": "random seed related questions",
        "question": "Randomness in multi-process data loadingnotes for what?",
        "context": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "All datasets that represent what should subclass it?": {
        "answer": "a map from keys to data samples",
        "question": "All datasets that represent what should subclass it?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What should all subclasses overwrite__getitem__() support?": {
        "answer": "fetching a data sample for a given key",
        "question": "What should all subclasses overwrite__getitem__() support?",
        "context": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What are subclasses expected to overwrite__len__()?": {
        "answer": "manySamplerimplementations and the default options ofDataLoader",
        "question": "What are subclasses expected to overwrite__len__()?",
        "context": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What type of class represents aDataset?": {
        "answer": "abstract class",
        "question": "What type of class represents aDataset?",
        "context": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "All datasets that represent a map from keys to data samples should what?": {
        "answer": "subclass it",
        "question": "All datasets that represent a map from keys to data samples should what?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What should all subclasses do to support fetching a data sample for a given key?": {
        "answer": "overwrite__getitem__()",
        "question": "What should all subclasses do to support fetching a data sample for a given key?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What could subclasses optionally do?": {
        "answer": "overwrite__len__()",
        "question": "What could subclasses optionally do?",
        "context": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What should all subclasses overwrite to overwrite__len__()?": {
        "answer": "Note",
        "question": "What should all subclasses overwrite to overwrite__len__()?",
        "context": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What do all datasets that represent from keys to data samples should subclass it?": {
        "answer": "a map",
        "question": "What do all datasets that represent from keys to data samples should subclass it?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of a subclass that should overwrite__getitem__()?": {
        "answer": "Note",
        "question": "What is the name of a subclass that should overwrite__getitem__()?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What should all datasets that represent a map from keys to data samples do?": {
        "answer": "subclass it",
        "question": "What should all datasets that represent a map from keys to data samples do?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does DataLoaderby default construct?": {
        "answer": "index sampler",
        "question": "What does DataLoaderby default construct?",
        "context": "Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What must be provided to make it work with a map-style dataset with non-integral indices/keys?": {
        "answer": "a custom sampler must be provided",
        "question": "What must be provided to make it work with a map-style dataset with non-integral indices/keys?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does a custom sampler need to be provided to make it work with a map-style dataset with non-integral indices/": {
        "answer": "iterable Dataset",
        "question": "What does a custom sampler need to be provided to make it work with a map-style dataset with non-integral indices/",
        "context": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What should all datasets that represent an iterable of data samples should do?": {
        "answer": "subclass it",
        "question": "What should all datasets that represent an iterable of data samples should do?",
        "context": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Where does data come from?": {
        "answer": "a stream",
        "question": "Where does data come from?",
        "context": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What should all subclasses do?": {
        "answer": "overwrite__iter__()",
        "question": "What should all subclasses do?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does DataLoaderby use to create a custom sampler?": {
        "answer": "An iterable Dataset",
        "question": "What does DataLoaderby use to create a custom sampler?",
        "context": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What default constructs a index sampler that yields integral indices?": {
        "answer": "DataLoaderby",
        "question": "What default constructs a index sampler that yields integral indices?",
        "context": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is a custom sampler required to work with a map-style dataset with non-integral indices/keys?": {
        "answer": "iterable Dataset",
        "question": "What is a custom sampler required to work with a map-style dataset with non-integral indices/keys?",
        "context": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What should all datasets that represent an iterable of data samples do?": {
        "answer": "subclass it",
        "question": "What should all datasets that represent an iterable of data samples do?",
        "context": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "When are iterable datasets particularly useful?": {
        "answer": "when data come from a stream",
        "question": "When are iterable datasets particularly useful?",
        "context": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the dataset that represents an iterable of data samples?": {
        "answer": "An iterable Dataset",
        "question": "What is the name of the dataset that represents an iterable of data samples?",
        "context": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is an iterable dataset particularly useful when data come from?": {
        "answer": "a stream",
        "question": "What is an iterable dataset particularly useful when data come from?",
        "context": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What would return an iterator of samples in an iterable dataset?": {
        "answer": "overwrite__iter__()",
        "question": "What would return an iterator of samples in an iterable dataset?",
        "context": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is a dataset that represents an iterable of data samples called?": {
        "answer": "iterable Dataset",
        "question": "What is a dataset that represents an iterable of data samples called?",
        "context": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What type of stream does a subclass of data come from?": {
        "answer": "a stream",
        "question": "What type of stream does a subclass of data come from?",
        "context": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "When are subclasses particularly useful?": {
        "answer": "when data come from a stream",
        "question": "When are subclasses particularly useful?",
        "context": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What would overwrite__iter__() do?": {
        "answer": "return an iterator of samples",
        "question": "What would overwrite__iter__() do?",
        "context": "All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the example of splitting workload across all workers using worker_init_fn?": {
        "answer": "splitting workload across all workers",
        "question": "What is the example of splitting workload across all workers using worker_init_fn?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What will each sample be retrieved by?": {
        "answer": "indexing tensors along the first dimension",
        "question": "What will each sample be retrieved by?",
        "context": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What does *tensors (Tensor) mean?": {
        "answer": "tensors that have the same size of the first dimension",
        "question": "What does *tensors (Tensor) mean?",
        "context": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is a concatenation of multiple datasets?": {
        "answer": "Dataset",
        "question": "What is a concatenation of multiple datasets?",
        "context": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is this class useful to?": {
        "answer": "assemble different existing datasets",
        "question": "What is this class useful to?",
        "context": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the definition of datasets to be concatenated?": {
        "answer": "List of datasets to be concatenated",
        "question": "What is the definition of datasets to be concatenated?",
        "context": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the example 1 of?": {
        "answer": "splitting workload across all workers",
        "question": "What is the example 1 of?",
        "context": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Each sample will be retrieved by what along the first dimension?": {
        "answer": "indexing tensors",
        "question": "Each sample will be retrieved by what along the first dimension?",
        "context": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What are tensors that have the same size of the first dimension?": {
        "answer": "*tensors(Tensor)",
        "question": "What are tensors that have the same size of the first dimension?",
        "context": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What class is useful to assemble different existing datasets?": {
        "answer": "Dataset",
        "question": "What class is useful to assemble different existing datasets?",
        "context": "When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is this class useful for?": {
        "answer": "assemble different existing dataset streams",
        "question": "What is this class useful for?",
        "context": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the list of datasets to be concatenated?": {
        "answer": "datasets(sequence) \u2013 List of datasets to be concatenated",
        "question": "What is the name of the list of datasets to be concatenated?",
        "context": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is used to split workload across all workers?": {
        "answer": "worker_init_fn",
        "question": "What is used to split workload across all workers?",
        "context": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "How is each sample retrieved?": {
        "answer": "indexing tensors along the first dimension",
        "question": "How is each sample retrieved?",
        "context": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is a concatenation of multiple datasets useful for?": {
        "answer": "to assemble different existing datasets",
        "question": "What is a concatenation of multiple datasets useful for?",
        "context": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "What is the name of the list of datasets to be concatenated Dataset for chainning multipleIterableDatasets?": {
        "answer": "List of datasets to be concatenated Dataset for chainning multipleIterableDatasets",
        "question": "What is the name of the list of datasets to be concatenated Dataset for chainning multipleIterableDatasets?",
        "context": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. When called in a worker, this returns an object guaranteed to have the\nfollowing attributes: id: the current worker id. num_workers: the total number of workers. ",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    "Returns what in the inputtensor?": {
        "answer": "total number of elements",
        "question": "Returns what in the inputtensor?",
        "context": "Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "dtype(torch.dtype, optional) \u2013 what type of returned tensor?": {
        "answer": "desired data type",
        "question": "dtype(torch.dtype, optional) \u2013 what type of returned tensor?",
        "context": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is casted todtype before the operation is performed?": {
        "answer": "preventing data type overflows",
        "question": "What is casted todtype before the operation is performed?",
        "context": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "Why is the input tensor casted todtypebefore the operation is performed?": {
        "answer": "preventing data type overflows",
        "question": "Why is the input tensor casted todtypebefore the operation is performed?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is the default value of the input tensor?": {
        "answer": "None",
        "question": "What is the default value of the input tensor?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What does this return?": {
        "answer": "the sum of each row of theinputtensor in the given dimensiondim",
        "question": "What does this return?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "Ifdimis a list of dimensions, what does it do?": {
        "answer": "reduce over all of them",
        "question": "Ifdimis a list of dimensions, what does it do?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "If specified, the input tensor is casted what before the operation is performed?": {
        "answer": "todtype",
        "question": "If specified, the input tensor is casted what before the operation is performed?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is this useful for preventing?": {
        "answer": "data type overflows",
        "question": "What is this useful for preventing?",
        "context": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is the default for the input tensor?": {
        "answer": "None",
        "question": "What is the default for the input tensor?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is a return of the input tensor in the given dimensiondim?": {
        "answer": "the sum of each row of theinputtensor in the given dimensiondim",
        "question": "What is a return of the input tensor in the given dimensiondim?",
        "context": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "IfkeepdimisTrue, the output tensor is of what size?": {
        "answer": "size 1",
        "question": "IfkeepdimisTrue, the output tensor is of what size?",
        "context": "IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "How many dimensions does the output tensor have?": {
        "answer": "1",
        "question": "How many dimensions does the output tensor have?",
        "context": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is intortuple of python:ints?": {
        "answer": "dim",
        "question": "What is intortuple of python:ints?",
        "context": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is returned when a row of the inputtensor is set to a given dimensiondim?": {
        "answer": "Returns the sum of each row of theinputtensor in the given dimensiondim",
        "question": "What is returned when a row of the inputtensor is set to a given dimensiondim?",
        "context": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "Ifdimis a list of dimensions, reduce over all of them?": {
        "answer": "IfkeepdimisTrue",
        "question": "Ifdimis a list of dimensions, reduce over all of them?",
        "context": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is the name of the output tensor that has fewer dimension(s)?": {
        "answer": "orlen(dim)",
        "question": "What is the name of the output tensor that has fewer dimension(s)?",
        "context": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is the sum of each row of theinputtensor in the given dimensiondim?": {
        "answer": "Ifdimis a list of dimensions",
        "question": "What is the sum of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "Ifdimis a list of dimensions, what do you do over all of them?": {
        "answer": "reduce",
        "question": "Ifdimis a list of dimensions, what do you do over all of them?",
        "context": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What returns the output tensor of the same size as input except in the dimension(s)dim where it is of size 1?": {
        "answer": "IfkeepdimisTrue",
        "question": "What returns the output tensor of the same size as input except in the dimension(s)dim where it is of size 1?",
        "context": "Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What is the name of the output tensor that has fewer dimension(s) than input?": {
        "answer": "orlen(dim)",
        "question": "What is the name of the output tensor that has fewer dimension(s) than input?",
        "context": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "If the output tensor is of the same size as input, what is it?": {
        "answer": "IfkeepdimisTrue",
        "question": "If the output tensor is of the same size as input, what is it?",
        "context": "IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    "What does the input tensor compute?": {
        "answer": "bitwise NOT",
        "question": "What does the input tensor compute?",
        "context": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    "What type of type does the input tensor have?": {
        "answer": "Boolean types",
        "question": "What type of type does the input tensor have?",
        "context": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    "For what type of tensor, it computes the logical NOT of the input tensor?": {
        "answer": "bool tensors",
        "question": "For what type of tensor, it computes the logical NOT of the input tensor?",
        "context": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    "What is out of the input tensor?": {
        "answer": "output tensor",
        "question": "What is out of the input tensor?",
        "context": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    "What does compute the bitwise NOT of the given input tensor do?": {
        "answer": "Computes the bitwise NOT",
        "question": "What does compute the bitwise NOT of the given input tensor do?",
        "context": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    "The input tensor must be of what types?": {
        "answer": "integral or Boolean types",
        "question": "The input tensor must be of what types?",
        "context": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    "For what type of tensor does it compute the logical NOT?": {
        "answer": "bool tensors",
        "question": "For what type of tensor does it compute the logical NOT?",
        "context": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    "What is a tensor with two or more dimensions?": {
        "answer": "Splitsinput",
        "question": "What is a tensor with two or more dimensions?",
        "context": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What is splitsinput?": {
        "answer": "view ofinput",
        "question": "What is splitsinput?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is the equivalent of calling Splitsinput?": {
        "answer": "torch",
        "question": "What is the equivalent of calling Splitsinput?",
        "context": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What is the function based on?": {
        "answer": "NumPy",
        "question": "What is the function based on?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is tensor to split?": {
        "answer": "input(Tensor)",
        "question": "What is tensor to split?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is used to split a tensor with two or more dimensions into multiple tensors vertically?": {
        "answer": "indices_or_sections",
        "question": "What is used to split a tensor with two or more dimensions into multiple tensors vertically?",
        "context": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What is each split?": {
        "answer": "a view ofinput",
        "question": "What is each split?",
        "context": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What is the difference between splitsinput and torch.tensor_split(input, indices_or_sections": {
        "answer": "ifindices_or_sectionsis an integer it must evenly divide the split dimension",
        "question": "What is the difference between splitsinput and torch.tensor_split(input, indices_or_sections",
        "context": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What is this function based on?": {
        "answer": "NumPy\u2019snumpy.vsplit()",
        "question": "What is this function based on?",
        "context": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What is the equivalent of calling input, indices_or_sections, dim=0?": {
        "answer": "torch.tensor_split",
        "question": "What is the equivalent of calling input, indices_or_sections, dim=0?",
        "context": "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What is the split dimension of torch.tensor_split?": {
        "answer": "dim=0",
        "question": "What is the split dimension of torch.tensor_split?",
        "context": "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What is input to split?": {
        "answer": "tensor",
        "question": "What is input to split?",
        "context": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is the name of the tensor to split?": {
        "answer": "indices_or_sections",
        "question": "What is the name of the tensor to split?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is the difference between torch.tensor_split and torch.tensor_split?": {
        "answer": "ifindices_or_sectionsis an integer it must evenly divide the split dimension",
        "question": "What is the difference between torch.tensor_split and torch.tensor_split?",
        "context": "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "What does input(Tensor) - tensor to split?": {
        "answer": "input(Tensor) \u2013 tensor to split",
        "question": "What does input(Tensor) - tensor to split?",
        "context": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is the argument for indices_or_sections(Tensor,intorlistortuple of py": {
        "answer": "intorch.tensor_split()",
        "question": "What is the argument for indices_or_sections(Tensor,intorlistortuple of py",
        "context": "This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is a tensor with one or more dimensions?": {
        "answer": "Splitsinput",
        "question": "What is a tensor with one or more dimensions?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is a split in a tensor?": {
        "answer": "view ofinput",
        "question": "What is a split in a tensor?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What are splitsinput according to?": {
        "answer": "indices",
        "question": "What are splitsinput according to?",
        "context": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is the split dimension of tensor_split?": {
        "answer": "zero",
        "question": "What is the split dimension of tensor_split?",
        "context": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is equivalent to calling torch?": {
        "answer": "ifinputhas two or more dimensions",
        "question": "What is equivalent to calling torch?",
        "context": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is input derived from NumPy'snumpy.hsplit()?": {
        "answer": "tensor",
        "question": "What is input derived from NumPy'snumpy.hsplit()?",
        "context": "This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    "What is the tensor to split?": {
        "answer": "input(Tensor)",
        "question": "What is the tensor to split?",
        "context": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    "In what direction do the entries in each row of tensor appear in a different order than before?": {
        "answer": "left/right",
        "question": "In what direction do the entries in each row of tensor appear in a different order than before?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "What are preserved, but appear in a different order than before?": {
        "answer": "Columns",
        "question": "What are preserved, but appear in a different order than before?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "What is the name of'snp.fliplr'?": {
        "answer": "NumPy",
        "question": "What is the name of'snp.fliplr'?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "What is torch.fliplris expected to be?": {
        "answer": "slower thannp.fliplr",
        "question": "What is torch.fliplris expected to be?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "What does flip tensor in the left/right direction return?": {
        "answer": "a new tensor",
        "question": "What does flip tensor in the left/right direction return?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "What does flip tensor do?": {
        "answer": "Flip the entries in each row in the left/right direction",
        "question": "What does flip tensor do?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "What must the tensor be at least?": {
        "answer": "2-D",
        "question": "What must the tensor be at least?",
        "context": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    "What is the name of the elements ofinput that returns a new tensor?": {
        "answer": "hyperbolic cosine",
        "question": "What is the name of the elements ofinput that returns a new tensor?",
        "context": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    "What library does torch.cosh use?": {
        "answer": "Sleef library",
        "question": "What library does torch.cosh use?",
        "context": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    "What does the Sleef library use?": {
        "answer": "Seeherefor details",
        "question": "What does the Sleef library use?",
        "context": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    "Returns what with the hyperbolic cosine of the elements of input?": {
        "answer": "a new tensor",
        "question": "Returns what with the hyperbolic cosine of the elements of input?",
        "context": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    "Wheninputis on the CPU, the implementation of torch.cosh may use what library?": {
        "answer": "Sleef library",
        "question": "Wheninputis on the CPU, the implementation of torch.cosh may use what library?",
        "context": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    "What does the Sleef library provide?": {
        "answer": "details",
        "question": "What does the Sleef library provide?",
        "context": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    "Fillsselftensor with numbers samples from what distribution parameterized by the given meanmuand standard deviationsigma?": {
        "answer": "log-normal distribution",
        "question": "Fillsselftensor with numbers samples from what distribution parameterized by the given meanmuand standard deviationsigma?",
        "context": "Fillsselftensor with numbers samples from the log-normal distribution\nparameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3. Note thatmeanandstdare the mean and\nstandard deviation of the underlying normal distribution, and not of the\nreturned distribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_"
    },
    "What is not represented by the mean and standard deviation of the underlying normal distribution?": {
        "answer": "returned distribution",
        "question": "What is not represented by the mean and standard deviation of the underlying normal distribution?",
        "context": "Fillsselftensor with numbers samples from the log-normal distribution\nparameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3. Note thatmeanandstdare the mean and\nstandard deviation of the underlying normal distribution, and not of the\nreturned distribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_"
    },
    "What is similar tobroadcast_tensors()but for?": {
        "answer": "shapes",
        "question": "What is similar tobroadcast_tensors()but for?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What is the equivalent tobroadcast_tensors()but for shapes?": {
        "answer": "totorch.broadcast_tensors",
        "question": "What is the equivalent tobroadcast_tensors()but for shapes?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What is this useful for?": {
        "answer": "broadcasting tensors of common batch shape",
        "question": "What is this useful for?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What is used for broadcasting tensors of common batch shape but different rightmost shape?": {
        "answer": "mean vectors with covariance matrices",
        "question": "What is used for broadcasting tensors of common batch shape but different rightmost shape?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What is an example of a shape compatible with all input shapes?": {
        "answer": "*shapes(torch.Size) \u2013 Shapes of tensors",
        "question": "What is an example of a shape compatible with all input shapes?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What is a shape compatible with all input shapes?": {
        "answer": "RuntimeError",
        "question": "What is a shape compatible with all input shapes?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What is similar to tobroadcast_tensors() but for?": {
        "answer": "shapes",
        "question": "What is similar to tobroadcast_tensors() but for?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What does shapebut avoid?": {
        "answer": "create to intermediate tensors",
        "question": "What does shapebut avoid?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What shape is used to broadcast tensors of common batch shape but different?": {
        "answer": "rightmost shape",
        "question": "What shape is used to broadcast tensors of common batch shape but different?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What is useful for broadcasting tensors of common batch shape but different rightmost shape?": {
        "answer": "mean vectors with covariance matrices",
        "question": "What is useful for broadcasting tensors of common batch shape but different rightmost shape?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "A shape compatible with what?": {
        "answer": "all input shapes",
        "question": "A shape compatible with what?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What happens if shapes are incompatible?": {
        "answer": "RuntimeError",
        "question": "What happens if shapes are incompatible?",
        "context": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    "What is the behavior similar to python\u2019sitertools.product?": {
        "answer": "Do cartesian product",
        "question": "What is the behavior similar to python\u2019sitertools.product?",
        "context": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod"
    },
    "What is a *tensor?": {
        "answer": "any number of 1 dimensional tensors",
        "question": "What is a *tensor?",
        "context": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod"
    },
    "What is a tensor equivalent to?": {
        "answer": "converting all the input tensors into lists",
        "question": "What is a tensor equivalent to?",
        "context": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod"
    },
    "What is a tensor equivalent to converting all the input tensors into lists?": {
        "answer": "Tensor Example",
        "question": "What is a tensor equivalent to converting all the input tensors into lists?",
        "context": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod"
    },
    "What package contains data structures for multi-dimensional tensors?": {
        "answer": "torch",
        "question": "What package contains data structures for multi-dimensional tensors?",
        "context": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the torch package provide for efficient serializing of Tensors and arbitrary types?": {
        "answer": "utilities",
        "question": "What does the torch package provide for efficient serializing of Tensors and arbitrary types?",
        "context": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the torch package have?": {
        "answer": "CUDA counterpart",
        "question": "What does the torch package have?",
        "context": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the torch package return?": {
        "answer": "total number of elements in theinputtensor",
        "question": "What does the torch package return?",
        "context": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What utility does the torch package provide for Tensors and arbitrary types?": {
        "answer": "efficient serializing",
        "question": "What utility does the torch package provide for Tensors and arbitrary types?",
        "context": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the counterpart of the torch package?": {
        "answer": "CUDA",
        "question": "What is the counterpart of the torch package?",
        "context": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does True ifobjis return?": {
        "answer": "PyTorch storage object",
        "question": "What does True ifobjis return?",
        "context": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the PyTorch tensor have?": {
        "answer": "CUDA counterpart",
        "question": "What does the PyTorch tensor have?",
        "context": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is True ifobjis?": {
        "answer": "PyTorch storage object",
        "question": "What is True ifobjis?",
        "context": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the data type ofinputis a complex data type?": {
        "answer": "one oftorch.complex64, andtorch.complex128",
        "question": "What is the data type ofinputis a complex data type?",
        "context": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the data type ofinputis?": {
        "answer": "floating point data type",
        "question": "What is the data type ofinputis?",
        "context": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the GPU that allows you to run tensor computations on an NVIDIA GPU with compute capability >= 3.0": {
        "answer": "CUDA",
        "question": "What is the name of the GPU that allows you to run tensor computations on an NVIDIA GPU with compute capability >= 3.0",
        "context": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does True return ifobjis a PyTorch tensor?": {
        "answer": "PyTorch storage object",
        "question": "What does True return ifobjis a PyTorch tensor?",
        "context": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns True if the data type ofinput is a what?": {
        "answer": "complex data type",
        "question": "Returns True if the data type ofinput is a what?",
        "context": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns True if the data type ofinputis a what type of data type?": {
        "answer": "complex data type",
        "question": "Returns True if the data type ofinputis a what type of data type?",
        "context": "Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns True ifobjis a what?": {
        "answer": "PyTorch storage object",
        "question": "Returns True ifobjis a what?",
        "context": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns True if the data type ofinputis a what?": {
        "answer": "floating point data type",
        "question": "Returns True if the data type ofinputis a what?",
        "context": "Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is not equal to zero after type conversions?": {
        "answer": "a single element tensor",
        "question": "What is not equal to zero after type conversions?",
        "context": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Returns True if the data type ofinputis a single element tensor which is not equal to zero after type conversion": {
        "answer": "Sets the default floating point dtype tod",
        "question": "What does Returns True if the data type ofinputis a single element tensor which is not equal to zero after type conversion",
        "context": "Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned ifobjis a PyTorch storage object?": {
        "answer": "PyTorch storage object",
        "question": "What is returned ifobjis a PyTorch storage object?",
        "context": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns True what if theinputis a single element tensor?": {
        "answer": "if theinputis a single element tensor",
        "question": "Returns True what if theinputis a single element tensor?",
        "context": "Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it do if the input is a single element tensor which is not equal to zero after type conversions?": {
        "answer": "Sets the default floating point dtype tod",
        "question": "What does it do if the input is a single element tensor which is not equal to zero after type conversions?",
        "context": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does Sets the default floating point dtype tod return?": {
        "answer": "current default floating pointtorch.dtype",
        "question": "What does Sets the default floating point dtype tod return?",
        "context": "Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it do if the data type ofinputis a single element tensor which is not equal to zero after type conversions": {
        "answer": "Sets the default floating point dtype tod",
        "question": "What does it do if the data type ofinputis a single element tensor which is not equal to zero after type conversions",
        "context": "Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what if the data type ofinputis a floating point data type?": {
        "answer": "True",
        "question": "Returns what if the data type ofinputis a floating point data type?",
        "context": "Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it do if the input is a single element tensor?": {
        "answer": "Sets the default floating point dtype tod",
        "question": "What does it do if the input is a single element tensor?",
        "context": "Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Sets what to floating point tensor typet?": {
        "answer": "defaulttorch.Tensortype",
        "question": "Sets what to floating point tensor typet?",
        "context": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned when the defaulttorch.Tensortype is set to floating point tensor typet?": {
        "answer": "the total number of elements in theinputtensor",
        "question": "What is returned when the defaulttorch.Tensortype is set to floating point tensor typet?",
        "context": "Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Set options for what?": {
        "answer": "printing",
        "question": "Set options for what?",
        "context": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what if theinputis a single element tensor which is not equal to zero after type conversions?": {
        "answer": "True",
        "question": "Returns what if theinputis a single element tensor which is not equal to zero after type conversions?",
        "context": "Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Sets what?": {
        "answer": "random number generator state",
        "question": "Sets what?",
        "context": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the default floating pointtorch.dtype?": {
        "answer": "current",
        "question": "What is the default floating pointtorch.dtype?",
        "context": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned by the defaulttorch.Tensortype?": {
        "answer": "the total number of elements in theinputtensor",
        "question": "What is returned by the defaulttorch.Tensortype?",
        "context": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Disables what on CPU?": {
        "answer": "denormal floating numbers",
        "question": "Disables what on CPU?",
        "context": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Disables denormal floating numbers on CPU?": {
        "answer": "Note",
        "question": "Disables denormal floating numbers on CPU?",
        "context": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the default floating point dtype tod?": {
        "answer": "current default floating pointtorch.dtype",
        "question": "What is the default floating point dtype tod?",
        "context": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned by setting the defaulttorch.Tensortype to floating point tensor typet?": {
        "answer": "the total number of elements in theinputtensor",
        "question": "What is returned by setting the defaulttorch.Tensortype to floating point tensor typet?",
        "context": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the defaulttorch.Tensortype set to floating point tensor typet?": {
        "answer": "current default floating pointtorch.dtype",
        "question": "What does the defaulttorch.Tensortype set to floating point tensor typet?",
        "context": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the defaulttorch.Tensortype?": {
        "answer": "current default floating pointtorch.dtype",
        "question": "What is the defaulttorch.Tensortype?",
        "context": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the default option for?": {
        "answer": "printing",
        "question": "What is the default option for?",
        "context": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the default floating pointtorch.dtype mean?": {
        "answer": "Note",
        "question": "What does the default floating pointtorch.dtype mean?",
        "context": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What do you get?": {
        "answer": "current default floating pointtorch.dtype",
        "question": "What do you get?",
        "context": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a floating point tensor typet?": {
        "answer": "defaulttorch.Tensortype",
        "question": "What is a floating point tensor typet?",
        "context": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the defaulttorch.Tensortype to floating point tensor typet?": {
        "answer": "Note",
        "question": "What is the defaulttorch.Tensortype to floating point tensor typet?",
        "context": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the defaulttorch.Tensortype do?": {
        "answer": "Disables denormal floating numbers on CPU",
        "question": "What does the defaulttorch.Tensortype do?",
        "context": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the variable that can be used to set denormal floating numbers on the CPU?": {
        "answer": "Note",
        "question": "What is the name of the variable that can be used to set denormal floating numbers on the CPU?",
        "context": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the total number of elements in?": {
        "answer": "theinputtensor",
        "question": "What is the total number of elements in?",
        "context": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a Disables denormal floating numbers on CPU?": {
        "answer": "Set options for printing",
        "question": "What is a Disables denormal floating numbers on CPU?",
        "context": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a denormal floating number on a CPU do?": {
        "answer": "Disables denormal floating numbers on CPU",
        "question": "What does a denormal floating number on a CPU do?",
        "context": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are listed underRandom sampling?": {
        "answer": "Random sampling creation ops",
        "question": "What are listed underRandom sampling?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor have?": {
        "answer": "Splits a tensor into a specific number of chunks",
        "question": "What does a tensor have?",
        "context": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does this function do?": {
        "answer": "Unpacks the data and pivots",
        "question": "What does this function do?",
        "context": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of creation ops are listed under Random sampling?": {
        "answer": "Random sampling",
        "question": "What type of creation ops are listed under Random sampling?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is constructed withdata?": {
        "answer": "a tensor",
        "question": "What is constructed withdata?",
        "context": "Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are Disables denormal floating numbers on CPU?": {
        "answer": "Set options for printing",
        "question": "What are Disables denormal floating numbers on CPU?",
        "context": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the CPU do?": {
        "answer": "Disables denormal floating numbers",
        "question": "What does the CPU do?",
        "context": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of withdata does torch.empty() construct?": {
        "answer": "tensor",
        "question": "What type of withdata does torch.empty() construct?",
        "context": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does it do to disable denormal floating numbers on CPU?": {
        "answer": "Set options for printing",
        "question": "What does it do to disable denormal floating numbers on CPU?",
        "context": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What happens on CPU?": {
        "answer": "Disables denormal floating numbers",
        "question": "What happens on CPU?",
        "context": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What are listed underRandom samplingand include:torch.rand()torch.rand_like()torch.": {
        "answer": "Random sampling creation ops",
        "question": "What are listed underRandom samplingand include:torch.rand()torch.rand_like()torch.",
        "context": "Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does torch.empty() construct?": {
        "answer": "tensor withdata",
        "question": "What does torch.empty() construct?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What format does asparse tensor in?": {
        "answer": "COO(rdinate) format",
        "question": "What format does asparse tensor in?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What creation ops are listed under Random sampling?": {
        "answer": "Random sampling",
        "question": "What creation ops are listed under Random sampling?",
        "context": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does asparse tensor in COO(rdinate) format contain?": {
        "answer": "specified values at the givenindices",
        "question": "What does asparse tensor in COO(rdinate) format contain?",
        "context": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What do you convert the data into?": {
        "answer": "atorch.Tensor",
        "question": "What do you convert the data into?",
        "context": "Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the asparse tensor contain?": {
        "answer": "specified values at the givenindices",
        "question": "What does the asparse tensor contain?",
        "context": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the data converted into?": {
        "answer": "atorch.Tensor",
        "question": "What is the data converted into?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a view of an existingtorch.Tensorinput have?": {
        "answer": "specifiedsize,strideandstorage_offset",
        "question": "What does a view of an existingtorch.Tensorinput have?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of aTensor created?": {
        "answer": "anumpy.ndarray",
        "question": "What is the name of aTensor created?",
        "context": "Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor fill with the scalar value0 have?": {
        "answer": "the same size asinput",
        "question": "What does a tensor fill with the scalar value0 have?",
        "context": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor filled with the scalar value1?": {
        "answer": "the shape defined by the variable argumentsize",
        "question": "What is a tensor filled with the scalar value1?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does an existingtorch.Tensorinput have?": {
        "answer": "specifiedsize,strideandstorage_offset",
        "question": "What does an existingtorch.Tensorinput have?",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the same size asinput?": {
        "answer": "a tensor filled with the scalar value0",
        "question": "What is the same size asinput?",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor fill with the scalar value1 have?": {
        "answer": "the same size asinput",
        "question": "What does a tensor fill with the scalar value1 have?",
        "context": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the shape of a tensor filled with the scalar value0?": {
        "answer": "the shape defined by the variable argumentsize",
        "question": "What is the shape of a tensor filled with the scalar value0?",
        "context": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does the tensor fill with the scalar value0 have?": {
        "answer": "the same size asinput",
        "question": "What does the tensor fill with the scalar value0 have?",
        "context": "Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the shape of a tensor filled with the scalar value1?": {
        "answer": "the shape defined by the variable argumentsize",
        "question": "What is the shape of a tensor filled with the scalar value1?",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the name of the aTensor created from?": {
        "answer": "anumpy.ndarray",
        "question": "What is the name of the aTensor created from?",
        "context": "Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with the shape defined by the variable argumentsize. Returns a tensor filled with what value": {
        "answer": "scalar value1",
        "question": "Returns a tensor filled with the shape defined by the variable argumentsize. Returns a tensor filled with what value",
        "context": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is returned when the scalar value0 is returned?": {
        "answer": "a tensor",
        "question": "What is returned when the scalar value0 is returned?",
        "context": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with the scalar value0, with what as input?": {
        "answer": "same size",
        "question": "Returns a tensor filled with the scalar value0, with what as input?",
        "context": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize. Returns a": {
        "answer": "same size",
        "question": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize. Returns a",
        "context": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tensor of sizeendstartstepleftlceil fractextend?": {
        "answer": "1-D",
        "question": "What is the tensor of sizeendstartstepleftlceil fractextend?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with the scalar value1 with what?": {
        "answer": "the shape defined by the variable argumentsize",
        "question": "Returns a tensor filled with the scalar value1 with what?",
        "context": "Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with the scalar value1, with what as input?": {
        "answer": "same size",
        "question": "Returns a tensor filled with the scalar value1, with what as input?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what tensor of sizeendstartstepleftlceil fractextend - ": {
        "answer": "1-D",
        "question": "Returns what tensor of sizeendstartstepleftlceil fractextend - ",
        "context": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What defines the shape of a tensor filled with the scalar value?": {
        "answer": "variable argumentsize",
        "question": "What defines the shape of a tensor filled with the scalar value?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a tensor fill with the scalar value have?": {
        "answer": "the same size asinput",
        "question": "What does a tensor fill with the scalar value have?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tensor of sizeendstartstepleftlceil?": {
        "answer": "1-D",
        "question": "What is the tensor of sizeendstartstepleftlceil?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what type of tensor?": {
        "answer": "a 1-D tensor",
        "question": "Returns what type of tensor?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor of sizeendstartstepleftlceil fractextend?": {
        "answer": "1-D",
        "question": "What is a tensor of sizeendstartstepleftlceil fractextend?",
        "context": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor of sizeendstartstep+1leftlfloor fractextend?": {
        "answer": "1-D",
        "question": "What is a tensor of sizeendstartstep+1leftlfloor fractextend?",
        "context": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns what type of scalar value?": {
        "answer": "a tensor",
        "question": "Returns what type of scalar value?",
        "context": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Where are the values from the interval[start,end]taken?": {
        "answer": "fromstart",
        "question": "Where are the values from the interval[start,end]taken?",
        "context": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with the scalar value of how many tensors?": {
        "answer": "1",
        "question": "Returns a tensor filled with the scalar value of how many tensors?",
        "context": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How many -D tensor of sizeendstartstep+1leftlceil fractextend": {
        "answer": "1",
        "question": "How many -D tensor of sizeendstartstep+1leftlceil fractextend",
        "context": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tensor of sizesteps created?": {
        "answer": "one-dimensional",
        "question": "What is the tensor of sizesteps created?",
        "context": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Where are the values from the interval[start,end] taken?": {
        "answer": "fromstart",
        "question": "Where are the values from the interval[start,end] taken?",
        "context": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "How many tensors does this function return?": {
        "answer": "1",
        "question": "How many tensors does this function return?",
        "context": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of tensor of sizesteps is created?": {
        "answer": "one-dimensional",
        "question": "What type of tensor of sizesteps is created?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the tensor of sizeendstartstep+1leftlfloor?": {
        "answer": "1-D",
        "question": "What is the tensor of sizeendstartstep+1leftlfloor?",
        "context": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor of sizesteps of how many dimensions?": {
        "answer": "1",
        "question": "Returns a tensor of sizesteps of how many dimensions?",
        "context": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor of sizesteps that values are evenly spaced fromstarttoend, inclusive?": {
        "answer": "one-dimensional",
        "question": "What is a tensor of sizesteps that values are evenly spaced fromstarttoend, inclusive?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is the logarithmic scale of a one-dimensional tensor of sizesteps?": {
        "answer": "basebase",
        "question": "What is the logarithmic scale of a one-dimensional tensor of sizesteps?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor on the diagonal and zeros elsewhere?": {
        "answer": "2-D",
        "question": "What is a tensor on the diagonal and zeros elsewhere?",
        "context": " Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "On a logarithmic scale, what is used to create a one-dimensional tensor of sizesteps?": {
        "answer": "basebase",
        "question": "On a logarithmic scale, what is used to create a one-dimensional tensor of sizesteps?",
        "context": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a 2-D tensor with what?": {
        "answer": "ones on the diagonal and zeros elsewhere",
        "question": "Returns a 2-D tensor with what?",
        "context": " Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "Returns a tensor filled with what?": {
        "answer": "random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)",
        "question": "Returns a tensor filled with what?",
        "context": "  Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of tensor is created on a logarithmic scale with basebase?": {
        "answer": "one-dimensional",
        "question": "What type of tensor is created on a logarithmic scale with basebase?",
        "context": " Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What is a tensor filled with uninitialized data?": {
        "answer": "uninitialized",
        "question": "What is a tensor filled with uninitialized data?",
        "context": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What type of tensor is created?": {
        "answer": "one-dimensional tensor of sizesteps",
        "question": "What type of tensor is created?",
        "context": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    "What does a complex tensor return?": {
        "answer": "a view ofinputas",
        "question": "What does a complex tensor return?",
        "context": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex"
    },
    "What does this function return?": {
        "answer": "a view ofinput as a complex tensor",
        "question": "What does this function return?",
        "context": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex"
    },
    "The last dimension of the input tensor is expected to represent what?": {
        "answer": "real and imaginary components of complex numbers",
        "question": "The last dimension of the input tensor is expected to represent what?",
        "context": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex"
    },
    "What does the function return for an input complex tensor ofsizem1,m2,...,mim1, m2, do": {
        "answer": "torch.view_as_complex",
        "question": "What does the function return for an input complex tensor ofsizem1,m2,...,mim1, m2, do",
        "context": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex"
    },
    "What does compute the element-wise logical XOR of the given input tensors do?": {
        "answer": "Computes the element-wise logical XOR",
        "question": "What does compute the element-wise logical XOR of the given input tensors do?",
        "context": "Computes the element-wise logical XOR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute XOR with out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor"
    },
    "Nonzeros are treated as what?": {
        "answer": "True",
        "question": "Nonzeros are treated as what?",
        "context": "Computes the element-wise logical XOR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute XOR with out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor"
    },
    "What is the name of the function that generates a question?": {
        "answer": "Alias fortorch.abs()",
        "question": "What is the name of the function that generates a question?",
        "context": "Alias fortorch.abs() ",
        "source": "https://pytorch.org/docs/stable/generated/torch.absolute.html#torch.absolute"
    },
    "What does Alias fortorch.clamp() do?": {
        "answer": "Alias fortorch.clamp()",
        "question": "What does Alias fortorch.clamp() do?",
        "context": "Alias fortorch.clamp(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.clip.html#torch.clip"
    },
    "What is an object that represents the data type of atorch.Tensor?": {
        "answer": "Atorch.dtype",
        "question": "What is an object that represents the data type of atorch.Tensor?",
        "context": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How many different data types does PyTorch have?": {
        "answer": "twelve",
        "question": "How many different data types does PyTorch have?",
        "context": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How many -bit floating point1 torch does PyTorch have?": {
        "answer": "16",
        "question": "How many -bit floating point1 torch does PyTorch have?",
        "context": "Eachtorch.Tensorhas atorch.dtype,torch.device, andtorch.layout. Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the 16-bit floating point1 torch?": {
        "answer": "double",
        "question": "What is the 16-bit floating point1 torch?",
        "context": "Eachtorch.Tensorhas atorch.dtype,torch.device, andtorch.layout. Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What type of torch does atorch.Tensor have?": {
        "answer": "double torch",
        "question": "What type of torch does atorch.Tensor have?",
        "context": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the 16-bit floating point2 torch?": {
        "answer": "HalfTensor",
        "question": "What is the name of the 16-bit floating point2 torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the torch that float64ortorch.double torch?": {
        "answer": "64-bit floating point torch",
        "question": "What is the name of the torch that float64ortorch.double torch?",
        "context": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the unsigned torch.uint8 torch?": {
        "answer": "8-bit integer",
        "question": "What is the name of the unsigned torch.uint8 torch?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the number of integers in the torch.int8 torch?": {
        "answer": "8-bit",
        "question": "What is the number of integers in the torch.int8 torch?",
        "context": "torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the name of the torch.int16ortorch.short torch?": {
        "answer": "16-bit integer",
        "question": "What is the name of the name of the torch.int16ortorch.short torch?",
        "context": "torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the number of unsigned torch.uint8 torch in the BFloat16Tensor?": {
        "answer": "8-bit",
        "question": "What is the number of unsigned torch.uint8 torch in the BFloat16Tensor?",
        "context": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the integer (signed) torch.int8 torch?": {
        "answer": "8-bit",
        "question": "What is the name of the integer (signed) torch.int8 torch?",
        "context": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the complex torch.complex64ortorch.cfloat 128-bit complex torch?": {
        "answer": "64-bit",
        "question": "What is the name of the complex torch.complex64ortorch.cfloat 128-bit complex torch?",
        "context": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the torch that is int16ortorch.short torch?": {
        "answer": "16-bit integer",
        "question": "What is the name of the torch that is int16ortorch.short torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the number of integers in the torch.int32ortorch.int torch?": {
        "answer": "32-bit",
        "question": "What is the number of integers in the torch.int32ortorch.int torch?",
        "context": "torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What type of integer is intTensor?": {
        "answer": "64-bit",
        "question": "What type of integer is intTensor?",
        "context": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the number of integers in torch.int32ortorch.int torch?": {
        "answer": "32-bit",
        "question": "What is the number of integers in torch.int32ortorch.int torch?",
        "context": "torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "IntTensor is what type of integer (signed) torch.int64ortorch.long?": {
        "answer": "64-bit",
        "question": "IntTensor is what type of integer (signed) torch.int64ortorch.long?",
        "context": "torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How many -bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch?": {
        "answer": "128",
        "question": "How many -bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the number of unsigned torch in the BFloat16Tensor?": {
        "answer": "8-bit",
        "question": "What is the number of unsigned torch in the BFloat16Tensor?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the torch that is intTensor?": {
        "answer": "64-bit",
        "question": "What is the name of the torch that is intTensor?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is a 8-bit integer?": {
        "answer": "unsigned",
        "question": "What is a 8-bit integer?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the 8-bit integer (signed) torch.int8 torch?": {
        "answer": "ByteTensor",
        "question": "What is the name of the 8-bit integer (signed) torch.int8 torch?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "When is binary16 useful?": {
        "answer": "when precision is important",
        "question": "When is binary16 useful?",
        "context": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is important when a binary16 uses 1 sign, 5 exponent, and 10 significand bits?": {
        "answer": "precision",
        "question": "What is important when a binary16 uses 1 sign, 5 exponent, and 10 significand bits?",
        "context": "Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the sign of a ByteTensor?": {
        "answer": "8-bit integer",
        "question": "What is the sign of a ByteTensor?",
        "context": "torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is important when it has the same number of exponent bits asfloat32?": {
        "answer": "range",
        "question": "What is important when it has the same number of exponent bits asfloat32?",
        "context": "torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the sign for torch.int32ortorch.int torch?": {
        "answer": "32-bit integer",
        "question": "What is the sign for torch.int32ortorch.int torch?",
        "context": "32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the 64-bit integer (signed) torch.int64ortorch.long torch?": {
        "answer": "Boolean torch",
        "question": "What is the name of the 64-bit integer (signed) torch.int64ortorch.long torch?",
        "context": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the BoolTensor sometimes referred to as?": {
        "answer": "binary16",
        "question": "What is the BoolTensor sometimes referred to as?",
        "context": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is binary16 sometimes referred to as?": {
        "answer": "Brain Floating Point",
        "question": "What is binary16 sometimes referred to as?",
        "context": "Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "When is Brain Floating Point useful?": {
        "answer": "when range is important",
        "question": "When is Brain Floating Point useful?",
        "context": "torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of a torch.int32ortorch.long torch?": {
        "answer": "64-bit integer",
        "question": "What is the name of a torch.int32ortorch.long torch?",
        "context": "torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is another name for a 64-bit integer?": {
        "answer": "long torch",
        "question": "What is another name for a 64-bit integer?",
        "context": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is important when a brain floating point is used?": {
        "answer": "precision",
        "question": "What is important when a brain floating point is used?",
        "context": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is used to use 1 sign, 8 exponent and 7 significand bits?": {
        "answer": "Brain Floating Point",
        "question": "What is used to use 1 sign, 8 exponent and 7 significand bits?",
        "context": "torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What can be used to find out if atorch.dtypeis a floating point data type?": {
        "answer": "propertyis_floating_pointcan be used",
        "question": "What can be used to find out if atorch.dtypeis a floating point data type?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is BooleanTensor sometimes referred to as?": {
        "answer": "binary16",
        "question": "What is BooleanTensor sometimes referred to as?",
        "context": "Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "To find out if atorch.dtypeis a complex data type, what property can be used?": {
        "answer": "propertyis_complex",
        "question": "To find out if atorch.dtypeis a complex data type, what property can be used?",
        "context": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is an object representing the device on which atorch.Tensoris or will be allocated?": {
        "answer": "Atorch.device",
        "question": "What is an object representing the device on which atorch.Tensoris or will be allocated?",
        "context": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is a device type called?": {
        "answer": "cpu'or'cuda",
        "question": "What is a device type called?",
        "context": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is called if the device ordinal is not present?": {
        "answer": "aftertorch.cuda.set_device()",
        "question": "What is called if the device ordinal is not present?",
        "context": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "If the device ordinal is not present, this object will always represent what for the device type?": {
        "answer": "current device",
        "question": "If the device ordinal is not present, this object will always represent what for the device type?",
        "context": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What does the torch.device contains?": {
        "answer": "device type",
        "question": "What does the torch.device contains?",
        "context": "Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How can Atorch.Tensor's device be accessed?": {
        "answer": "theTensor.deviceproperty",
        "question": "How can Atorch.Tensor's device be accessed?",
        "context": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What does Thetorch.device contain?": {
        "answer": "device type",
        "question": "What does Thetorch.device contain?",
        "context": "Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What can be done by using a string or a device ordinal?": {
        "answer": "fast prototyping of code",
        "question": "What can be done by using a string or a device ordinal?",
        "context": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What returns an ordinal for cuda tensors?": {
        "answer": "matchesTensor.get_device()",
        "question": "What returns an ordinal for cuda tensors?",
        "context": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the device that is not supported for cpu tensors?": {
        "answer": "Note",
        "question": "What is the name of the device that is not supported for cpu tensors?",
        "context": "Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What does the device ordinal allow for?": {
        "answer": "fast prototyping of code",
        "question": "What does the device ordinal allow for?",
        "context": "Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is an object that represents the memory layout of atorch.Tensor?": {
        "answer": "A torch.layoutis",
        "question": "What is an object that represents the memory layout of atorch.Tensor?",
        "context": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What do strided tensors provide?": {
        "answer": "multi-dimensional,stridedview of a storage",
        "question": "What do strided tensors provide?",
        "context": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What represents the jump in the memory necessary to go from one element to the next in the k-th dimension of the Tensor?": {
        "answer": "k-th stride",
        "question": "What represents the jump in the memory necessary to go from one element to the next in the k-th dimension of the Tensor?",
        "context": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What does the k-th stride make it possible to do?": {
        "answer": "perform many tensor operations efficiently",
        "question": "What does the k-th stride make it possible to do?",
        "context": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is an object representing the memory format on which atorch.Tensoris or will be allocated?": {
        "answer": "A torch.memory_format is",
        "question": "What is an object representing the memory format on which atorch.Tensoris or will be allocated?",
        "context": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What are the possible values atorch.memory_format?": {
        "answer": "torch.contiguous_format",
        "question": "What are the possible values atorch.memory_format?",
        "context": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What type of order are values represented by?": {
        "answer": "decreasing order",
        "question": "What type of order are values represented by?",
        "context": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the tensor that is or will be allocated in dense non-overlapping memory?": {
        "answer": "torch.channels_last",
        "question": "What is the name of the tensor that is or will be allocated in dense non-overlapping memory?",
        "context": "torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "Where is Tensor allocated?": {
        "answer": "dense non-overlapping memory",
        "question": "Where is Tensor allocated?",
        "context": "torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What order are atorch.Tensoris represented by?": {
        "answer": "decreasing",
        "question": "What order are atorch.Tensoris represented by?",
        "context": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What are the possible values of torch.contiguous_format?": {
        "answer": "dense non-overlapping memory",
        "question": "What are the possible values of torch.contiguous_format?",
        "context": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the order in which tensor is or will be allocated in dense non-overlapping memory?": {
        "answer": "NHWC",
        "question": "What is the name of the order in which tensor is or will be allocated in dense non-overlapping memory?",
        "context": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How many bits is a short torch?": {
        "answer": "16",
        "question": "How many bits is a short torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How do we promote when the dtypes of inputs to an arithmetic operation differ?": {
        "answer": "by finding the minimum dtype that satisfies the following rules",
        "question": "How do we promote when the dtypes of inputs to an arithmetic operation differ?",
        "context": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What property returnsTrue if the data type is a floating point data type?": {
        "answer": "propertyis_floating_pointcan be used",
        "question": "What property returnsTrue if the data type is a floating point data type?",
        "context": "torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What property returnsTrue if the data type is a complex data type?": {
        "answer": "property is_complex can be used",
        "question": "What property returnsTrue if the data type is a complex data type?",
        "context": "torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is important when Brain Floating Point has the same number of exponent bits asfloat32?": {
        "answer": "range",
        "question": "What is important when Brain Floating Point has the same number of exponent bits asfloat32?",
        "context": "torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "When the dtypes of inputs to an operation (add,sub,div,mul) differ, we promote by finding the minimum d": {
        "answer": "arithmetic",
        "question": "When the dtypes of inputs to an operation (add,sub,div,mul) differ, we promote by finding the minimum d",
        "context": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the term used to describe the use of 1 sign, 5 exponent, and 10 significand bits?": {
        "answer": "binary16",
        "question": "What is the term used to describe the use of 1 sign, 5 exponent, and 10 significand bits?",
        "context": "torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What does get_device() do?": {
        "answer": "returns an ordinal for cuda tensors",
        "question": "What does get_device() do?",
        "context": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "Where is atorch.Tensor allocated?": {
        "answer": "dense non-overlapping memory",
        "question": "Where is atorch.Tensor allocated?",
        "context": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What order are atorch.tensoris strides represented by?": {
        "answer": "decreasing",
        "question": "What order are atorch.tensoris strides represented by?",
        "context": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is used in functions likecloneto preserve the memory format of the input tensor?": {
        "answer": "torch.preserve_format",
        "question": "What is used in functions likecloneto preserve the memory format of the input tensor?",
        "context": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "When is 1 sign, 5 exponent, and 10 significand bits used?": {
        "answer": "when precision is important",
        "question": "When is 1 sign, 5 exponent, and 10 significand bits used?",
        "context": "Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What are some inputs to an arithmetic operation?": {
        "answer": "add,sub,div,mul",
        "question": "What are some inputs to an arithmetic operation?",
        "context": "Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "How many bits does a CharTensor have?": {
        "answer": "16-bit",
        "question": "How many bits does a CharTensor have?",
        "context": "torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the size of a ShortTensor?": {
        "answer": "32-bit",
        "question": "What is the size of a ShortTensor?",
        "context": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "IntTensor is what type of integer?": {
        "answer": "64-bit",
        "question": "IntTensor is what type of integer?",
        "context": "torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "64-bit integer torch.int64ortorch is what?": {
        "answer": "long torch",
        "question": "64-bit integer torch.int64ortorch is what?",
        "context": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "If there are no higher-category zero-dim operands, we promote to a type with sufficient what to hold all dimensione": {
        "answer": "size and category",
        "question": "If there are no higher-category zero-dim operands, we promote to a type with sufficient what to hold all dimensione",
        "context": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is the name of the program that does not inspect values when determining the minimumdtypesof an operand?": {
        "answer": "numpy",
        "question": "What is the name of the program that does not inspect values when determining the minimumdtypesof an operand?",
        "context": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What types are not yet supported?": {
        "answer": "Quantized and complex types",
        "question": "What types are not yet supported?",
        "context": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "We do not inspect values when determining the minimumdtypesof an operand. Quantized and complex types are not yet supported. Unlike what": {
        "answer": "numpy",
        "question": "We do not inspect values when determining the minimumdtypesof an operand. Quantized and complex types are not yet supported. Unlike what",
        "context": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What operand has a higher category than dimensioned operands?": {
        "answer": "zero-dimension tensor",
        "question": "What operand has a higher category than dimensioned operands?",
        "context": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "Promotion Examples:": {
        "answer": "An integral output tensor cannot accept a floating point tensor",
        "question": "Promotion Examples:",
        "context": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    "What is optional for the device type?": {
        "answer": "device ordinal",
        "question": "What is optional for the device type?",
        "context": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    }
}