[
    {
        "X": "What happens if the global deterministic flag is turned on?",
        "Z": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "Y": "Returns True"
    },
    {
        "X": "What documentation does the global deterministic flag turn on?",
        "Z": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "Y": "totorch.use_deterministic_algorithms()"
    },
    {
        "X": "Returns what if the global deterministic flag is turned on?",
        "Z": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "Y": "True"
    },
    {
        "X": "What documentation does totorch.use_deterministic_algorithms() refer to for more details?",
        "Z": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "Y": "totorch.use_deterministic_algorithms()"
    },
    {
        "X": "What does torch.isfinite do?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "Y": "Returns a new tensor with boolean elements representing if each element is finite or not"
    },
    {
        "X": "What are finite when they are not NaN, negative infinity, or infinity?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "Y": "Real values"
    },
    {
        "X": "Complex values are what when both their real and imaginary parts are finite?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "Y": "finite"
    },
    {
        "X": "What are finite when both their real and imaginary parts are finite?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "Y": "Complex values"
    },
    {
        "X": "What type of distribution does fillselftensor with elements drawn from with torch.Tensor.geometric_?",
        "Z": "Fillsselftensor with elements drawn from the geometric distribution: ",
        "Y": "geometric distribution"
    },
    {
        "X": "In what direction does the flipud return a new tensor?",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "Y": "Flip the entries in each column in the up/down direction"
    },
    {
        "X": "What are preserved, but appear in a different order than before when flipud is used?",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "Y": "Rows"
    },
    {
        "X": "What is requirement on tensor when using flipud?",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "Y": "2-D"
    },
    {
        "X": "What is requirement on tensor when using fliplr?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "2-D"
    },
    {
        "X": "Does torch.flip make a copy of input's data?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flip makes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "Yes torch.flip makes a copy ofinput\u2019s data"
    },
    {
        "X": "What is the difference between torch.flipudis and numpy.flipud?",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "Y": "torch.flipud is slower than numpy.flipud"
    },
    {
        "X": "What does flip tensor in the up/down direction return?",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "Y": "torch.flipud"
    },
    {
        "X": "In which direction do the entries in each column flip?",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "Y": "up/down"
    },
    {
        "X": "What is required for the tensor to be at least?",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "Y": "1-D"
    },
    {
        "X": "What returns a view in constant time?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "NumPy\u2019snp.flip"
    },
    {
        "X": "Why is copying a tensor's data slower than viewing that data?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "more work"
    },
    {
        "X": "How is the tensor's dtype inferred with torch.full?",
        "Z": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "Y": "from fill_value"
    },
    {
        "X": "What is the value to fill the output tensor with torch.full?",
        "Z": "fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "Y": "fill_value(Scalar)"
    },
    {
        "X": "What is the desired data type of returned tensor?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted to dtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "returned tensor is casted to dtype, if specified"
    },
    {
        "X": "What is the default for a global default?",
        "Z": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (see torch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "Y": "ifNone"
    },
    {
        "X": "What  type of tensor is created with torch.full?",
        "Z": "Creates a tensor of size filled with fill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "Y": "a tensor of sizesizefilled withfill_value"
    },
    {
        "X": "What is inferred from fill_value?",
        "Z": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "Y": "datatype"
    },
    {
        "X": "What defines the shape of the output tensor with torch.ones?",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "Y": "a sequence of integers"
    },
    {
        "X": "How is the desired layout of returned Tensor with torch.ones?",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "Y": "with torch.layout"
    },
    {
        "X": "What is the name of the layout of returned Tensor with torch.ones?",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "Y": "Default:torch.strided"
    },
    {
        "X": "What is out(Tensor,optional)?",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "Y": "output tensor"
    },
    {
        "X": "What is the desired data type of returned Tensor with torch.zeros_like?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size as input.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "dtype, if specified"
    },
    {
        "X": "What is the default layout of returned Tensor with torch.zeros_like?",
        "Z": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "Y": "Default:torch.strided"
    },
    {
        "X": "How are numbers sampled from the continuous uniform distribution?",
        "Z": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "Y": "using torch.Tensor.uniform_"
    },
    {
        "X": "What distribution does fillselftensor with numbers sampled from?",
        "Z": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "Y": "uniform"
    },
    {
        "X": "What is filled with numbers sampled from the continuous uniform distribution?",
        "Z": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "Y": "self.tensor"
    },
    {
        "X": "What is the current state of the generator?",
        "Z": "Gets the current device of the generator. Example: Returns the Generator state as a torch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "a torch.ByteTensor"
    },
    {
        "X": "What is the current device of the generator state?",
        "Z": "Gets the current device of the generator. Example: Returns the Generator state as a torch.ByteTensor. A torch.ByteTensor which contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "atorch.ByteTensor"
    },
    {
        "X": "What contains all the necessary bits to restore a Generator to a specific point in time?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "Tensor"
    },
    {
        "X": "What is the name of the first seed for generating random numbers?",
        "Z": "Example: Returns the Generator state as a torch.ByteTensor. A torch.ByteTensor which contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "initial seed"
    },
    {
        "X": "What is the name of a seed that has a good balance of 0 and 1 bits in the seed?",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "a torch.Generator object"
    },
    {
        "X": "What is atorch.Generatorobject recommended to set?",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "a large seed"
    },
    {
        "X": "What is a good balance of a large seed?",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "0 and 1 bits"
    },
    {
        "X": "What does atorch.ByteTensor get?",
        "Z": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "current device"
    },
    {
        "X": "What does atorch.Generatorobject do?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "Sets the seed for generating random numbers"
    },
    {
        "X": "What returns the seed for generating random numbers?",
        "Z": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "Y": "a torch.Generator object"
    },
    {
        "X": "What is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "Avoid having many 0 bits in the seed"
    },
    {
        "X": "What is the pupose of initial seed for generating random numbers?",
        "Z": "Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "Sets the seed for generating random numbers"
    },
    {
        "X": "What is the Generator state returned as?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "a torch.ByteTensor"
    },
    {
        "X": "What Sets the seed for generating random numbers return?",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "a torch.Generator object"
    },
    {
        "X": "What should you do when setting a large seed?",
        "Z": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "Y": "Avoid having many 0 bits in the seed"
    },
    {
        "X": "What is an example of a seed that contains all the necessary bits to restore a Generator to a specific point in time?",
        "Z": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "Tensor"
    },
    {
        "X": "What returns the initial seed for generating random numbers?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "a torch.Generator object"
    },
    {
        "X": "What does Atorch.ByteTensor do?",
        "Z": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "Sets the seed for generating random numbers"
    },
    {
        "X": "What is recommended to set a number that has a good balance of 0 and 1 bits?",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "large seed"
    },
    {
        "X": "What  returns the initial seed for generating random numbers?",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "a torch.Generator object"
    },
    {
        "X": "What is the initial seed for generating random numbers?",
        "Z": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "Y": "Pythonlong"
    },
    {
        "X": "What does atorch.Generatorobject return for generating random numbers?",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "Sets the seed"
    },
    {
        "X": "What should you avoid in the seed?",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "Avoid having many 0 bits"
    },
    {
        "X": "What does atorch.Generatorobject mean?",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "Sets the seed for generating random numbers"
    },
    {
        "X": "What does it return for generating random numbers?",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "Y": "Returns the initial seed"
    },
    {
        "X": "What is the desired seed?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "seed(int)"
    },
    {
        "X": "Value must be within what range?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "inclusive range"
    },
    {
        "X": "What is raised if the value is not within the inclusive range?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "RuntimeError"
    },
    {
        "X": "What inputs are remapped to positive values with the formula0xfff_fff_fff_ffff",
        "Z": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "Y": "Negative inputs"
    },
    {
        "X": "What is the object that sets the seed for generating random numbers?",
        "Z": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "Y": "torch.Generator"
    },
    {
        "X": "What is an example of a torch.Generator object?",
        "Z": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: ",
        "Y": "Generator"
    },
    {
        "X": "What is returned if inference mode is enabled?",
        "Z": "Returns True if inference mode is currently enabled. ",
        "Y": "True"
    },
    {
        "X": "Which correction will be used to calculate the variance?",
        "Z": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "Y": "Bessel correction"
    },
    {
        "X": "What is calculated if Bessel's correction is true?",
        "Z": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "Y": "the sample variance"
    },
    {
        "X": "What is reduced with torch.var_mean?",
        "Z": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "Y": "dimension"
    },
    {
        "X": "What type of correction is used to calculate the variance?",
        "Z": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "Y": "when unbiased is true"
    },
    {
        "X": "How to specify that output tensor should retain the dimension or not?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "by specifying keepdim(bool)"
    },
    {
        "X": "What is the name of correction that will be used to calculate the variance?",
        "Z": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "Y": "Bessel's correction"
    },
    {
        "X": "If unbiasedisTrue, what is calculated, without any correction?",
        "Z": "out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). A tuple (var, mean) containing the variance and mean. Example: ",
        "Y": "the sample deviation"
    },
    {
        "X": "What is unbiased(bool)?",
        "Z": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "Y": "whether to use Bessel\u2019s correction"
    },
    {
        "X": "What is the name of the function that determines whether the output tensor has dim retained or not?",
        "Z": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "Y": "keepdim"
    },
    {
        "X": "What does the tuple contain when using torch.var_mean?",
        "Z": "out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). A tuple (var, mean) containing the variance and mean. Example: ",
        "Y": "variance and mean"
    },
    {
        "X": "What calculates the variance and mean of all elements in theinputtensor?",
        "Z": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "Y": "torch.var_mean Calculates the variance and mean of all elements in theinputtensor"
    },
    {
        "X": "What does unbiased(bool) mean?",
        "Z": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "Y": "whether to use Bessel\u2019s correction"
    },
    {
        "X": "What is calculated by a tuple containing the variance and mean of all elements in the inputtensor with torch.var_mean?",
        "Z": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "Y": "the variance and mean of all elements in theinputtensor with torch.var_mean"
    },
    {
        "X": "What is a tuple that calculates the variance and mean of all elements in?",
        "Z": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. ",
        "Y": "the input tensor"
    },
    {
        "X": "When is Bessel's correction applied",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "If unbiased is True"
    },
    {
        "X": "IfunbiasedisTrue, Bessel\u2019s correction will be used?",
        "Z": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "Y": "the sample deviation"
    },
    {
        "X": "If funbiasedisTrue, what will be used?",
        "Z": "out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). A tuple (var, mean) containing the variance and mean. Example: ",
        "Y": "Bessel's correction is used"
    },
    {
        "X": "What happens if Bessel's correction is not used?",
        "Z": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "Y": "the sample deviation is calculated, without any correction"
    },
    {
        "X": "What is used to determine whether to use Bessel's correction?",
        "Z": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "Y": "unbiased as True"
    },
    {
        "X": "What is a contiguous, one-dimensional array of a single data type?",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "A torch.Storage is"
    },
    {
        "X": "Who has a corresponding storage of the same data type?",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "Y": "Every torch.Tensor"
    },
    {
        "X": "What type of type does Everytorch.Tensor have a corresponding storage of the same data type?",
        "Z": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "bfloat16"
    },
    {
        "X": "What has a corresponding storage of the same data type?",
        "Z": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. ",
        "Y": "Everytorch.Tensor"
    },
    {
        "X": "What type of storage does Everytorch.Tensor cast?",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "Y": "bfloat16"
    },
    {
        "X": "What type of storage does Everytorch.Tensor return a copy of?",
        "Z": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "Y": "char"
    },
    {
        "X": "If the object is already in CUDA memory and on the correct device, what is performed and the original object is returned?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "no copy"
    },
    {
        "X": "What happens if the object is already in CUDA memory and on the correct device?",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "no copy is performed and the original object is returned"
    },
    {
        "X": "What type of type returns a copy of this storage?",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "char"
    },
    {
        "X": "What is device(int) called?",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "destination GPU id"
    },
    {
        "X": "What type of storage does Casts this storage to byte type Return?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "a copy"
    },
    {
        "X": "If the object is already in CUDA memory and on the correct device, what happens?",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. ",
        "Y": "no copy is performed"
    },
    {
        "X": "What is the destination GPU id?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "current device"
    },
    {
        "X": "What does Casts this storage return?",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "a copy"
    },
    {
        "X": "What does a copy of this storage return if it\u2019s not already on the CPU?",
        "Z": "Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "a CPU copy"
    },
    {
        "X": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of what type of storage?",
        "Z": "Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "float type"
    },
    {
        "X": "What type of type returns a CPU copy of this storage if it\u2019s not already on the CPU?",
        "Z": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "float"
    },
    {
        "X": "What is the device(int)?",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "destination GPU id"
    },
    {
        "X": "Casts this storage to complex what type?",
        "Z": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "float"
    },
    {
        "X": "What does the float type return if it\u2019s not already on the CPU?",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "a CPU copy"
    },
    {
        "X": "Where is a copy of this object stored?",
        "Z": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. ",
        "Y": "CUDA memory"
    },
    {
        "X": "If the object is already in CUDA memory and on the correct device, what is performed?",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. ",
        "Y": "no copy"
    },
    {
        "X": "The destination GPU id Defaults to what?",
        "Z": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. ",
        "Y": "current device"
    },
    {
        "X": "What happens to a CPU copy of this storage?",
        "Z": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. ",
        "Y": "if it\u2019s not already on the CPU"
    },
    {
        "X": "IfTrueand the source is in pinned memory, the copy will be what?",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "asynchronous"
    },
    {
        "X": "What has no effect on the copy of a CUDA memory?",
        "Z": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. ",
        "Y": "argument"
    },
    {
        "X": "What if true and the source is in pinned memory, the copy will be asynchronous with respect to the host?",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "Y": "non_blocking(bool)"
    },
    {
        "X": "If the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, what happens?",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "the argument has no effect"
    },
    {
        "X": "Where does a copy of the object return?",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "Y": "CUDA memory"
    },
    {
        "X": "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host?",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "the argument has no effect"
    },
    {
        "X": "**kwargs\u2013 For compatibility, may contain what?",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "key a sync in place of the non_blocking argument"
    },
    {
        "X": "Where does the object return a copy of?",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "Y": "CUDA memory"
    },
    {
        "X": "What does device(int) default to?",
        "Z": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type ",
        "Y": "current device"
    },
    {
        "X": "For compatibility, may contain what place of thenon_blockingargument?",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "keyasyncin"
    },
    {
        "X": "If the object is already in what memory, then no copy is performed and the original object is returned?",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "Y": "CUDA memory"
    },
    {
        "X": "What is the keyasyncin place of the keyasyncin place of thenon_blockingargument?",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "Y": "double type"
    },
    {
        "X": "If the object is already in what?",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "Y": "CUDA memory"
    },
    {
        "X": "What type of storage does the keyasyncin cast?",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "Y": "double type"
    },
    {
        "X": "What is the keyasyncin place of thenon_blockingargument?",
        "Z": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "keyasyncin place of thenon_blockingargument"
    },
    {
        "X": "What does bool stand for?",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "non_blocking"
    },
    {
        "X": "What type of storage does double type cast?",
        "Z": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "float type"
    },
    {
        "X": "What effect does the argument have ifTrueand the source is in pinned memory?",
        "Z": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "no effect"
    },
    {
        "X": "What is the float type of the storage?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "IfsharedisTrue"
    },
    {
        "X": "How are changes written to the file?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "All changes are written to the file"
    },
    {
        "X": "What does the changes on the storage do not affect the file?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "What type of argument has no effect if the source is in pinned memory?",
        "Z": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "non_blocking(bool)"
    },
    {
        "X": "What type of storage is cast to double type?",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "Y": "float type"
    },
    {
        "X": "What happens when memory is shared between all processes?",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "All changes are written to the file"
    },
    {
        "X": "If the changes on the storage do not affect the file, what is the default?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "What does **kwargs contain for compatibility?",
        "Z": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "keyasyncin place of thenon_blockingargument"
    },
    {
        "X": "What is the name of the storage that can be used to float?",
        "Z": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "IfsharedisTrue"
    },
    {
        "X": "What type of storage is casts this storage to?",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "float type"
    },
    {
        "X": "What happens when a storage is cast to float type IfsharedisTrue?",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "Y": "memory is shared between all processes"
    },
    {
        "X": "What is the number of elements in the storage?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "sizeis the number of elements in the storage"
    },
    {
        "X": "IfsharedisFalse, the file must contain what?",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. ",
        "Y": "at leastsize * sizeof(Type)bytes"
    },
    {
        "X": "What file will be created if needed?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "IfsharedisTruethe file will be created if needed"
    },
    {
        "X": "What type of storage is casts?",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map ",
        "Y": "float type"
    },
    {
        "X": "If the changes on the storage do not affect the file, what is the name of the type of storage?",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "What does filename(str) - file name to map?",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map ",
        "Y": "filename(str) \u2013 file name to map"
    },
    {
        "X": "What is the name of the file that is shared between all processes?",
        "Z": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "All changes are written to the file"
    },
    {
        "X": "What is the file name to map shared(bool)?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "share memory size(int)"
    },
    {
        "X": "What happens when a float type is shared between all processes?",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "Y": "All changes are written to the file"
    },
    {
        "X": "What does filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory?",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "Y": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory"
    },
    {
        "X": "What is shared between all processes?",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "Y": "memory"
    },
    {
        "X": "What does filename(str) \u2013 file name to map shared(bool)?",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "Y": "filename(str) \u2013 file name to map shared(bool)"
    },
    {
        "X": "Copies the storage to what if it's not already pinned?",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "pinned memory"
    },
    {
        "X": "What is the filename of the storage that is not already pinned?",
        "Z": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self ",
        "Y": "Copies the storage to pinned memory"
    },
    {
        "X": "What is a no-op for storages already in shared memory and CUDA storages?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "Storages in shared memory cannot be resized"
    },
    {
        "X": "What is a no-op for storages already in shared memory?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "CUDA storages"
    },
    {
        "X": "What is the return of storages in shared memory?",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "Y": "self Casts this storage to short type"
    },
    {
        "X": "What is a no-op for storages already in shared memory and for CUDA storages?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "Storages in shared memory cannot be resized"
    },
    {
        "X": "What storage does not need to be moved for sharing across processes?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "Y": "CUDA"
    },
    {
        "X": "What type of return does CUDA return?",
        "Z": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self ",
        "Y": "self"
    },
    {
        "X": "What type of storage does self Casts this storage to long type?",
        "Z": "shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "Y": "Copies the storage to pinned memory"
    },
    {
        "X": "What does this no-op do for storages already in shared memory and for CUDA storages?",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "Y": "Moves the storage to shared memory"
    },
    {
        "X": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what storage?",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "Y": "CUDA"
    },
    {
        "X": "What is the size of the storage Casts this storage to int type Casts this storage to long type?",
        "Z": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "Y": "Copies the storage to pinned memory"
    },
    {
        "X": "What is the name of the number of elements in a storage?",
        "Z": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "Y": "size(int)"
    },
    {
        "X": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what type of storage?",
        "Z": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "Y": "CUDA"
    },
    {
        "X": "What cannot be resized?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "Storages in shared memory"
    },
    {
        "X": "What does self Cast this storage to short type Returns a list containing the elements of this storage?",
        "Z": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "Y": "if dtype is not provided"
    },
    {
        "X": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, otherwise casts this object to the",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "self Casts this storage to short type"
    },
    {
        "X": "If this is already of the correct type, what is performed and the original object is returned?",
        "Z": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "no copy"
    },
    {
        "X": "If this is already of the correct type, no copy is performed and what is returned?",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "the original object is returned"
    },
    {
        "X": "If the object is already of the correct type, what is performed and the original object is returned?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "no copy"
    },
    {
        "X": "What is the desired type non_blocking(bool)?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "dtype(typeorstring)"
    },
    {
        "X": "What has no effect if the copy is performed asynchronously with respect to the host?",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "Y": "argument has no effect"
    },
    {
        "X": "What is the name of the type non_blocking(bool)?",
        "Z": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "Y": "dtype(typeorstring)"
    },
    {
        "X": "What happens if the copy is performed asynchronously with respect to the host?",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "the argument has no effect"
    },
    {
        "X": "What is the type that returns the object to the specified type?",
        "Z": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "Y": "ifdtypeis not provided"
    },
    {
        "X": "What effect does the argument have if the source is in pinned memory and destination is on the GPU or vice versa?",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "Y": "no effect"
    },
    {
        "X": "If true how copy is performed what with respect to the host?",
        "Z": "dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type ",
        "Y": "asynchronously"
    },
    {
        "X": "What is performed if the original object is returned?",
        "Z": "If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "Y": "no copy"
    },
    {
        "X": "What does the copy perform asynchronously with respect to the host?",
        "Z": "If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "Y": "argument has no effect"
    },
    {
        "X": "What type of storage is the async arg deprecated?",
        "Z": "Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type ",
        "Y": "bfloat16"
    },
    {
        "X": "If no copy is performed and the original object is returned, what happens?",
        "Z": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "Y": "If this is already of the correct type"
    },
    {
        "X": "What type of storage does Theasyncarg cast this storage to?",
        "Z": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type ",
        "Y": "bfloat16 type"
    },
    {
        "X": "What type of type Casts this storage to bfloat16 type?",
        "Z": "dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type ",
        "Y": "byte"
    },
    {
        "X": "IfTrue and the source are in pinned memory and destination is on the GPU or vice versa, how is copy performed?",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "asynchronously"
    },
    {
        "X": "What effect does the argument have ifTrue and the source is in pinned memory and destination is on the GPU or vice versa?",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "Y": "no effect"
    },
    {
        "X": "What type of type does this storage have?",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "char"
    },
    {
        "X": "What is the name of the function that performs asynchronously with respect to the host?",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "non_blocking(bool)"
    },
    {
        "X": "What type of storage does the keyasyncin cast this storage to?",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "bfloat16 type"
    },
    {
        "X": "What type of type does this storage go to?",
        "Z": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "Y": "bfloat16"
    },
    {
        "X": "What type of storage does the keyasyncarg cast this storage to?",
        "Z": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "Y": "bfloat16"
    },
    {
        "X": "What part of a complex tensor is equal to real?",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "Y": "real"
    },
    {
        "X": "What is the real part of the complex tensor?",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "Y": "real(Tensor)"
    },
    {
        "X": "Real(Tensor) \u2013 The real part of the complex tensor. Must be what?",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "Y": "float or double"
    },
    {
        "X": "What is the imaginary part of the complex tensor?",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "Y": "imag"
    },
    {
        "X": "What must the imag(Tensor) be?",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "Y": "same dtype asreal"
    },
    {
        "X": "If the inputs are torch.float64, ouput tensor must be what?",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example: ",
        "Y": "be torch.complex128"
    },
    {
        "X": "What does solve AX = b assume A to be?",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "Y": "upper-triangular"
    },
    {
        "X": "What can torch.triangular_solve take in?",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "Y": "2D inputs A and b"
    },
    {
        "X": "If the inputs are batches, what does X support input of float, double, cfloat and cdouble data types?",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "Y": "batched outputs"
    },
    {
        "X": "What does solve AX = b  solve?",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "Y": "a system of equations with a triangular coefficient matrix"
    },
    {
        "X": "What does solve solve AX = b  assume A is?",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "upper-triangular"
    },
    {
        "X": "What can torch.triangular_solve(b, A) take in?",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "2D inputs"
    },
    {
        "X": "What does torch.triangular_solve(b, A) return if the inputs are batches?",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "batched outputs"
    },
    {
        "X": "What does torch.triangular_solve take in?",
        "Z": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "Y": "2D inputsb"
    },
    {
        "X": "What does the torch.triangular_solve return if the inputs are batches?",
        "Z": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions ",
        "Y": "batched outputs"
    },
    {
        "X": "What is the name for multiple right-hand sides of size(,m,k)(*, m, k)(,",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "Y": "b(Tensor)"
    },
    {
        "X": "What supports input of float, double, cfloat and cdouble data types?",
        "Z": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "Y": "batched outputs X"
    },
    {
        "X": "What does torch.triangular_solve return if the inputs are batches?",
        "Z": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "Y": "batched outputs"
    },
    {
        "X": "What is the name of the input triangular coefficient matrix of size(,m,k)(*, m, k)(",
        "Z": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "Y": "b(Tensor)"
    },
    {
        "X": "What is the name of the multiple right-hand sides of size(,m,k)(*, m, k)(",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "Y": "b(Tensor)"
    },
    {
        "X": "What is the default for the lower-triangular system of equations?",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "Y": "True"
    },
    {
        "X": "Supports input of float, double, cfloat and what other data type?",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "Y": "cdouble"
    },
    {
        "X": "What is the default value for the upper-triangular system of equations?",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "Y": "True"
    },
    {
        "X": "What is the multiple right-hand sides of size(,m,k)(*, m, k)(,m,",
        "Z": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "b(Tensor)"
    },
    {
        "X": "What is the default for transpose?",
        "Z": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "True"
    },
    {
        "X": "What is the default for a b(Tensor)?",
        "Z": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "False"
    },
    {
        "X": "What is the input triangular coefficient matrix of size(,m,m)(*, m, k)(,m",
        "Z": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "b(Tensor)"
    },
    {
        "X": "What is transpose(bool,optional)?",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "Y": "whether A should be transposed before being sent into the solver"
    },
    {
        "X": "What is the default value of transpose(bool,optional)?",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "False"
    },
    {
        "X": "What is the input triangular coefficient matrix of size?",
        "Z": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "Y": "A(Tensor)"
    },
    {
        "X": "What is the default matrix system of equations?",
        "Z": "upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. ",
        "Y": "upper triangular"
    },
    {
        "X": "What is the default value for transpose?",
        "Z": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "Y": "Default:False"
    },
    {
        "X": "What is the default value of unitriangular(bool,optional)?",
        "Z": "upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. ",
        "Y": "Default:False"
    },
    {
        "X": "If True, the diagonal elements of A are assumed to be 1 and not referenced from A..",
        "Z": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "Y": "Default:False"
    },
    {
        "X": "What is a solution, cloned_coefficient?",
        "Z": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples: ",
        "Y": "namedtuple"
    },
    {
        "X": "What is a tensor filled with?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "uninitialized data"
    },
    {
        "X": "What is a sequence of integers defining the shape of the output tensor?",
        "Z": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "Y": "size(int...)"
    },
    {
        "X": "What can be a variable number of arguments?",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "Y": "a collection like a list or tuple"
    },
    {
        "X": "What is out a variable number of arguments or a collection like a list or tuple?",
        "Z": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "Y": "output tensor"
    },
    {
        "X": "What module is modeled after SciPy'sspecialmodule?",
        "Z": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "Y": "The torch.special module"
    },
    {
        "X": "What is the torch.special module modeled after?",
        "Z": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "Y": "SciPy"
    },
    {
        "X": "What does the torch.special module compute?",
        "Z": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "Y": "error function ofinput"
    },
    {
        "X": "What is the error function defined as?",
        "Z": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "Y": "input(Tensor) \u2013 the input tensor"
    },
    {
        "X": "What is still being added to the torch.special module?",
        "Z": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "Y": "New functions"
    },
    {
        "X": "What is the error function of input defined as?",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. ",
        "Y": "input(Tensor) \u2013 the input tensor"
    },
    {
        "X": "What is out(Tensor,optional) defined as?",
        "Z": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. ",
        "Y": "output tensor"
    },
    {
        "X": "What is an example of a PyTorch module?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "Example:"
    },
    {
        "X": "What is the function that computes the error function of input?",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. ",
        "Y": "torch.special.expit Computes the error function ofinput"
    },
    {
        "X": "What is the error function of input do?",
        "Z": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "Y": "Computes the complementary error function ofinput"
    },
    {
        "X": "What is the complementary error function of input do?",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "Computes the complementary error function ofinput"
    },
    {
        "X": "What is the error function defined as: input(Tensor) \u2013 the input tensor. out(Tensor,",
        "Z": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. ",
        "Y": "Computes the error function ofinput"
    },
    {
        "X": "What is the complementary error function defined as: input(Tensor) \u2013 the input tensor?",
        "Z": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "Y": "output tensor"
    },
    {
        "X": "What is the complementary error function defined as?",
        "Z": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "Y": "input(Tensor) \u2013 the input tensor"
    },
    {
        "X": "What error function is defined in the range(1,1)(-1,1)(1,1)?",
        "Z": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. ",
        "Y": "inverse error function"
    },
    {
        "X": "What is defined in the range(1,1)(-1, 1)(1,1)?",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "inverse error function"
    },
    {
        "X": "What is the logistic sigmoid function?",
        "Z": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "Y": "the expit"
    },
    {
        "X": "What error function is defined in the range(1,1)(-1, 1)(1,1)?",
        "Z": "Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "Y": "inverse error function"
    },
    {
        "X": "What is the expit also known as?",
        "Z": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "Y": "the logistic sigmoid function"
    },
    {
        "X": "Computes the expit also known as what?",
        "Z": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "Y": "the logistic sigmoid function"
    },
    {
        "X": "What is the natural value of the gamma function oninput?",
        "Z": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "Y": "logarithm"
    },
    {
        "X": "What is the first kind of the first kind ofinput?",
        "Z": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "Y": "exponentially scaled zeroth order modified Bessel function"
    },
    {
        "X": "Computes the exponentially scaled zeroth order modified Bessel function of what kind?",
        "Z": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "Y": "first kind"
    },
    {
        "X": "What is the base two exponential function ofinput?",
        "Z": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "Y": "Computes the base two exponential function ofinput"
    },
    {
        "X": "What is the absolute value of the gamma function oninput?",
        "Z": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "Y": "natural logarithm"
    },
    {
        "X": "What is the order modified Bessel function of the first kind?",
        "Z": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "Y": "zeroth"
    },
    {
        "X": "What is the exponentially scaled zeroth order modified Bessel function of?",
        "Z": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. ",
        "Y": "first kind"
    },
    {
        "X": "What is the first kind of the input tensor?",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "Y": "exponentially scaled zeroth order modified Bessel function"
    },
    {
        "X": "What is computed for each element of input with torch.special.expit?",
        "Z": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "Y": "exponentially scaled zeroth order modified Bessel function"
    },
    {
        "X": "What is the first kind of gamma function oninput?",
        "Z": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "Y": "exponentially scaled zeroth order modified Bessel function"
    },
    {
        "X": "What is the result of the natural logarithm of the absolute value of the gamma function oninput?",
        "Z": "Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "Y": "Computes the natural logarithm of the absolute value of the gamma function oninput"
    },
    {
        "X": "What is the logit of the elements ofinput.inputis clamped to when eps is not None?",
        "Z": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "Y": "eps"
    },
    {
        "X": "When eps is None andinput 0 orinput> 1, the function will yield what?",
        "Z": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "Y": "NaN"
    },
    {
        "X": "What is the epsilon for input clamp bound?",
        "Z": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "Y": "float"
    },
    {
        "X": "What is another example of the output tensor?",
        "Z": "out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "Y": "Computesinput*log1p(other)with the following cases"
    },
    {
        "X": "What is sscipy.special.xlog1py similar to?",
        "Z": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "Y": "scipy.special of SciPy "
    },
    {
        "X": "What is a tensor whose shape isbroadcastablewith the first argument?",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "Y": "Computesinput"
    },
    {
        "X": "What is a boolean tensor that is True whereinputis greater thanotherand False elsewhere?",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "Y": "output tensor"
    },
    {
        "X": "What does input return with each of the elements of inputrounded to the closest integer?",
        "Z": "Returns a new tensor with each of the elements ofinputrounded\nto the closest integer. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "Returns a new tensor"
    },
    {
        "X": "What does PCA perform on a low-rank matrix?",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "Y": "linear Principal Component Analysis"
    },
    {
        "X": "What does U,S,V return?",
        "Z": "This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. ",
        "Y": "namedtuple"
    },
    {
        "X": "What represents the principal directions of PCA?",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions ",
        "Y": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns"
    },
    {
        "X": "What does PCA stand for?",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions ",
        "Y": "Principal Component Analysis"
    },
    {
        "X": "What is the nearly optimal approximation of a singular value decomposition of a centered matrix?",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "Y": "namedtuple(U,S,V)"
    },
    {
        "X": "What do theVVVcolumns represent?",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "Y": "the principal directions"
    },
    {
        "X": "What is the relation of the data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S",
        "Z": "This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. ",
        "Y": "AAAis"
    },
    {
        "X": "What is the nearly optimal approximation of a singular value decomposition of a centered matrixAAA?",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "Y": "namedtuple(U,S,V)"
    },
    {
        "X": "What represents the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont",
        "Z": "The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix ",
        "Y": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns"
    },
    {
        "X": "What is the size of returned matrices?",
        "Z": "The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix ",
        "Y": "UUUis m x q matrix"
    },
    {
        "X": "What is a data matrix withmsamples andnfeatures theVVVcolumns?",
        "Z": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "Y": "AAAis"
    },
    {
        "X": "matmul(A,V[:,:k])projects data to what?",
        "Z": "Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "Y": "first k principal components"
    },
    {
        "X": "What represent the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont",
        "Z": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "Y": "theVVVcolumns"
    },
    {
        "X": "What represent the principal directions?",
        "Z": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "Y": "theVVVcolumns"
    },
    {
        "X": "What does S2/(m1)S ** 2 / (m - 1)S2/(m1)contains?",
        "Z": "Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "Y": "eigenvalues"
    },
    {
        "X": "What does the pseudorandom number generator do to obtain repeatable results?",
        "Z": "S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator ",
        "Y": "reset the seed"
    },
    {
        "X": "What is the default value of q?",
        "Z": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "Y": "min(6,m,n)"
    },
    {
        "X": "What is the input tensor of size(,m,n)(*, m, n)(,m,",
        "Z": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "Y": "A(Tensor)"
    },
    {
        "X": "By default, what is the center of the input tensor?",
        "Z": "Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. ",
        "Y": "By default,q=min(6,m,n)"
    },
    {
        "X": "By default, q=min(6,m,n). center(bool,optional) \u2013 if True, center the",
        "Z": "UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. ",
        "Y": "By default,q=min(6,m,n)"
    },
    {
        "X": "What is the number of subspace iterations to conduct?",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "Y": "specified by niter"
    },
    {
        "X": "By default, what is the seed for the pseudorandom number generator A(Tensor)?",
        "Z": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "Y": "By default,q=min(6,m,n)"
    },
    {
        "X": "What does the returned tensor andinputtensor share?",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "Y": "underlying storage"
    },
    {
        "X": "What is the starting dimension length of the tensor?",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "Y": "the distance to the ending dimension"
    },
    {
        "X": "Returns a new tensor that is what?",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "Y": "a narrowed version ofinputtensor"
    },
    {
        "X": "What is the name of the inputtensor that returns a new tensor that is a narrowed version ofinputten",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "Y": "dimensiondimis input fromstarttostart+length"
    },
    {
        "X": "What do the returned tensor andinputtensor share?",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "Y": "underlying storage"
    },
    {
        "X": "Input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow what",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "Y": "start"
    },
    {
        "X": "What function returns the minimum value of all elements in?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "theinputtensor"
    },
    {
        "X": "What is the value of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "Y": "a namedtuple"
    },
    {
        "X": "What is argmin?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "Y": "index location"
    },
    {
        "X": "What kind of (sub)gradients does this function produce?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "Y": "deterministic"
    },
    {
        "X": "Returns a namedtuple(values,indices) wherevaluesis the minimum value of each row of the inputtensor",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "Y": "wherevaluesis the minimum value of each row of theinputtensor in the given dimensiondim"
    },
    {
        "X": "What is the minimum value of each row of the inputtensor in the given dimensiondim?",
        "Z": "Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "Y": "index location"
    },
    {
        "X": "What type of subgradients does this function produce?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "deterministic"
    },
    {
        "X": "What is the value of each row of the inputtensor in the given dimensiondim?",
        "Z": "Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "Y": "the minimum value of each row of theinputtensor in the given dimensiondim"
    },
    {
        "X": "What does indices mean for each minimum value found?",
        "Z": "input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "Y": "index location"
    },
    {
        "X": "IfkeepdimisTrue, the output tensors have what?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "1 fewer dimension than input"
    },
    {
        "X": "What does the output tensors have the same size asinput?",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "If keepdim is True"
    },
    {
        "X": "If the output tensors are of the same size as input except in the dimensiondimwhere they are of size 1 what is the default",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "IfkeepdimisTrue"
    },
    {
        "X": "If keepdimisTrue, the output tensors are of the same size asinput except in the dimensiondim where they",
        "Z": "Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "Y": "1 fewer dimension"
    },
    {
        "X": "What happens if there are multiple minimal values in a reduced row?",
        "Z": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "the indices of the first minimal value are returned"
    },
    {
        "X": "What is the dimension to reduce?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "dim(int)"
    },
    {
        "X": "If the output tensors are of the same size as input, what is the default?",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "IfkeepdimisTrue"
    },
    {
        "X": "If keepdimisTrue, the output tensors are of the same size as input except in the dimensiondimwhere they are",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "1 fewer dimension"
    },
    {
        "X": "What are returned if there are multiple minimal values in a reduced row?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "the indices of the first minimal value"
    },
    {
        "X": "What determines whether the output tensor hasdimretained or not?",
        "Z": "IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "keepdim(bool)"
    },
    {
        "X": "Out(tuple,optional) \u2013 what is the tuple of two output tensors?",
        "Z": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "the tuple of two output tensors"
    },
    {
        "X": "What type of function does this function call LAPACK's geqrf directly?",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "Y": "low-level"
    },
    {
        "X": "What does this function return for geqrf?",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. ",
        "Y": "namedtuple"
    },
    {
        "X": "What is the function called for calling LAPACK's geqrf directly?",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. ",
        "Y": "low-level function"
    },
    {
        "X": "What are stored in the same output tensora?",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "Y": "Both Q and R matrices"
    },
    {
        "X": "Where are elements ofRare stored?",
        "Z": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "Y": "on and above the diagonal"
    },
    {
        "X": "Where is matrixQare stored?",
        "Z": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "Y": "below the diagonal"
    },
    {
        "X": "What does withtorch.ormqr() use for?",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "Y": "matrix-matrix multiplication"
    },
    {
        "X": "For more details, see the LAPACK documentation for what?",
        "Z": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "Y": "geqrf"
    },
    {
        "X": "What does thedriver=\"gels\"option use to solve matrix equations?",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "Y": "a QR decomposition"
    },
    {
        "X": "What is the output tuple of (Tensor, Tensor)?",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "Y": "input(Tensor)"
    },
    {
        "X": "What is Ignored by a function that can solve matrix equations using a QR decomposition?",
        "Z": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "Y": "ifNone is ignored"
    },
    {
        "X": "Where does each row containnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "tensor"
    },
    {
        "X": "What do rows ofinputdo not need to sum to one?",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "non-negative, finite and have a non-zero sum"
    },
    {
        "X": "What are ordered from left to right according to when each was sampled?",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "Indices"
    },
    {
        "X": "What is a vector of sizenum_samples?",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "If input is a vector"
    },
    {
        "X": "What is returned when each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "a tensor"
    },
    {
        "X": "What must the rows ofinput be?",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. ",
        "Y": "non-negative, finite and have a non-zero sum"
    },
    {
        "X": "Indices are ordered from left to right according to when each was sampled.",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "first samples are placed in first column"
    },
    {
        "X": "What are ordered from left to right according to when each sample was sampled?",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "Indices"
    },
    {
        "X": "Ifinputis a vector,outis a vector of what?",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "sizenum_samples"
    },
    {
        "X": "What is ifinputis a matrix of shape?",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "matrix with m rows"
    },
    {
        "X": "If what isTrue, samples are drawn with replacement?",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "Y": "replacement"
    },
    {
        "X": "How many rows of inputdo not need to sum to one?",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "Y": "rows ofinputdo not need to sum to one"
    },
    {
        "X": "Ifinputis a vector,outis a matrix of what?",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "Y": "matrix withmrows"
    },
    {
        "X": "If what isTrue, samples are drawn with what?",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "replacement"
    },
    {
        "X": "What do rows ofinputdo have to be?",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "Y": "non-negative, finite and have a non-zero sum"
    },
    {
        "X": "What must the rows of inputdo not need to sum to one?",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "Y": "non-negative, finite and have a non-zero sum"
    },
    {
        "X": "Ifinputis a matrix withmrows,outis a matrix of shape(mnum_samples)(m",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "Y": "Ifinputis a vector"
    },
    {
        "X": "Ifinputis a matrix,outis a matrix of what?",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "Y": "matrix withmrows"
    },
    {
        "X": "If what is true, samples are drawn with replacement?",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "replacement"
    },
    {
        "X": "What happens when a sample index is drawn for a row?",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "they are drawn without replacement"
    },
    {
        "X": "What is the name of a sample index that cannot be drawn again for a row?",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "Note"
    },
    {
        "X": "How are samples ordered from left to right?",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "first samples are placed in first column"
    },
    {
        "X": "Ifinputis a matrix,outis a matrix of shape(mnum_samples)(m times",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "matrix withmrows"
    },
    {
        "X": "If a sample index is drawn for what, it cannot be drawn again for that row?",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "a row"
    },
    {
        "X": "What is the difference between a vector and a matrix of sizenum_samples?",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "Note"
    },
    {
        "X": "What means that when a sample index is drawn for a row, it cannot be drawn again for that row?",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "they are drawn without replacement"
    },
    {
        "X": "What is outputis a vector of sizenum_samples?",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "Y": "Ifinputis a vector"
    },
    {
        "X": "What is the difference between samples drawn with replacement?",
        "Z": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "Y": "If replacement isTrue"
    },
    {
        "X": "What is the min number of non-zero elements in each row ofinputif it is a matrix?",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "number of non-zero elements in input"
    },
    {
        "X": "What is input tensor?",
        "Z": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "Y": "input tensor"
    },
    {
        "X": "If what is true, samples are drawn with replacement. If not, they are drawn without replacement?",
        "Z": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "Y": "If replacement isTrue"
    },
    {
        "X": "When a sample index is drawn without replacement, it cannot be drawn again for what row?",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "a row"
    },
    {
        "X": "If num_samples is lower than the min number of non-zero elements in each row of input, what is it?",
        "Z": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "matrix"
    },
    {
        "X": "What is the input tensor containing probabilities?",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "input(Tensor)"
    },
    {
        "X": "What is drawn for a row when it cannot be drawn again for that row?",
        "Z": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "a sample index"
    },
    {
        "X": "What is the input tensor containing probabilities num_samples?",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "Y": "input(Tensor)"
    },
    {
        "X": "num_samples must be lower than the min number of non-zero elements in each row of input if it is a",
        "Z": "Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "matrix"
    },
    {
        "X": "What is an example of a pseudorandom number generator?",
        "Z": "Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "Example"
    },
    {
        "X": "What does PyTorch use to represent neural networks?",
        "Z": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. ",
        "Y": "modules"
    },
    {
        "X": "What are the building blocks of stateful computation?",
        "Z": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. ",
        "Y": "Modules"
    },
    {
        "X": "How does PyTorch integrate with its autogradsystem?",
        "Z": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. ",
        "Y": "Tightly integrated"
    },
    {
        "X": "What type of networks can PyTorch modules allow for?",
        "Z": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "Y": "multi-layer neural networks"
    },
    {
        "X": "How does PyTorch'sautogradsystem integrate?",
        "Z": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "Y": "Tightly integrated"
    },
    {
        "X": "What are some of PyTorch's modules able to do?",
        "Z": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "Y": "prune, quantize"
    },
    {
        "X": "Modules make it simple to specify what for PyTorch\u2019s Optimizers to update?",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "Y": "learnable parameters"
    },
    {
        "X": "How are Modules easy to work with and transform?",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "Y": "Easy to work with and transform"
    },
    {
        "X": "What is PyTorch's Optimizers tightly integrated with?",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "Y": "PyTorch\u2019sautogradsystem"
    },
    {
        "X": "What are modules easy to work with and do?",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "Y": "transform"
    },
    {
        "X": "What is easy to work with and transform?",
        "Z": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "Y": "Easy to work with and transform"
    },
    {
        "X": "Why are modules so fundamental to PyTorch?",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "modules are so fundamental to PyTorch"
    },
    {
        "X": "What is a simple custom module module called?",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "Y": "Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features"
    },
    {
        "X": "How are modules to save and restore?",
        "Z": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State ",
        "Y": "straightforward"
    },
    {
        "X": "Who is this note intended for?",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "all PyTorch users"
    },
    {
        "X": "What are so fundamental to PyTorch?",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "modules"
    },
    {
        "X": "What are the Building Blocks of Neural Network Training?",
        "Z": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State ",
        "Y": "Modules Module State"
    },
    {
        "X": "What does this note describe?",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "Y": "modules"
    },
    {
        "X": "What is a Simple Custom Module Module Module as Building Blocks Neural Network Training with Modules?",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features ",
        "Y": "Module State Module Hooks Advanced Features"
    },
    {
        "X": "What type of training does A Simple Custom Module Modules provide?",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features ",
        "Y": "Neural Network Training"
    },
    {
        "X": "What module applies an affine transformation to its input?",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "PyTorch\u2019sLinearmodule"
    },
    {
        "X": "What does PyTorch'sLinearmodule apply to its input?",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "an affine transformation"
    },
    {
        "X": "What module inherits from the base Module class?",
        "Z": "This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "Y": "module"
    },
    {
        "X": "What are Modules as Building Blocks Neural Network Training with?",
        "Z": "Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "Module State Module Hooks Advanced Features"
    },
    {
        "X": "What basic characteristics does PyTorch'sLinearmodule have?",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "module has the following fundamental characteristics of modules"
    },
    {
        "X": "What are Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features?",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "Simple Custom Module Modules"
    },
    {
        "X": "What does PyTorch'sLinearmodule have?",
        "Z": "Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "Y": "Neural Network Training with Modules Module State Module Hooks Advanced Features"
    },
    {
        "X": "What is the name of the advanced features of Neural Network Training?",
        "Z": "Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "Y": "Neural Network Training with Modules Module State Module Hooks Advanced Features"
    },
    {
        "X": "What is a custom version of PyTorch'sLinearmodule?",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "Module Hooks Advanced Features"
    },
    {
        "X": "What are the basic features of PyTorch'sLinearmodule?",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "Y": "Module Hooks Advanced Features"
    },
    {
        "X": "What module uses an affine transformation to its input?",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "Y": "PyTorch\u2019sLinearmodule"
    },
    {
        "X": "What is a simple, custom version of PyTorch'sLinearmodule?",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "a simpler, custom version of PyTorch\u2019sLinearmodule"
    },
    {
        "X": "What should subclassModule for composability with other modules?",
        "Z": "It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. ",
        "Y": "module"
    },
    {
        "X": "What does the module define in computation?",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "Y": "state"
    },
    {
        "X": "What is each of the state defined as?",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "Y": "aParameter"
    },
    {
        "X": "What can be considered the \"learnable\" aspects of the module\u2019s computation?",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "Y": "Parameters"
    },
    {
        "X": "Parameters are not required to have what?",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "state"
    },
    {
        "X": "What do random-initializedweightandbiastensors define?",
        "Z": "This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "Y": "affine transformation"
    },
    {
        "X": "What is each of the randomly-initializedweightandbiastensors defined as?",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "Y": "aParameter"
    },
    {
        "X": "Parameters can be considered what?",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "Y": "the \u201clearnable\u201d aspects of the module\u2019s computation"
    },
    {
        "X": "What are not required to have state, and can also be stateless?",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "Y": "modules"
    },
    {
        "X": "What function performs the computation?",
        "Z": "It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "forward()"
    },
    {
        "X": "What can perform arbitrary computation involving any number of inputs and outputs?",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "theforward()implementation"
    },
    {
        "X": "The input is matrix-multiplied with what?",
        "Z": "It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "theweightparameter"
    },
    {
        "X": "What can theforward()implementation for a module perform?",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "Y": "arbitrary computation involving any number of inputs and outputs"
    },
    {
        "X": "What module demonstrates how modules package state and computation together?",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "Y": "module"
    },
    {
        "X": "What can be constructed and called?",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "Y": "Instances"
    },
    {
        "X": "What demonstrates how modules package state and computation together?",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "Y": "module"
    },
    {
        "X": "What are aspects of the module\u2019s computation that should be \u201clearned\u201d?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "parameters registered by a module"
    },
    {
        "X": "What is the name of the Optimizer used by a module?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. ",
        "Y": "PyTorch"
    },
    {
        "X": "Before we get to that, let\u2019s first examine what?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. ",
        "Y": "how modules can be composed with one another"
    },
    {
        "X": "What are aspects of a module's computation that should be learned?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "the parameters registered by a module"
    },
    {
        "X": "What does PyTorch use to update parameters?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "Optimizers"
    },
    {
        "X": "What should we examine first?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "how modules can be composed with one another"
    },
    {
        "X": "What is the easiest way to do this?",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. ",
        "Y": "using theSequentialmodule"
    },
    {
        "X": "Where does theSequentialmodule feed the output of the firstMyLinearmodule?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "theReLU"
    },
    {
        "X": "What is theSequentialmodule limited to?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "in-order chaining of modules"
    },
    {
        "X": "What can contain other modules, making them useful building blocks for developing more elaborate functionality?",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): ",
        "Y": "Modules"
    },
    {
        "X": "What allows us to chain together multiple modules?",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): ",
        "Y": "theSequentialmodule"
    },
    {
        "X": "What does Sequential feed the output of the first MyLinearmodule as input into?",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. ",
        "Y": "theReLU"
    },
    {
        "X": "What doesSequential automatically feed the output of the firstMyLinearmodule?",
        "Z": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "input into theReLU"
    },
    {
        "X": "How is the output of the firstMyLinearmodule limited?",
        "Z": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "in-order chaining of modules"
    },
    {
        "X": "What is recommended for anything beyond the simplest use cases?",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "a custom module"
    },
    {
        "X": "What is a simple neural network implemented as?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "a custom module"
    },
    {
        "X": "What does Sequentialautomatically feed the output of the first MyLinearmodule as input into?",
        "Z": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: ",
        "Y": "theReLU"
    },
    {
        "X": "What is the limit of Sequentialautomatically feeding the output of the firstMyLinearmodule as input into the secondMyLinearmodule?",
        "Z": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: ",
        "Y": "in-order chaining of modules"
    },
    {
        "X": "Why is it recommended to define a custom module for anything beyond the simplest use cases?",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "Y": "full flexibility on how submodules are used for a module\u2019s computation"
    },
    {
        "X": "What is implemented as a custom module?",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "a simple neural network"
    },
    {
        "X": "What is a custom module?",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "Y": "a simple neural network"
    },
    {
        "X": "What does defining a custom module give you?",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: ",
        "Y": "full flexibility"
    },
    {
        "X": "What are the two submodules of a neural network called?",
        "Z": "For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: ",
        "Y": "children"
    },
    {
        "X": "What can be iterated through via a call tochildren() ornamed_children()?",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "Y": "children"
    },
    {
        "X": "What module registers submodules from a list or dict?",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "Y": "TheModuleListandModuleDictmodules"
    },
    {
        "X": "What do TheModuleListandModuleDictmodules do?",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "Y": "register submodules from a list or dict"
    },
    {
        "X": "What is sometimes necessary for a module to do?",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "Y": "dynamically define submodules"
    },
    {
        "X": "What does the call toparameters()andnamed_parameters() include?",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: ",
        "Y": "child parameters"
    },
    {
        "X": "Sometimes, it\u2019s necessary for a module to do what?",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "Y": "dynamically define submodules"
    },
    {
        "X": "What do calls toparameters() andnamed_parameters() do?",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "Y": "recursively include child parameters"
    },
    {
        "X": "What are the parameters of a module?",
        "Z": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): ",
        "Y": "its direct parameters as well as the parameters of all submodules"
    },
    {
        "X": "How can elaborate neural networks be formed?",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "Y": "module composition"
    },
    {
        "X": "PyTorch provides a large library of modules that perform computation commonly found within neural networks?",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "Y": "thetorch.nnnamespace"
    },
    {
        "X": "What provides a large library of performant modules within thetorch.nnnamespace?",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "Y": "PyTorch"
    },
    {
        "X": "In the next section, we give a full example of what?",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "Y": "training a neural network"
    },
    {
        "X": "What does PyTorch.nnnamespace provide?",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "Y": "more information"
    },
    {
        "X": "What do we give in the next section of training a neural network?",
        "Z": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "Y": "a full example"
    },
    {
        "X": "What can you find on PyTorch's website?",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "Y": "more information"
    },
    {
        "X": "What does the next section give a full example of?",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, ",
        "Y": "training a neural network"
    },
    {
        "X": "What is a function that can be used to train a neural network?",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "Y": "Recursivelyapply()a function"
    },
    {
        "X": "What can be used to optimize a neural network?",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "Y": "PyTorch\u2019s Optimizers"
    },
    {
        "X": "What is a function that can be used to a module and its submodules library of PyTorch-provided modules?",
        "Z": "For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "Y": "Recursivelyapply()a function"
    },
    {
        "X": "What can be used to optimize a network's parameters?",
        "Z": "For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "Y": "PyTorch\u2019s Optimizers"
    },
    {
        "X": "What function does a module and its submodules library of PyTorch-provided modules:torch.nn have?",
        "Z": "Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "Y": "Recursivelyapply()a function"
    },
    {
        "X": "What can be used to optimize a neural net?",
        "Z": "Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "Y": "PyTorch\u2019s Optimizers"
    },
    {
        "X": "What is the name of a network that needs to be trained?",
        "Z": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "Y": "Defining neural net modules"
    },
    {
        "X": "What is present in a network?",
        "Z": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: ",
        "Y": "key parts of training"
    },
    {
        "X": "When a network is built, what does it need to be trained?",
        "Z": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. ",
        "Y": "it has to be trained"
    },
    {
        "X": "What is the key part of training?",
        "Z": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "Y": "A network is created"
    },
    {
        "X": "What type of optimizer is created?",
        "Z": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "Y": "stochastic gradient descent optimizer"
    },
    {
        "X": "What does an optimizer do?",
        "Z": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "Y": "computes a loss"
    },
    {
        "X": "What is the name of an optimizer created?",
        "Z": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "Y": "stochastic gradient descent optimizer"
    },
    {
        "X": "What happens to the network's parameters' gradients?",
        "Z": "An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "Y": "zeros"
    },
    {
        "X": "What is the name of the optimizer that is created when a network is created?",
        "Z": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "Y": "stochastic gradient descent optimizer"
    },
    {
        "X": "What does the optimizer do?",
        "Z": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation ",
        "Y": "computes a loss"
    },
    {
        "X": "What is the name of the optimizer that is created?",
        "Z": "An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "Y": "stochastic gradient descent optimizer"
    },
    {
        "X": "What is the value of the network's parameters' gradients?",
        "Z": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "Y": "zeros"
    },
    {
        "X": "What is the value ofl1'sweightparameter closer to?",
        "Z": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): ",
        "Y": "0"
    },
    {
        "X": "What does the network do?",
        "Z": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): ",
        "Y": "computes a loss"
    },
    {
        "X": "What is the name of the function used to train a neural network?",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "Y": "Recursivelyapply()a function"
    },
    {
        "X": "What is an example of training a neural network?",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "Y": "Using Optimizers"
    },
    {
        "X": "What does loss.backward() call to apply the gradients to the parameters?",
        "Z": "calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. ",
        "Y": "optimizer.step()"
    },
    {
        "X": "Who computes a loss, zeros the network's parameters' gradients, calls loss.backward() to update the parameters' gradients",
        "Z": "runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: ",
        "Y": "runs the network"
    },
    {
        "X": "What can sometimes be tricky?",
        "Z": "zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: ",
        "Y": "Training neural networks"
    },
    {
        "X": "How many layers does _layer_net_optim.html have?",
        "Z": "Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "Y": "two"
    },
    {
        "X": "What is called to apply gradients to the parameters?",
        "Z": "calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. ",
        "Y": "optimizer.step()"
    },
    {
        "X": "How many layers of a neural network can be optimized?",
        "Z": "calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. ",
        "Y": "two"
    },
    {
        "X": "What shows that the value ofl1's weightparameter is now much closer to 0?",
        "Z": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "Y": "examining the value ofl1\u2019sweightparameter"
    },
    {
        "X": "What is the name of the network that trains neural networks?",
        "Z": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "Y": "Using Optimizers"
    },
    {
        "X": "What is the name of the network that is used to train neural networks?",
        "Z": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "Y": "Neural network training"
    },
    {
        "X": "What is the value ofl1\u2019sweightparameter now closer to?",
        "Z": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "Y": "0"
    },
    {
        "X": "What type of training can be tricky?",
        "Z": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "Y": "Neural network training"
    },
    {
        "X": "What can we save if we want to save a model to disk?",
        "Z": "In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "Y": "itsstate_dict"
    },
    {
        "X": "What is itsstate_dict?",
        "Z": "In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "Y": "state dictionary"
    },
    {
        "X": "What state does a module's _dictcontains state that affects its computation?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "state"
    },
    {
        "X": "What does a module'sstate_dictcontains state that affects its computation?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "module\u2019s parameters"
    },
    {
        "X": "What is a module'sstate_dictcontains state that affects module computation?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "beyond parameters"
    },
    {
        "X": "What provides the concept of \"buffers\"?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "PyTorch"
    },
    {
        "X": "What are the parameters of a module'sstate_dictcontains state that affects its computation?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "learnable aspects of computation"
    },
    {
        "X": "What does a module'sstate_dict contain?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "state that affects its computation"
    },
    {
        "X": "A module'sstate_dict includes state that affects its computation. This includes, but is not limited to, what?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "the module\u2019s parameters"
    },
    {
        "X": "For some modules, it may be useful to have state beyond parameters that affects module computation but is what?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "not learnable"
    },
    {
        "X": "What does PyTorch provide for state beyond parameters that affects module computation but is not learnable?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "buffers"
    },
    {
        "X": "Parameters are contained within what?",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "Y": "thestate_dict"
    },
    {
        "X": "Where are learnable aspects of computation contained?",
        "Z": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "Y": "thestate_dict"
    },
    {
        "X": "What are non-learnable aspects of computation?",
        "Z": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "Y": "Persistentbuffers"
    },
    {
        "X": "What are non-persistentbuffers?",
        "Z": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "Y": "not contained within thestate_dict"
    },
    {
        "X": "Non-persistentbuffers are left out of what?",
        "Z": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "Y": "serialization"
    },
    {
        "X": "Where are Persistentbuffers contained?",
        "Z": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "Y": "thestate_dict"
    },
    {
        "X": "Why are non-persistentbuffers not contained within thestate_dict?",
        "Z": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization) ",
        "Y": "left out of serialization"
    },
    {
        "X": "What is a motivating example for the use of buffers?",
        "Z": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "Y": "a simple module that maintains a running mean"
    },
    {
        "X": "What does the snippet show how to do this?",
        "Z": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "Y": "register_buffer()"
    },
    {
        "X": "What is the purpose of the current value of the running mean?",
        "Z": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: ",
        "Y": "it will be restored when loading a serialized form of the module"
    },
    {
        "X": "What does this snippet show how to use to accomplish this?",
        "Z": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "Y": "register_buffer()"
    },
    {
        "X": "What is considered part of the module\u2019sstate_dictand?",
        "Z": "Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: ",
        "Y": "the current value of the running mean"
    },
    {
        "X": "For what do you need to check out:",
        "Z": "Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: ",
        "Y": "more information"
    },
    {
        "X": "What can be iterated over usingbuffers() ornamed_buffers()?",
        "Z": "Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html What is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html ",
        "Y": "Buffers of a module"
    },
    {
        "X": "For more information, check out what?",
        "Z": "Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html What is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html ",
        "Y": "Saving and loading"
    },
    {
        "X": "What are called during the backward pass?",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "Backward hooks"
    },
    {
        "X": "How can backward hooks be installed?",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "withregister_full_backward_hook()"
    },
    {
        "X": "When will backward hooks be called?",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "when the backward for this Module has been computed"
    },
    {
        "X": "How can backward hooks be installed for all modules?",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "globally"
    },
    {
        "X": "When are backward hooks called?",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "when the backward for this Module has been computed"
    },
    {
        "X": "What will be used throughout the rest of the computation?",
        "Z": "All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "updated value"
    },
    {
        "X": "How can hooks be used to execute arbitrary code along the regular module forward/backward?",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "modify some inputs/outputs"
    },
    {
        "X": "What allows the user to return an updated value that will be used throughout the remaining computation?",
        "Z": "All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "hooks"
    },
    {
        "X": "What can hooks be used to do without having to change the module'sforward()function?",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "Y": "modify some inputs/outputs"
    },
    {
        "X": "What provides several more advanced features that are designed to work with modules?",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "Y": "PyTorch"
    },
    {
        "X": "What are the functionalities that PyTorch provides when writing a new module?",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "Y": "inherited"
    },
    {
        "X": "Where can a detailed discussion of these features be found?",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "Y": "links below"
    },
    {
        "X": "What is the name of the feature that PyTorch provides?",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "Y": "Profiling"
    },
    {
        "X": "What company provides several more advanced features that are designed to work with modules?",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "Y": "PyTorch"
    },
    {
        "X": "What happens to PyTorch's functionalities when writing a new module?",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "Y": "inherited"
    },
    {
        "X": "What type of discussion of PyTorch's advanced features can be found in the links below?",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "Y": "In-depth"
    },
    {
        "X": "What is another name for profiling?",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "Y": "Pruning"
    },
    {
        "X": "What is the export of modules to?",
        "Z": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "Y": "TorchScript"
    },
    {
        "X": "From what language does TorchScript export modules to?",
        "Z": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "Y": "C++"
    },
    {
        "X": "Exporting modules to what?",
        "Z": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "Y": "TorchScript"
    },
    {
        "X": "What language is TorchScript used for?",
        "Z": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "Y": "C++"
    },
    {
        "X": "What is a multi-dimensional matrix containing elements of a single data type?",
        "Z": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "Y": "Atorch.Tensoris"
    },
    {
        "X": "How many tensor types are defined by Torch?",
        "Z": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "Y": "10"
    },
    {
        "X": "What is Atorch.Tensoris?",
        "Z": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 ",
        "Y": "multi-dimensional matrix"
    },
    {
        "X": "What is the name of the tensor type defined by Torch?",
        "Z": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 ",
        "Y": "bfloat16"
    },
    {
        "X": "What is the bit floating point torch?",
        "Z": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "Y": "32"
    },
    {
        "X": "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?",
        "Z": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "Y": "Data type dtype"
    },
    {
        "X": "What is the data type dtype CPU tensor GPU tensor?",
        "Z": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "Y": "32-bit floating point torch"
    },
    {
        "X": "What is the float32ortorch?",
        "Z": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "Y": "32-bit floating point torch"
    },
    {
        "X": "What is a 32-bit floating point torch?",
        "Z": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "Y": "CPU tensor GPU tensor"
    },
    {
        "X": "What is the name of the floating point torch?",
        "Z": "torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). ",
        "Y": "torch"
    },
    {
        "X": "How many bits of complex torch is it?",
        "Z": "GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) ",
        "Y": "8-bit"
    },
    {
        "X": "What is a double 8-bit integer?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 ",
        "Y": "unsigned"
    },
    {
        "X": "What is the name of the torch?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "bfloat16 torch"
    },
    {
        "X": "What is a double torch?",
        "Z": "torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor ",
        "Y": "double torch"
    },
    {
        "X": "How many -bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "Y": "128"
    },
    {
        "X": "What type of tensoris float64?",
        "Z": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "Y": "double "
    },
    {
        "X": "What is the name of the double torch?",
        "Z": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "Y": "torch.float64ortorch"
    },
    {
        "X": "What is the floating point1 torch?",
        "Z": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) ",
        "Y": "16-bit"
    },
    {
        "X": "What is the name of the torch.cuda.DoubleTensor?",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short ",
        "Y": "16-bit floating point1 torch"
    },
    {
        "X": "What is a half torch?",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short ",
        "Y": "half torch"
    },
    {
        "X": "What is the name of the floating point1 torch?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "16-bit floating point1 torch"
    },
    {
        "X": "How many bits floating point1 torch?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor ",
        "Y": "16"
    },
    {
        "X": "What is the name of the torch.float16ortorch?",
        "Z": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "half torch"
    },
    {
        "X": "What is the term for torch.float16ortorch?",
        "Z": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor ",
        "Y": "half torch"
    },
    {
        "X": "How many bits of complex torch does BFloat16Tensor have?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor ",
        "Y": "32"
    },
    {
        "X": "How many byte complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128or",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "Y": "32"
    },
    {
        "X": "What is the name of the torch.cuda.HalfTensor?",
        "Z": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "16-bit floating point2 torch"
    },
    {
        "X": "How many byte complex torch?",
        "Z": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int ",
        "Y": "32"
    },
    {
        "X": "What is the name of the bfloat16 torch?",
        "Z": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "16-bit floating point2 torch"
    },
    {
        "X": "What type of torch is bfloat16?",
        "Z": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "16-bit floating point2"
    },
    {
        "X": "What type of torch does bfloat16Tensor have?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor ",
        "Y": "32-bit complex torch"
    },
    {
        "X": "What type of complex torch is the BFloat16Tensor 32-bit complex torch.complex64 128-bit complex torch?",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "Y": "64-bit"
    },
    {
        "X": "How many -bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128",
        "Z": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long ",
        "Y": "32"
    },
    {
        "X": "What is unsigned torch.uint8 torch?",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "cdouble 8-bit integer"
    },
    {
        "X": "What type of complex torch is complex32 64-bit complex torch?",
        "Z": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor ",
        "Y": "32-bit"
    },
    {
        "X": "How many bits of complex torch is torch.complex32?",
        "Z": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor ",
        "Y": "64"
    },
    {
        "X": "What is the name of the LongTensor torch?",
        "Z": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor ",
        "Y": "LongTensor torch.cuda"
    },
    {
        "X": "What type of complex torch is it?",
        "Z": "64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "Y": "64-bit"
    },
    {
        "X": "What is the name of the LongTensor Boolean torch?",
        "Z": "64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "Y": "LongTensor torch.cuda"
    },
    {
        "X": "What is the name of the Boolean torch?",
        "Z": "torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "Y": "Boolean torch"
    },
    {
        "X": "How many bits complex torch?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "Y": "128"
    },
    {
        "X": "What is the unsigned number of torch.uint8 torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "Y": "8-bit"
    },
    {
        "X": "What is the unsigned part of a torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is the name of the torch.int8 torch.ByteTensor torch.cuda.Byte",
        "Z": "torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "8-bit integer"
    },
    {
        "X": "What does ByteTensor stand for?",
        "Z": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is the name of the torch.int8 torch?",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is the sign of a ByteTensor torch?",
        "Z": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is the sign of a torch?",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "Y": "16-bit integer"
    },
    {
        "X": "How many -bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "Y": "16"
    },
    {
        "X": "How many -bit integer (signed) torch.int16ortorch.short torch?",
        "Z": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "Y": "16"
    },
    {
        "X": "What is the signature of a torch?",
        "Z": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "Y": "16-bit integer"
    },
    {
        "X": "What is the number of integers in torch.int32ortorch.int torch.IntTensor torch.cu",
        "Z": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "Y": "32-bit"
    },
    {
        "X": "What does CharTensor stand for?",
        "Z": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "Y": "16-bit integer"
    },
    {
        "X": "How large is the torch?",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "Y": "16-bit"
    },
    {
        "X": "What is the name of the torch.int32ortorch.int torch?",
        "Z": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "32-bit integer"
    },
    {
        "X": "What is the short torch?",
        "Z": "torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "Y": "int16ortorch"
    },
    {
        "X": "How many bits does the torch contain?",
        "Z": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 ",
        "Y": "4-bit integer"
    },
    {
        "X": "What is the sign of an IntTensor?",
        "Z": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 ",
        "Y": "4-bit integer"
    },
    {
        "X": "How many -bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cu",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 ",
        "Y": "4"
    },
    {
        "X": "What type of torch.cuda.LongTensor Boolean torch.bool torch.BoolTen",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 ",
        "Y": "LongTensor"
    },
    {
        "X": "What does torch.tensor() always copydata?",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "Y": "torch.tensor()always copiesdata"
    },
    {
        "X": "What do you need to change to avoid a copy of a Tensordata?",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "Y": "userequires_grad_()ordetach()"
    },
    {
        "X": "What is used if you have a numpy array and want to avoid a copy?",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "Y": "usetorch.as_tensor()"
    },
    {
        "X": "A tensor of specific data type can be constructed by passing what to a constructor or tensor creation op?",
        "Z": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "Y": "atorch.dtype"
    },
    {
        "X": "What is the name of the indexing and slicing notation used to create a tensor?",
        "Z": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "Y": "Python"
    },
    {
        "X": "What is the name of Python's indexing and slicing notation to get a Python number from a tensor?",
        "Z": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "Y": "Usetorch.Tensor.item()"
    },
    {
        "X": "What is the purpose of creating a tensor?",
        "Z": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "Y": "automatic differentiation"
    },
    {
        "X": "What is the name of Python's indexing and slicing notation?",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "Y": "Usetorch.Tensor.item()"
    },
    {
        "X": "What is the purpose of a tensor?",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "change an existing tensor\u2019"
    },
    {
        "X": "What can a tensor be created for automatic differentiation?",
        "Z": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "Y": "withrequires_grad=Trueso thattorch.autogradrecords operations"
    },
    {
        "X": "What holds the data of each tensor?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "associatedtorch.Storage"
    },
    {
        "X": "What does the tensor class provide?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "multi-dimensional,stridedview of a storage"
    },
    {
        "X": "What is the name of a tensor class that provides multi-dimensional,stridedview of a storage?",
        "Z": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "Y": "Note"
    },
    {
        "X": "What does Usetorch.Tensor.item() get a Python number from?",
        "Z": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "Y": "a tensor"
    },
    {
        "X": "What does the associatedtorch.Storage do?",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "holds its data"
    },
    {
        "X": "What does the tensor class do?",
        "Z": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "Y": "defines numeric operations on it"
    },
    {
        "X": "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage and defines numeric operations",
        "Z": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "Y": "Note"
    },
    {
        "X": "How can a tensor be created?",
        "Z": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation"
    },
    {
        "X": "For more information on tensor views, see what?",
        "Z": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note ",
        "Y": "Tensor Views"
    },
    {
        "X": "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage?",
        "Z": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note ",
        "Y": "Note"
    },
    {
        "X": "What can a tensor be created?",
        "Z": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation"
    },
    {
        "X": "What holds the data of a tensor?",
        "Z": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "associatedtorch.Storage"
    },
    {
        "X": "For more information on tensor views, seeTensor what?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "Views"
    },
    {
        "X": "What is the name of the tensor class that can be created withrequires_grad=Trueso thattorch",
        "Z": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "Y": "Note"
    },
    {
        "X": "What is the name of the tensor attributes of atorch.Tensor?",
        "Z": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "Y": "Note"
    },
    {
        "X": "What does the tensor class provide of a storage?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "Y": "multi-dimensional,stridedview"
    },
    {
        "X": "What are some examples of atorch.Tensor attributes?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "thetorch.dtype,torch.device, andtorch.layoutattributes"
    },
    {
        "X": "Note Methods which mutate a tensor are marked with what?",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "underscore suffix"
    },
    {
        "X": "What does torch.FloatTensor.abs()compute the result in?",
        "Z": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "a new tensor"
    },
    {
        "X": "What is the name of a method that mutates a tensor?",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What is the name of atorch.Tensor?",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "thetorch.dtype,torch.device, andtorch.layoutattributes"
    },
    {
        "X": "What is the tensor'storch.deviceand/ortorch.dtype?",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "to()method"
    },
    {
        "X": "What is the name of the tensor'storch.device and/ortorch.dtype?",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What is another name for a tensor?",
        "Z": "Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "atorch.Tensor"
    },
    {
        "X": "What does torch.FloatTensor.abs()compute?",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "the result in a new tensor"
    },
    {
        "X": "What method is used to change an existing tensor'storch.deviceand/ortorch.dtype?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "usingto()method"
    },
    {
        "X": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of what",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "atorch.Tensor"
    },
    {
        "X": "What does usingto()method on a tensor do to change an existing tensor'storch.deviceand/or",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What do Note Methods which mutate a tensor are marked with an underscore suffix?",
        "Z": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "usingto()method"
    },
    {
        "X": "What does torch.FloatTensor.abs_() compute?",
        "Z": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "the absolute value in-place"
    },
    {
        "X": "What suffix does a method that mutate a tensor have?",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "underscore"
    },
    {
        "X": "What does the tensor need to change?",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "Y": "usingto()method"
    },
    {
        "X": "What does a tensor use to change their tensor'storch.deviceand/ortorch.dtype",
        "Z": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "usingto()method"
    },
    {
        "X": "What could a current implementation oftorch.Tensor lead to?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "unexpectedly high memory usage"
    },
    {
        "X": "What type of tensor should you use to change existing tensor'storch.deviceand/ortorch.d",
        "Z": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "one large structure"
    },
    {
        "X": "What does current implementation oftorch.Tensor introduce?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "memory overhead"
    },
    {
        "X": "What should you use if you have many tiny tensors?",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "one large structure"
    },
    {
        "X": "How do you create a tensor with pre-existing data?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "tensor"
    },
    {
        "X": "What does usetorch.tensor() do to create a tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "pre-existing data"
    },
    {
        "X": "What does usetorch.tensor() have to do to create a tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "specific size"
    },
    {
        "X": "What is used to create a tensor with the same size as another tensor?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "Y": "*tensor creation ops"
    },
    {
        "X": "What is a tensor with the same size as another tensor?",
        "Z": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "Y": "usetorch"
    },
    {
        "X": "What is the name of the tensor created by usetorch.tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "*_liketensor creation ops"
    },
    {
        "X": "What is a Tensor with its dimensions reversed?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "usetensor.new_*creation ops"
    },
    {
        "X": "What is a Tensor with reversed?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "its dimensions"
    },
    {
        "X": "What is the main way to create a tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "tensor"
    },
    {
        "X": "What does usetorch.tensor() create a tensor with?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "pre-existing data"
    },
    {
        "X": "What do you need to create a tensor with?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "specific size"
    },
    {
        "X": "What does usetorch do to create a tensor with specific size?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "*tensor creation ops"
    },
    {
        "X": "What is the size of a tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "same size"
    },
    {
        "X": "What creates a tensor with the same size as another tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "*_liketensor creation ops"
    },
    {
        "X": "What type of tensor is used to create a tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "similar type but different size"
    },
    {
        "X": "What is reversed in a tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "its dimensions"
    },
    {
        "X": "What is the name of the pre-existing data to create a tensor with pre-existing data?",
        "Z": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "Y": "usetorch.tensor()"
    },
    {
        "X": "What does usetorch.tensor() have to do to create a tensor with?",
        "Z": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "Y": "specific size"
    },
    {
        "X": "What is the name of the tensor with the same size as another tensor?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "*_liketensor creation ops"
    },
    {
        "X": "What is the name of the tensor with its dimensions reversed?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "Y": "usetensor.new_*creation ops"
    },
    {
        "X": "What does usetensor.new_*creation ops have?",
        "Z": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "Y": "its dimensions reversed"
    },
    {
        "X": "What is the number of dimensions inx,x.Tis equivalent tox.permute?",
        "Z": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "Y": "Ifnis"
    },
    {
        "X": "What is used to create a tensor with specific size?",
        "Z": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "Y": "*tensor creation ops"
    },
    {
        "X": "Is this Tensor with its dimensions reversed or reversed?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "reversed"
    },
    {
        "X": "Ifnis the number of dimensions inx,x.Tis equivalent what?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "tox.permute"
    },
    {
        "X": "What does usetorch mean to create a tensor?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "Y": "specific size"
    },
    {
        "X": "What does the Tensor have its dimensions?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "Y": "reversed"
    },
    {
        "X": "What is the Tensor equivalent tox.permute?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "Y": "Tensor.new_tensor"
    },
    {
        "X": "Usetorch to create a tensor with what?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "Y": "specific size"
    },
    {
        "X": "What do you use to create a tensor with specific size?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "Y": "*tensor creation ops"
    },
    {
        "X": "What is used to create a tensor with reversed dimensions?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "Y": "Tensor.new_tensor"
    },
    {
        "X": "What is the name of the tensor to create a tensor with the same size as another tensor?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "Y": "usetorch"
    },
    {
        "X": "What is the name of a tensor with similar type but different size as another tensor?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "usetensor.new_*creation ops"
    },
    {
        "X": "What is a Tensor with its dimensions?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "Y": "reversed"
    },
    {
        "X": "Tensor.new_tensor Returns a new Tensor with what?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "data"
    },
    {
        "X": "What does Tensor.new_full return?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "Tensor of sizesizefilled withfill_value"
    },
    {
        "X": "What is a Tensor of sizesizefilled withfill_value?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "Tensor.new_empty"
    },
    {
        "X": "What does usetorch do to create a tensor with the same size as another tensor?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "Y": "*_liketensor creation ops"
    },
    {
        "X": "What type of tensor does usetorch. *_liketensor creation ops create?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "similar type but different size"
    },
    {
        "X": "What Returns a new Tensor withdataas the tensor data?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "new_tensor"
    },
    {
        "X": "What value does Tensor.new_full return?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "withfill_value"
    },
    {
        "X": "What type of tensor returns a new Tensor?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "Y": "empty"
    },
    {
        "X": "Tensor.new_tensor Returns a new Tensor with what as the tensor data?",
        "Z": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "Y": "data"
    },
    {
        "X": "What returns a Tensor of sizesizefilled withfill_value?",
        "Z": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "Y": "Tensor.new_full"
    },
    {
        "X": "What does Tensor.new_empty return a Tensor of sizesizefilled with?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "uninitialized data"
    },
    {
        "X": "Tensor.new_ones Returns a Tensor of sizesizefilled with what?",
        "Z": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ ",
        "Y": "1"
    },
    {
        "X": "What ops is used to create a tensor with similar type but different size as another tensor?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "Y": "tensor.new_*creation"
    },
    {
        "X": "Is this tensor with its dimensions reversed or reversed?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "Y": "reversed"
    },
    {
        "X": "What does Tensor.new_tensor return?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "new Tensor"
    },
    {
        "X": "What value does Tensor.new_full return a Tensor of sizesizefilled?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "Y": "withfill_value"
    },
    {
        "X": "What returns a Tensor of sizesizefilled with1?",
        "Z": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "Y": "Tensor.new_ones"
    },
    {
        "X": "What is the dimensions of the Tensor?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "Y": "reversed"
    },
    {
        "X": "What does Tensor.new_zeros return?",
        "Z": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "Y": "a Tensor of sizesizefilled with0"
    },
    {
        "X": "What is the name of the Tensor with its dimensions reversed?",
        "Z": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "Y": "Tensor.is_cuda"
    },
    {
        "X": "Ifnis the number of dimensions inx,x.Tis equivalent to what?",
        "Z": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "Y": "tox.permute"
    },
    {
        "X": "What returns a Tensor of sizesizefilled?",
        "Z": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "Y": "withfill_value"
    },
    {
        "X": "Tensor.new_ones Returns a Tensor of sizesizefilled what?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "with1"
    },
    {
        "X": "What does Tensor.new_tensor return a new Tensor withdataas?",
        "Z": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "tensor data"
    },
    {
        "X": "Tensor.new_empty Returns a Tensor of sizesizefilled with what?",
        "Z": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() ",
        "Y": "uninitialized data"
    },
    {
        "X": "Where is the Tensor stored?",
        "Z": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "GPU"
    },
    {
        "X": "What is true if the Tensor is stored on the GPU?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "Y": "Tensor.is_quantized"
    },
    {
        "X": "Tensor.new_tensor Returns a new Tensor withdataas what?",
        "Z": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "Y": "tensor data"
    },
    {
        "X": "Tensor.new_full Returns a Tensor of what?",
        "Z": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin ",
        "Y": "sizesizefilled withfill_value"
    },
    {
        "X": "Tensor.new_zeros Returns a Tensor of sizesizefilled with what?",
        "Z": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() ",
        "Y": "0"
    },
    {
        "X": "What isTrueif the Tensor is quantized?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "Y": "quantized"
    },
    {
        "X": "What is the name of the Tensor?",
        "Z": "torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "Y": "Tensor"
    },
    {
        "X": "What is a new Tensor withdataas?",
        "Z": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "Y": "tensor data"
    },
    {
        "X": "Who returns a Tensor of sizesizefilled withfill_value?",
        "Z": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() ",
        "Y": "Tensor"
    },
    {
        "X": "What is the Tensor?",
        "Z": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "Y": "quantized"
    },
    {
        "X": "What does Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "Z": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "Y": "quantized"
    },
    {
        "X": "What is the name of the meta tensor?",
        "Z": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "meta"
    },
    {
        "X": "What is a meta tensor?",
        "Z": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "Y": "Tensor.device"
    },
    {
        "X": "What is the Tensor of?",
        "Z": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "Y": "sizesizefilled withfill_value"
    },
    {
        "X": "What is the Tensor.device?",
        "Z": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "Y": "thetorch.device"
    },
    {
        "X": "What is the Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "Z": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "Y": "quantized"
    },
    {
        "X": "What is the meta tensor?",
        "Z": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "Y": "meta"
    },
    {
        "X": "What is thetorch.device where this Tensor is?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "Y": "Tensor.grad"
    },
    {
        "X": "What does a Tensor of sizesizefilled with?",
        "Z": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ ",
        "Y": "uninitialized data"
    },
    {
        "X": "Tensor.new_zeros returns a Tensor of sizesizefilled with what?",
        "Z": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin ",
        "Y": "0"
    },
    {
        "X": "What is the Tensor.is_trueif the Tensor is quantized?",
        "Z": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "quantized"
    },
    {
        "X": "What is the first call for a Tensor?",
        "Z": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "Y": "tobackward()"
    },
    {
        "X": "What is the Tensor of sizesizefilled with?",
        "Z": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "Y": "1"
    },
    {
        "X": "What does the Tensor are quantized?",
        "Z": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "Y": "quantized"
    },
    {
        "X": "What attribute isnoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself",
        "Z": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "Y": "Tensor.ndim Alias fordim() Tensor.real"
    },
    {
        "X": "Tensor.new_zeros returns a Tensor of what size?",
        "Z": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "Y": "size size filled with0"
    },
    {
        "X": "What does the Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "Z": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "Y": "quantized"
    },
    {
        "X": "Tensor.grad isnoneby default and becomes a Tensor the first time a call?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "Y": "tobackward()"
    },
    {
        "X": "What does a Tensor return?",
        "Z": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "Y": "sizesizefilled with0"
    },
    {
        "X": "What is the name of the tensor that is quantized?",
        "Z": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "Y": "quantized"
    },
    {
        "X": "What is a Tensor the first time a call tobackward()computes gradients forself?",
        "Z": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "Y": "Tensor.ndim Alias fordim() Tensor.real"
    },
    {
        "X": "What GPU is Tensor.is_cuda IsTrueif the Tensor is stored on?",
        "Z": "Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag ",
        "Y": "GPU"
    },
    {
        "X": "What Returns a new tensor containing real values of theselftensor?",
        "Z": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "Y": "real"
    },
    {
        "X": "What is a new tensor containing real values of theselftensor?",
        "Z": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "Y": "real"
    },
    {
        "X": "Where is IsTrueif the Tensor is stored?",
        "Z": "IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag ",
        "Y": "GPU"
    },
    {
        "X": "What meta tensor isTrueif the Tensor is a meta tensor?",
        "Z": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "Y": "meta"
    },
    {
        "X": "Tensor.grad isNoneby default and becomes a Tensor the first time a call?",
        "Z": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "tobackward()computes gradients forself"
    },
    {
        "X": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing what?",
        "Z": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm ",
        "Y": "real values of theselftensor"
    },
    {
        "X": "Tensor.imag Returns a new tensor containing what?",
        "Z": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "Y": "imaginary values of theselftensor"
    },
    {
        "X": "What is a new tensor containing imaginary values of theselftensor?",
        "Z": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "Y": "Tensor.abs"
    },
    {
        "X": "What does it mean to be a meta tensor?",
        "Z": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "Y": "IsTrueif the Tensor is quantized"
    },
    {
        "X": "What is the Tensor.is_?",
        "Z": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "Y": "meta"
    },
    {
        "X": "What is the name of Tensor.abs?",
        "Z": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "Y": "Seetorch.abs()"
    },
    {
        "X": "IsTrueif the Tensor is what?",
        "Z": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "Y": "quantized"
    },
    {
        "X": "What is true if the Tensor is a meta tensor?",
        "Z": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "Y": "meta"
    },
    {
        "X": "What is the name of the device where the Tensor is?",
        "Z": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "Y": "thetorch.device"
    },
    {
        "X": "What attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself",
        "Z": "Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm ",
        "Y": "Tensor.grad"
    },
    {
        "X": "What does Tensor.ndim return a new tensor containing?",
        "Z": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "Y": "real values of theselftensor"
    },
    {
        "X": "What Returns a new tensor containing imaginary values of theselftensor?",
        "Z": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ ",
        "Y": "Tensor.imag"
    },
    {
        "X": "What Seetorch.abs() returns a new tensor containing imaginary values of theselftensor?",
        "Z": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "Y": "Tensor.abs"
    },
    {
        "X": "What isTrueif the Tensor is a meta tensor?",
        "Z": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "Y": "meta"
    },
    {
        "X": "What is the olute of the Tensor?",
        "Z": "Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute ",
        "Y": "In-place version ofabs() Tensor.abs"
    },
    {
        "X": "What isTrueif the Tensor is?",
        "Z": "IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() ",
        "Y": "meta tensor"
    },
    {
        "X": "What is the name of the in-place version of abs()?",
        "Z": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "Y": "Alias forabs() Tensor.abs"
    },
    {
        "X": "What is Tensor.device?",
        "Z": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward ",
        "Y": "thetorch.device"
    },
    {
        "X": "What is the In-place version ofabs() Tensor.absolute?",
        "Z": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "Y": "Alias forabs"
    },
    {
        "X": "Where is the Tensor.device located?",
        "Z": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ ",
        "Y": "thetorch.device"
    },
    {
        "X": "What does the Alias fordim() Tensor.real Return a new tensor containing?",
        "Z": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "Y": "real values of theselftensor"
    },
    {
        "X": "What returns a new tensor containing imaginary values of theselftensor?",
        "Z": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "Y": "Tensor.imag"
    },
    {
        "X": "What does Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alia",
        "Z": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "Y": "Tensor.abs"
    },
    {
        "X": "What does thetorch.device make a Tensor the first time a call?",
        "Z": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "Y": "tobackward()computes gradients forself"
    },
    {
        "X": "What is the name of the in-place version of a Tensor?",
        "Z": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "Y": "Alias forabs_()"
    },
    {
        "X": "Where is this Tensor located?",
        "Z": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ ",
        "Y": "thetorch.device"
    },
    {
        "X": "What is the in-place version ofabs() Tensor.absolute?",
        "Z": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "Y": "Alias forabs() Tensor"
    },
    {
        "X": "What does Tensor.grad become a Tensor the first time a call?",
        "Z": "Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm ",
        "Y": "tobackward()computes gradients forself"
    },
    {
        "X": "What is the name of the name of the name of the name of the name of the name of the name of the name of the name of the name",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "Y": "Tensor.addbmm"
    },
    {
        "X": "What does this attribute become a Tensor the first time a call?",
        "Z": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "Y": "tobackward()computes gradients forself"
    },
    {
        "X": "What attribute becomes a Tensor the first time a call tobackward()computes gradients forself?",
        "Z": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "Y": "isNoneby"
    },
    {
        "X": "What is arccos Seetorch.arccos() Tensor.add?",
        "Z": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "In-place version ofacos() Tensor"
    },
    {
        "X": "What does the new tensor contain?",
        "Z": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "real values of theselftensor"
    },
    {
        "X": "What kind of values of theselftensor does Tensor.imag return?",
        "Z": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "Y": "imaginary"
    },
    {
        "X": "Tensor.real Returns a new tensor containing real values of theselftensor?",
        "Z": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "Alias fordim"
    },
    {
        "X": "What is the name of the in-place version of arccos() Tensor?",
        "Z": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "In-place version ofarccos() Tensor"
    },
    {
        "X": "Who returns a new tensor containing real values of theselftensor?",
        "Z": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "Y": "Alias fordim"
    },
    {
        "X": "What is added to the selftensor?",
        "Z": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ ",
        "Y": "scalar or tensor"
    },
    {
        "X": "What adds a scalar or tensor toselftensor?",
        "Z": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ ",
        "Y": "Tensor.add_"
    },
    {
        "X": "Tensor.imag returns a new tensor containing what?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "Y": "imaginary values of theselftensor"
    },
    {
        "X": "What does tensor add toselftensor?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "Y": "scalar"
    },
    {
        "X": "What does Tensor.imag return a new tensor containing?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin ",
        "Y": "imaginary values of theselftensor"
    },
    {
        "X": "What do you add to theselftensor?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "Y": "scalar or tensor"
    },
    {
        "X": "What is another name for Add a scalar or tensor toselftensor?",
        "Z": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "Y": "addbmm"
    },
    {
        "X": "Returns a new tensor containing what?",
        "Z": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() ",
        "Y": "imaginary values of theselftensor"
    },
    {
        "X": "What is added to theselftensor?",
        "Z": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "Y": "scalar or tensor"
    },
    {
        "X": "What is in-place version ofabs() Tensor.absolute?",
        "Z": "Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "Y": "Alias forabs"
    },
    {
        "X": "What is the In-place version ofaddbmm() Tensor?",
        "Z": "Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "Y": "addbmm"
    },
    {
        "X": "What does Tensor.absolute stand for?",
        "Z": "Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "Y": "Alias forabs"
    },
    {
        "X": "What is the In-place version ofadd() Tensor?",
        "Z": "Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "Y": "addbmm"
    },
    {
        "X": "What does Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor",
        "Z": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "Y": "addbmm"
    },
    {
        "X": "What do you add to a Tensor?",
        "Z": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "Y": "scalar or tensor toselftensor"
    },
    {
        "X": "What is the In-place version of addbmm() Tensor?",
        "Z": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ ",
        "Y": "addbmm"
    },
    {
        "X": "In what axis does a n-D tensor reverse the order of a n-D tensor?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "dims"
    },
    {
        "X": "What is the name of'snp.flip'?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "NumPy"
    },
    {
        "X": "What does torch.flipis expect to be?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "slower thannp.flip"
    },
    {
        "X": "What is a listortuple to flip on?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "axis"
    },
    {
        "X": "What is the order of a tensor along a given axis in dims?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "n-D"
    },
    {
        "X": "Why is copying a tensor's data slower than numPy'snp.flip?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "more work"
    },
    {
        "X": "What does dims(a listortuple) represent?",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "Y": "axis to flip on"
    },
    {
        "X": "Computes what of either a matrix or batch of matricesinput?",
        "Z": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note ",
        "Y": "singular value decomposition"
    },
    {
        "X": "What is the singular value decomposition represented as?",
        "Z": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note ",
        "Y": "namedtuple(U, S, V)"
    },
    {
        "X": "What is the transpose ofVfor?",
        "Z": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning ",
        "Y": "real inputs"
    },
    {
        "X": "Ifinputis a batch of what, thenU,S, andVare also batched with the same batch dimensions asinput?",
        "Z": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning ",
        "Y": "matrices"
    },
    {
        "X": "What method returns the reduced singular value decomposition?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "Y": "IfsomeisTrue"
    },
    {
        "X": "What will the returnedUandVmatrices contain if the last two dimensions ofinputaremandn?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. ",
        "Y": "onlymin(n, m)orthonormal columns"
    },
    {
        "X": "Whencompute_uvisFalse has no effect?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "Y": "argumentsomehas no effect"
    },
    {
        "X": "What is the default value of the method that returns the reduced singular value decomposition?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. ",
        "Y": "IfsomeisTrue"
    },
    {
        "X": "If someisTrue(default), the returnedUandVmatrices will contain what?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "Y": "onlymin(n, m)orthonormal columns"
    },
    {
        "X": "Ifcompute_uvisFalse, the returnedUandVwill be what?",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning ",
        "Y": "zero-filled matrices"
    },
    {
        "X": "What effect does argumentsome have whencompute_uvisFalse?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "Y": "argumentsomehas no effect whencompute_uvisFalse"
    },
    {
        "X": "What will the returnedUandV be ifcompute_uvisFalse?",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "Y": "zero-filled matrices"
    },
    {
        "X": "What does the argumentsome have no effect whencompute_uvisFalse?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "Y": "float, double, cfloat and cdouble data types"
    },
    {
        "X": "What will the dtypes ofUandV always be?",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "Y": "real-valued"
    },
    {
        "X": "What is deprecated in favor oftorch.linalg.svd()?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "Y": "Warning torch.svd()"
    },
    {
        "X": "What happens whencompute_uvisFalse?",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "Y": "argumentsomehas no effect"
    },
    {
        "X": "What data type is supported by float, double, cfloat and cfloat?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "Y": "cdouble"
    },
    {
        "X": "The dtypes ofUandVare the same asinput\u2019s.Swill always be what?",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning ",
        "Y": "real-valued"
    },
    {
        "X": "What is deprecated in favor of oftorch.linalg.svd()?",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "Y": "torch.svd()"
    },
    {
        "X": "What are the supportsinput of?",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "Y": "float, double, cfloat and cdouble data types"
    },
    {
        "X": "What will the dtypes ofUandVare always be?",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "Y": "real-valued"
    },
    {
        "X": "What should U,S,V=torch.svd(A,some=some,compute_uv=True)",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "Y": "Note Differences"
    },
    {
        "X": "Along with cfloat and cfloat, what data type is supported by Torch?",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "Y": "cdouble"
    },
    {
        "X": "The dtypes ofUandVare will always be what?",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "Y": "real-valued"
    },
    {
        "X": "What should be replaced with Note Differences?",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "Y": "withtorch.linalg.svd()"
    },
    {
        "X": "What should be replaced with _,S,_=torch.svd(A,some=some,compute_uv",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "Y": "U,S,V"
    },
    {
        "X": "What is the default value for both oftorch.linalg.svd()'sfull_matrices?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "Y": "default value for both isTrue"
    },
    {
        "X": "What does torch.svd() return?",
        "Z": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "Y": "torch.svd()returnsV"
    },
    {
        "X": "What should be replaced with withtorch.linalg.svd()?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "Y": "Note Differences"
    },
    {
        "X": "What does whereastorch.linalg.svd()returnsVh?",
        "Z": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "Y": "torch.svd()returnsV"
    },
    {
        "X": "What does torch.svd()return forUandVh?",
        "Z": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "Y": "zero-filled tensors"
    },
    {
        "X": "In what order are the singular values returned?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "Y": "descending order"
    },
    {
        "X": "What is the name of a batch of matrices?",
        "Z": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "Y": "Ifinputis"
    },
    {
        "X": "Note TheStensor can only be used to what?",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "Y": "compute gradients"
    },
    {
        "X": "What can be used to compute gradients ifcompute_uvisTrue?",
        "Z": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "Y": "Note"
    },
    {
        "X": "What does torch.svd() return forUandVh?",
        "Z": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "Y": "zero-filled tensors"
    },
    {
        "X": "What is the name of the tensor that can only be used to compute gradients ifcompute_uvisTrue?",
        "Z": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "Y": "Note"
    },
    {
        "X": "WhensomeisFalse, the gradients onU[..., :, min(m, n):]andV[...",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning ",
        "Y": "backward pass"
    },
    {
        "X": "What algorithm does the implementation oftorch.linalg.svd()on CPU use instead of?gesvdfor speed?",
        "Z": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "Y": "LAPACK"
    },
    {
        "X": "Which CPU version of CUDA is a routinegesddon?",
        "Z": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "Y": "MAGMA"
    },
    {
        "X": "What is the name of CUDA's routinegesdd?",
        "Z": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "Y": "Note"
    },
    {
        "X": "What does the implementation oftorch.linalg.svd() on CPU use instead of?gesvdfor speed?",
        "Z": "The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "Y": "LAPACK\u2019s routine?gesdd"
    },
    {
        "X": "What version of CUDA does cuSOLVER's routinesgesvdjandgesvdjBatchedon?",
        "Z": "The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "Y": "10.1.243"
    },
    {
        "X": "What happens when a matrix is represented as a column-major matrix?",
        "Z": "Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning ",
        "Y": "returnedUwill not be contiguous"
    },
    {
        "X": "The matrix (or batch of matrices) will be represented as what?",
        "Z": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "column-major matrix"
    },
    {
        "X": "The matrix (or batch of matrices) will be represented as a column-major matrix (i.e. what?",
        "Z": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "Fortran-contiguous"
    },
    {
        "X": "What is the name of the warning that the matrix will not be contiguous?",
        "Z": "The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning ",
        "Y": "Warning"
    },
    {
        "X": "What does returnedUnot be contiguous?",
        "Z": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "returnedUwill not be contiguous"
    },
    {
        "X": "What will the gradients with respect toUandV only be when the input does not have zero nor repeated singular values?",
        "Z": "Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "Y": "finite"
    },
    {
        "X": "What will the gradients with respect toUandV be if the distance between two singular values is close to zero?",
        "Z": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "Y": "numerically unstable"
    },
    {
        "X": "What does the matrix have?",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "Y": "small singular values"
    },
    {
        "X": "What happens when the matrix has small singular values?",
        "Z": "The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "Y": "Warning"
    },
    {
        "X": "When will the gradients with respect toUandVonly be finite?",
        "Z": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "Y": "when the input does not have zero nor repeated singular values"
    },
    {
        "X": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be what?",
        "Z": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "numerically unstable"
    },
    {
        "X": "When do the gradients with respect toUandVbe numerically unstable?",
        "Z": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "Y": "when the matrix has small singular values"
    },
    {
        "X": "What will be numerically unstable if the distance between two singular values is close to zero?",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "Y": "the gradients with respect toUandV"
    },
    {
        "X": "When do gradients depend onS1?",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "Y": "when the matrix has small singular values"
    },
    {
        "X": "What is the warning if the distance between singular values is close to zero?",
        "Z": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What can be multiplied byUandV?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "arbitrary phase factor"
    },
    {
        "X": "What can be used to multiply the columns of the spanning subspace inUandV?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "Y": "a rotation matrix"
    },
    {
        "X": "Different platforms, like NumPy, or inputs on different device types, may produce what?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "differentUandVtensors"
    },
    {
        "X": "For what is the singular value decomposition not unique?",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "Y": "complex-valuedinput"
    },
    {
        "X": "What happens wheninputhas repeated singular values?",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "Y": "wheninputhas repeated singular values"
    },
    {
        "X": "What may produce differentUandVtensors?",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "Y": "Different platforms"
    },
    {
        "X": "What is used to multiply the columns of the spanning subspace inUandV?",
        "Z": "For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "Y": "a rotation matrix"
    },
    {
        "X": "What is the input tensor of size(*, m, n)where*is zero or more batch dimensions consisting of(",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "input(Tensor)"
    },
    {
        "X": "What controls whether to compute the reduced or full decomposition?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "some(bool,optional)"
    },
    {
        "X": "What is the default value of some(bool,optional) that controls whether to compute the reduced or full decomposition?",
        "Z": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "Default:True"
    },
    {
        "X": "What is the name of the function that controls whether to computeUandV?",
        "Z": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "compute_uv"
    },
    {
        "X": "What is the default value of compute_uv(bool,optional)?",
        "Z": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "Default:True"
    },
    {
        "X": "What is the output tuple of tensors?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "out(tuple,optional)"
    },
    {
        "X": "What does SciPy do with the following cases?",
        "Z": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "Computesinput*log(other)"
    },
    {
        "X": "Computesinput*log(other)with the following cases. Similar to what?",
        "Z": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "SciPy\u2019sscipy.special.xlogy"
    },
    {
        "X": "At least one of inputorothermust be what?",
        "Z": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "a tensor"
    },
    {
        "X": "What does a DLPack do to a tensor?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "Decodes a DLPack to a tensor"
    },
    {
        "X": "What is dlpack?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "PyCapsule object"
    },
    {
        "X": "How often can each dlpack be consumed?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "once"
    },
    {
        "X": "What represents the tensor?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "a DLPack"
    },
    {
        "X": "What is a tensor to be exported?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "tensor"
    },
    {
        "X": "Decodes a DLPack to what?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "a tensor"
    },
    {
        "X": "What is a PyCapsule object with the dltensor?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "dlpack"
    },
    {
        "X": "How many times can a DLPack be consumed?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "once"
    },
    {
        "X": "What is the tensor represented by?",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "Y": "DLPack"
    },
    {
        "X": "What does a float tensor convert to?",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "Y": "per-channel quantized tensor"
    },
    {
        "X": "What is input to quantize scales?",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "Y": "float 1D tensor"
    },
    {
        "X": "Converts a float tensor to what?",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 ",
        "Y": "a per-channel quantized tensor"
    },
    {
        "X": "What is input(Tensor) to quantize scales?",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "Y": "float tensor"
    },
    {
        "X": "What must be used to convert a float tensor to a per-channel quantized tensor?",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 ",
        "Y": "one of the quantized dtypes"
    },
    {
        "X": "What is input(Tensor) used to quantize scales?",
        "Z": "input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "Y": "float tensor"
    },
    {
        "X": "What is an example of a quantized tensor Tensor?",
        "Z": "input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "Y": "newly quantized tensor Tensor Example:"
    },
    {
        "X": "What enables gradient calculation?",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "Y": "Context-manager"
    },
    {
        "X": "What does the context-manager do if it has been disabled viano_gradorset_grad_enabled?",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "Y": "Enables gradient calculation"
    },
    {
        "X": "Why is this context manager thread local?",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "Y": "it will not affect computation in other threads"
    },
    {
        "X": "What does this context manager function as?",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "Y": "decorator"
    },
    {
        "X": "Make sure to instantiate with what?",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "Y": "parenthesis"
    },
    {
        "X": "What is one of several mechanisms that can enable or disable gradients locally?",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example: ",
        "Y": "enable_grad"
    },
    {
        "X": "What is supported only if eigenvalues and eigenvectors are all real valued?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. ",
        "Y": "backward pass"
    },
    {
        "X": "Wheninputis on CUDA,torch.eig() causes what?",
        "Z": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing ",
        "Y": "host-device synchronization"
    },
    {
        "X": "What might be complex?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing ",
        "Y": "eigenvalues and eigenvectors"
    },
    {
        "X": "When input is on CUDA torch.eig()causes what?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "host-device synchronization"
    },
    {
        "X": "Returns a what?",
        "Z": "  Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "Y": "random permutation of integers from0ton-1"
    },
    {
        "X": "What does Sets the default floating point dtype tod get?",
        "Z": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "Y": "current default floating pointtorch.dtype"
    },
    {
        "X": "Returns a what type of tensor of sizeendstartstepleftlceil fractextend",
        "Z": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "Y": "1-D tensor"
    },
    {
        "X": "What is the current default floating pointtorch.dtype?",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "Y": "current default floating pointtorch.dtype"
    },
    {
        "X": "Returns a tensor filled with what value1?",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ",
        "Y": "scalar"
    },
    {
        "X": "Returns a what type of tensor?",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "Y": "1-D tensor"
    },
    {
        "X": "Returns a what type of tensor of sizeendstartstep+1leftlfloor fractext",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",
        "Y": "1-D tensor"
    },
    {
        "X": "Creation ops are listed under Random sampling and include:torch.rand()torch.rand()torch.randin",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ",
        "Y": "Random sampling"
    },
    {
        "X": "Returns what tensor of sizeendstartstep+1leftlfloor fractextend -",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data. ",
        "Y": "1-D tensor"
    },
    {
        "X": "What type of tensor is returned?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ",
        "Y": "1-D tensor"
    },
    {
        "X": "What is created of sizestepswhose values are evenly spaced fromstarttoend inclusive?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ",
        "Y": "one-dimensional tensor"
    },
    {
        "X": "What is the name of the Random sampling creation ops?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ",
        "Y": "Create"
    },
    {
        "X": "What is constructed in COO(rdinate) format with specified values at the givenindices?",
        "Z": "Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   ",
        "Y": "asparse tensor"
    },
    {
        "X": "Returns a what size tensor of sizeendstartstep+1leftlfloor fractextend",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "1-D"
    },
    {
        "X": "What type of tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive?",
        "Z": "Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   ",
        "Y": "one-dimensional"
    },
    {
        "X": "What type of tensor of sizestepswhose values are evenly spaced frombasestarttextbasetextstartbasestart",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "Y": "one-dimensional"
    },
    {
        "X": "Returns a what tensor with ones on the diagonal and zeros elsewhere?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "2-D"
    },
    {
        "X": "Returns an uninitialized tensor with what size asinput?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "same size asinput"
    },
    {
        "X": "Returns a tensor filled with uninitialized data. Returns an uninitialized tensor with the",
        "Z": "Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   ",
        "Y": "Return"
    },
    {
        "X": "What is the tensor of sizeendstartstep+1leftlfloor fractextend -",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "1-D"
    },
    {
        "X": "What type of tensor of sizestepswhose values are evenly spaced fromstarttoend inclusive?",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "Y": "one-dimensional tensor"
    },
    {
        "X": "Returns what tensor with ones on the diagonal and zeros elsewhere?",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "Y": "2-D tensor"
    },
    {
        "X": "Returns an uninitialized tensor with what size?",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "Y": "same size asinput"
    },
    {
        "X": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "Y": "Con"
    },
    {
        "X": "Concatenates what in the given dimension?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "given sequence ofseqtensors"
    },
    {
        "X": "Splitsinput, a tensor with three or more dimensions, into what?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "multiple tensors depthwise"
    },
    {
        "X": "Stack tensors in sequence depthwise (along what axis)?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "third axis"
    },
    {
        "X": "Gathers values along an axis specified what?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "bydim"
    },
    {
        "X": "Splitsinput, a tensor with one or more dimensions, into what?",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   ",
        "Y": "multiple tensors"
    },
    {
        "X": "What does it mean to split a tensor into a specific number of chunks?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension. Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().Returns a new tensor that is a narrowed version ofinputtensor.",
        "Y": "Concatenates the given sequence ofseqtensors in the given dimension"
    },
    {
        "X": "What is the difference between a tensor and a specific number of chunks?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "Splits a tensor into a specific number of chunks"
    },
    {
        "X": "What is a tensor with three or more dimensions?",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   ",
        "Y": "Splitsinput"
    },
    {
        "X": "How does Splitsinput create a new tensor?",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   ",
        "Y": "horizontally stacking"
    },
    {
        "X": "What does Stack tensors in sequence depthwise?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "Y": "Gathers values along an axis specified bydim"
    },
    {
        "X": "What axis does Splitsinput gather values along?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   ",
        "Y": "bydim"
    },
    {
        "X": "What does the given sequence ofseqtensors in the given dimension do?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   ",
        "Y": "Concatenates"
    },
    {
        "X": "What happens when a tensor is broken into chunks?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   ",
        "Y": "Splits"
    },
    {
        "X": "What does Splitsinput use to split a tensor into multiple tensors depthwise?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   ",
        "Y": "indices_or_sections"
    },
    {
        "X": "What creates a new tensor?",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "horizontally stacking the tensors intensors"
    },
    {
        "X": "How many chunks does a tensor split into?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "Y": "chunks"
    },
    {
        "X": "How many dimensions does Splitsinput have?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "Y": "one or more dimensions"
    },
    {
        "X": "What is the name of the tensor that Stacks tensors in sequence depthwise?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "Y": "Gathers values along an axis specified bydim"
    },
    {
        "X": "What does Splits a tensor into?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "Y": "a specific number of chunks"
    },
    {
        "X": "How many tensors does Splitsinput split into?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "multiple tensors depthwise"
    },
    {
        "X": "What does a tensor do in sequence depthwise?",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   ",
        "Y": "Stack tensors"
    },
    {
        "X": "What is the axis specified by Splitsinput?",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   ",
        "Y": "bydim"
    },
    {
        "X": "How many tensors can a tensor with one or more dimensions be split into?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   ",
        "Y": "multiple tensors horizontally"
    },
    {
        "X": "What is Splitsinput?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections"
    },
    {
        "X": "How do tensors stack in sequence?",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   ",
        "Y": "horizontally"
    },
    {
        "X": "What does Splitsinput use to split a tensor?",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   ",
        "Y": "indices_or_sections"
    },
    {
        "X": "What does Splitsinput do to create a new tensor?",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   ",
        "Y": "Stack tensors"
    },
    {
        "X": "What do tensors with one or more dimensions splitsinput into?",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   ",
        "Y": "multiple tensors horizontally"
    },
    {
        "X": "Stack tensors in sequence horizontally (what?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "Y": "column wise"
    },
    {
        "X": "How does a tensor create a new tensor?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "horizontally stacking"
    },
    {
        "X": "What does Stack tensors in sequence depthwise mean?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "Y": "third axis"
    },
    {
        "X": "What is acolumn wise?",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   ",
        "Y": "Stack tensors in sequence horizontally"
    },
    {
        "X": "What is the tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   ",
        "Y": "aLongTensor"
    },
    {
        "X": "Where do tensors stack in sequence?",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   ",
        "Y": "depthwise"
    },
    {
        "X": "What does Stack tensors do?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "Y": "Gathers values along an axis specified bydim"
    },
    {
        "X": "What is the name of the tensor that splitsinput into multiple tensors horizontally?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   ",
        "Y": "indices_or_sections"
    },
    {
        "X": "Stack tensors in sequence horizontally (what way)?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "Y": "column wise"
    },
    {
        "X": "What does aLongTensor do?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   ",
        "Y": "indexes theinputtensor along dimensiondimusing the entries inindex"
    },
    {
        "X": "What is the length of Stack tensors in sequence depthwise?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "Y": "third axis"
    },
    {
        "X": "What does Stack tensors in sequence depthwise gather?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "Y": "Gathers values along an axis specified bydim"
    },
    {
        "X": "How do Stack tensors in sequence?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "Y": "horizontally"
    },
    {
        "X": "What is the new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "Z": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "Y": "aLongTensor"
    },
    {
        "X": "What is the new tensor that indexes theinputtensor according to the boolean maskmask?",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   ",
        "Y": "1-D"
    },
    {
        "X": "What is the boolean maskmask?",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "Y": "aBoolTensor"
    },
    {
        "X": "What is the name of the tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   ",
        "Y": "Alias fortorch.transpose()"
    },
    {
        "X": "What does Splitsinput do?",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   ",
        "Y": "Gathers values along an axis specified bydim"
    },
    {
        "X": "How do you stack tensors in sequence horizontally?",
        "Z": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   ",
        "Y": "Stack tensors"
    },
    {
        "X": "What type of tensor is in sequence horizontally?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   ",
        "Y": "Stack"
    },
    {
        "X": "What is a new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   ",
        "Y": "aLongTensor"
    },
    {
        "X": "What moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "Y": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination"
    },
    {
        "X": "What does a tensor with one or more dimensions do horizontally?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   ",
        "Y": "Stack tensors"
    },
    {
        "X": "What does move the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?",
        "Z": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "Y": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination"
    },
    {
        "X": "What is dimensiondimusing the entries inindex?",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "aLongTensor"
    },
    {
        "X": "What is a new tensor that indexes theinputtensor according to the boolean maskmask?",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "Y": "1-D"
    },
    {
        "X": "What is the name of the tensor that is a narrowed version ofinputtensor?",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "Y": "Alias oftorch.vstack()"
    },
    {
        "X": "What type of version ofinputtensor is Alias fortorch.movedim()?",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "Y": "narrowed"
    },
    {
        "X": "Stack tensors in sequence what?",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "horizontally"
    },
    {
        "X": "What does fortorch.movedim() do?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination"
    },
    {
        "X": "What is used to move the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "Alias fortorch.movedim()"
    },
    {
        "X": "What version ofinputtensor is Alias fortorch.movedim()?",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "narrowed"
    },
    {
        "X": "What is the name of the index that returns a new tensor that indexes theinputtensor along dimensiondimusing",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "aLongTensor"
    },
    {
        "X": "Returns a new 1-D tensor which indexes theinputtensor according to what boolean maskmask",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "aBoolTensor"
    },
    {
        "X": "What is the name of the new tensor that indexes theinputtensor according to the boolean maskmask",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   ",
        "Y": "1-D"
    },
    {
        "X": "What is the name of the tensor that indexes theinputtensor according to the boolean maskmask?",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "Y": "Alias oftorch.vstack()"
    },
    {
        "X": "What is the name of the version oftorch.Tensor.scatter_()?",
        "Z": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "Y": "Out-of-place"
    },
    {
        "X": "What does move the dimension(s) ofinput?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   ",
        "Y": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination"
    },
    {
        "X": "What method returns a new tensor that is a narrowed version ofinputtensor?",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   ",
        "Y": "Alias fortorch.movedim()"
    },
    {
        "X": "Returns a tensor with the same data and number of elements asinput, but with what?",
        "Z": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "Y": "specified shape"
    },
    {
        "X": "What is the name of the function that returns a tensor with the same data and number of elements as input?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "Alias oftorch.vstack()"
    },
    {
        "X": "What does Alias fortorch.movedim() do?",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "Y": "Alias oftorch.vstack()"
    },
    {
        "X": "What does a tensor have with the same data and number of elements asinput?",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "Y": "the specified shape"
    },
    {
        "X": "What is the tensor split into?",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "Y": "chunks"
    },
    {
        "X": "What is the name of the function that moves the dimension(s) ofinput to the position(s) indestination?",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "Y": "Alias fortorch.movedim()"
    },
    {
        "X": "Returns a tensor with the same data and number of elements as input, but with what?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "specified shape"
    },
    {
        "X": "Out-of-place version oftorch.Tensor.scatter_add_() Splits the tensor into",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   ",
        "Y": "chunks"
    },
    {
        "X": "What does Alias fortorch.movedim() have?",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "Y": "the specified shape"
    },
    {
        "X": "What is a tensor with all the dimensions ofinput?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "size1removed"
    },
    {
        "X": "What happens to a sequence of tensors?",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   ",
        "Y": "Concatenates a sequence of tensors along a new dimension"
    },
    {
        "X": "What function returns a new tensor that is a narrowed version ofinputtensor?",
        "Z": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "Y": "Alias fortorch.movedim()"
    },
    {
        "X": "What is returned with the same data and number of elements as input but with the specified shape?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   ",
        "Y": "a tensor"
    },
    {
        "X": "Returns a tensor with what?",
        "Z": "  Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition. ",
        "Y": "all the dimensions ofinputof size1removed"
    },
    {
        "X": "What happens to a sequence of tensors along a new dimension?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "Concatenates"
    },
    {
        "X": "What type of version ofinputtensor is a new tensor?",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "Y": "narrowed"
    },
    {
        "X": "What does a new tensor have?",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "arcsine"
    },
    {
        "X": "What is the name of the function that converts a sequence of tensors along a new dimension?",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   ",
        "Y": "Alias fortorch.transpose()"
    },
    {
        "X": "What is the name of the tensor that returns a tensor with the same data and number of elements asinput?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "Alias oftorch.vstack()"
    },
    {
        "X": "What does a tensor concatenate?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   ",
        "Y": "a sequence of tensors along a new dimension"
    },
    {
        "X": "What does Alias fortorch.transpose() do?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "Alias fortorch.transpose()"
    },
    {
        "X": "What splits the tensor into chunks?",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   ",
        "Y": "Out-of-place version oftorch.Tensor.scatter_()"
    },
    {
        "X": "Returns a tensor with what data removed?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   ",
        "Y": "all the dimensions"
    },
    {
        "X": "What is another name for fortorch.transpose()?",
        "Z": "     Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   ",
        "Y": "Alias fortorch.transpose()"
    },
    {
        "X": "What does the generator object generate?",
        "Z": "  Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. ",
        "Y": "pseudo random numbers"
    },
    {
        "X": "What does the generator object produce?",
        "Z": "  Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. ",
        "Y": "pseudo random numbers"
    },
    {
        "X": "What type of random number is the seed for generating random numbers?",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   ",
        "Y": "non-deterministic"
    },
    {
        "X": "What sets the seed for generating random numbers to a non-deterministic random number?",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   ",
        "Y": "Sets the seed for generating random numbers"
    },
    {
        "X": "What is the random number generator state?",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "Draws binary random numbers (0 or 1) from a Bernoulli distribution"
    },
    {
        "X": "What does atorch.ByteTensor set?",
        "Z": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "Y": "random number generator state"
    },
    {
        "X": "What does atorch.ByteTensor mean?",
        "Z": "Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   ",
        "Y": "Sets the seed for generating random numbers"
    },
    {
        "X": "What is a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "Draws binary random numbers (0 or 1) from a Bernoulli distribution"
    },
    {
        "X": "What does each row containnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution ",
        "Y": "tensor"
    },
    {
        "X": "What is atorch.ByteTensor?",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution ",
        "Y": "random number generator state"
    },
    {
        "X": "Returns the random number generator state as what?",
        "Z": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "Y": "atorch.ByteTensor"
    },
    {
        "X": "What are given to a tensor of random numbers drawn from separate normal distributions?",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).",
        "Y": "mean and standard deviation"
    },
    {
        "X": "Returns a what of random numbers drawn from separate normal distributions whose mean and standard deviation are given?",
        "Z": "  Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "Y": "tensor"
    },
    {
        "X": "What is the name of the state that draws binary random numbers from a Bernoulli distribution?",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "Sets the random number generator state"
    },
    {
        "X": "What does the random number generator state do?",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   ",
        "Y": "Draws binary random numbers (0 or 1) from a Bernoulli distribution"
    },
    {
        "X": "What does a Bernoulli distribution draw?",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   ",
        "Y": "Draws binary random numbers"
    },
    {
        "X": "What parameter is given by the corresponding element ininputi?",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "rate parameter given by the corresponding element ininputi"
    },
    {
        "X": "Returns a tensor of random numbers drawn from separate normal distributions what are given?",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   ",
        "Y": "whose mean and standard deviation"
    },
    {
        "X": "The tensor returns a tensor of the same size as input with each element sampled from what distribution?",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   ",
        "Y": "Poisson"
    },
    {
        "X": "What distribution is each element sampled from?",
        "Z": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "Y": "Poisson"
    },
    {
        "X": "What does a tensor return with each element sampled from a Poisson distribution with rate parameter given by the corresponding element inin",
        "Z": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   ",
        "Y": "a tensor of the same size asinput"
    },
    {
        "X": "Returns a tensor filled with what from a uniform distribution on the interval?",
        "Z": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   ",
        "Y": "random numbers"
    },
    {
        "X": "Where is a tensor filled with random numbers?",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "uniform distribution"
    },
    {
        "X": "Where is a tensor filled with random integers generated?",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "betweenlow(inclusive) andhigh(exclusive)"
    },
    {
        "X": "What is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1) Returns a ",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "a tensor"
    },
    {
        "X": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   ",
        "Y": "exclusive"
    },
    {
        "X": "What is returned with the same shape as Tensorinput?",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "tensor"
    },
    {
        "X": "What is a tensor with the same size asinput filled with random numbers?",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "uniform distribution"
    },
    {
        "X": "What is the name of the tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh?",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "exclusive"
    },
    {
        "X": "What is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1]?",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   ",
        "Y": "a tensor"
    },
    {
        "X": "What is a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Z": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution ",
        "Y": "tensor"
    },
    {
        "X": "What is the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Z": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   ",
        "Y": "same shape"
    },
    {
        "X": "What is the standard normal distribution?",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "mean0and variance1"
    },
    {
        "X": "What is the same shape as Tensorinputfilled with random integers generated?",
        "Z": "Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. ",
        "Y": "uniformly"
    },
    {
        "X": "What is a random permutation of integers from0ton-1?",
        "Z": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. ",
        "Y": "mean 0 and variance 1."
    },
    {
        "X": "What is a tensor filled with random numbers from a normal distribution with mean0and variance1?",
        "Z": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "random permutation of integers from0ton-1"
    },
    {
        "X": "What does a random permutation of integers from0ton-1 mean?",
        "Z": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "mean 0 and variance 1."
    },
    {
        "X": "What is a random permutation of integers?",
        "Z": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: ",
        "Y": "from0ton-1"
    },
    {
        "X": "What are some more in-place random sampling functions defined on?",
        "Z": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "Tensors"
    },
    {
        "X": "What is another name for a normal distribution with mean0and variance1?",
        "Z": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "standard normal distribution"
    },
    {
        "X": "What is returned when a tensor is filled with random numbers from a normal distribution with mean 0 and variance 1?",
        "Z": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "a tensor with the same size asinput"
    },
    {
        "X": "What are there a few more defined on Tensors?",
        "Z": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution ",
        "Y": "in-place random sampling functions"
    },
    {
        "X": "Where are there more in-place random sampling functions defined?",
        "Z": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution ",
        "Y": "Tensors"
    },
    {
        "X": "What type of distribution does tensor.log_normal_() originate from?",
        "Z": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "Y": "log-normal distribution"
    },
    {
        "X": "What distribution does Tensor.log_normal_() sample from?",
        "Z": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution ",
        "Y": "log-normal"
    },
    {
        "X": "What type of distribution did the numbers sample from?",
        "Z": "torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "discrete uniform distribution"
    },
    {
        "X": "What are numbers sampled from the discrete uniform distribution torch?",
        "Z": "torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "Tensor.random_()"
    },
    {
        "X": "What type of distribution are the numbers sampled from?",
        "Z": "  Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution ",
        "Y": "discrete uniform distribution"
    },
    {
        "X": "What is an engine for generating (scrambled) Sobol sequences?",
        "Z": "quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "Y": "SobolEngine Thetorch.quasirandom"
    },
    {
        "X": "What type of sequence is generated by the SobolEngine Thetorch?",
        "Z": "quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "Y": "quasirandom"
    },
    {
        "X": "What is saved to a disk file?",
        "Z": "  Saves an object to a disk file.   Loads an object saved withtorch.save()from a file. ",
        "Y": "an object"
    },
    {
        "X": "Loads an object saved from a file?",
        "Z": "  Saves an object to a disk file.   Loads an object saved withtorch.save()from a file. ",
        "Y": "withtorch.save()"
    },
    {
        "X": "What does it do when an object is saved to a disk file?",
        "Z": "  Saves an object to a disk file.   Loads an object saved withtorch.save()from a file. ",
        "Y": "Saves an object to a disk file"
    },
    {
        "X": "Loads an object saved what from a file?",
        "Z": "  Saves an object to a disk file.   Loads an object saved withtorch.save()from a file. ",
        "Y": "withtorch.save()"
    },
    {
        "X": "What sets the number of threads used for intraop parallelism on CPU?",
        "Z": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "Y": "the number of threads used for parallelizing CPU operations"
    },
    {
        "X": "What is the number of threads used for inter-op parallelism on CPU?",
        "Z": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "Y": "the number of threads used for interop parallelism"
    },
    {
        "X": "What returns the number of threads used for parallelizing CPU operations?",
        "Z": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "Y": "Returns the number of threads used for parallelizing CPU operations"
    },
    {
        "X": "Returns the number of threads used for what parallelism on CPU?",
        "Z": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "Y": "inter-op"
    },
    {
        "X": "What does Returns the number of threads used for inter-op parallelism on CPU?",
        "Z": "  Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g. ",
        "Y": "Sets the number of threads used for interop parallelism"
    },
    {
        "X": "What is a Context-manager?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "disabled gradient calculation"
    },
    {
        "X": "What does Context-manager enable?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "gradient calculation"
    },
    {
        "X": "What does Context-manager set gradient calculation to?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "on or off"
    },
    {
        "X": "What does grad mode return if grad mode is currently enabled?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "True"
    },
    {
        "X": "What does inference mode return if inference mode is currently enabled?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "True"
    },
    {
        "X": "What is the name of the manager that disabled gradient calculation?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "Context-manager"
    },
    {
        "X": "What does a Context-manager do?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "enables gradient calculation"
    },
    {
        "X": "What does a Context-manager set gradient calculation to?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "on or off"
    },
    {
        "X": "What is currently enabled by the Context-manager?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "grad mode"
    },
    {
        "X": "What mode does a Context-manager enable or disable?",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled. ",
        "Y": "inference mode"
    },
    {
        "X": "What does Alias fortorch.abs() compute?",
        "Z": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "Y": "inverse cosine"
    },
    {
        "X": "Who returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "Alias fortorch.acosh()"
    },
    {
        "X": "What does Alias fortorch.abs() return a new tensor?",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "inverse hyperbolic cosine"
    },
    {
        "X": "What does the scalarotherto each element of inputinput return a new resulting tensor?",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "Alias fortorch.acosh()"
    },
    {
        "X": "What does Alias fortorch.acosh() do?",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "inverse hyperbolic cosine"
    },
    {
        "X": "What does the element-wise division oftensor1bytensor2 do?",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "multiply the result by the scalarvalueand add it toinput"
    },
    {
        "X": "What computes the inverse cosine of each element ininput?",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "Alias fortorch.abs"
    },
    {
        "X": "What function returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   ",
        "Y": "Alias fortorch.acosh()"
    },
    {
        "X": "Alias fortorch.acos() returns a new tensor with what cosine of the elements of input?",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "inverse hyperbolic"
    },
    {
        "X": "What returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   ",
        "Y": "Alias fortorch.acos()"
    },
    {
        "X": "What is the element-wise division performed by Alias fortorch.acosh?",
        "Z": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "oftensor1bytensor2"
    },
    {
        "X": "What does Alias fortorch.abs() compute the inverse cosine of each element ininput?",
        "Z": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "Y": "Alias fortorch.acos()"
    },
    {
        "X": "What does Alias fortorch.acosh() add to each element of inputinput?",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "scalarotherto"
    },
    {
        "X": "What is the name of the function that returns a new tensor with the inverse hyperbolic cosine of the elements of input",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   ",
        "Y": "Alias fortorch.acosh()"
    },
    {
        "X": "What is the element-wise division performed by Alias fortorch.acosh()?",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "oftensor1bytensor2"
    },
    {
        "X": "What does Alias fortorch.acos() compute?",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "inverse cosine"
    },
    {
        "X": "What type of cosine does Alias fortorch.acosh() return a new tensor?",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "hyperbolic"
    },
    {
        "X": "What is the cosine of each element in input?",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "inverse"
    },
    {
        "X": "Returns a new tensor with what cosine of the elements of input?",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "inverse hyperbolic"
    },
    {
        "X": "What is the name of the function that returns a new tensor with the inverse hyperbolic cosine of the elements ofin",
        "Z": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   ",
        "Y": "Alias fortorch.acosh()"
    },
    {
        "X": "What is the cosine of elements ofinput?",
        "Z": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "hyperbolic"
    },
    {
        "X": "What does Alias fortorch.acosh() perform?",
        "Z": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   ",
        "Y": "the element-wise multiplication"
    },
    {
        "X": "What type of cosine does Alias fortorch.acos() return?",
        "Z": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   ",
        "Y": "inverse hyperbolic"
    },
    {
        "X": "What is the name of the element-wise multiplication performed by Alias fortorch.acosh?",
        "Z": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   ",
        "Y": "oftensor1bytensor2"
    },
    {
        "X": "What is the inverse of the elements ofinput?",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "Y": "hyperbolic sine"
    },
    {
        "X": "What does add the scalarotherto each element of the inputinputand returns a new resulting tensor?",
        "Z": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   ",
        "Y": "Alias fortorch.acosh()"
    },
    {
        "X": "What does Alias fortorch.acosh() add?",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   ",
        "Y": "scalarotherto each element of the inputinput"
    },
    {
        "X": "What does the scalarotherto multiply the result by the scalarvalueand add it toinput?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "Y": "element-wise division"
    },
    {
        "X": "What does the element-wise division oftensor1bytensor2 perform?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "Y": "the element-wise multiplication"
    },
    {
        "X": "What is the element-wise angle of the giveninputtensor?",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "Y": "radians"
    },
    {
        "X": "What is the cosine of the elements of input?",
        "Z": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "inverse hyperbolic"
    },
    {
        "X": "What is the name of the element-wise division?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "Y": "oftensor1bytensor2"
    },
    {
        "X": "What is the name of the element-wise multiplication?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "Y": "oftensor1bytensor2"
    },
    {
        "X": "Computes what of the given inputtensor?",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "Y": "element-wise angle"
    },
    {
        "X": "What is the name of the elements ofinput?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "Y": "arcsine"
    },
    {
        "X": "What is the name of the function that adds the scalarotherto each element of the input?",
        "Z": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   ",
        "Y": "Alias fortorch.acosh()"
    },
    {
        "X": "What is the name of the element-wise division performed by Alias fortorch.acosh?",
        "Z": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   ",
        "Y": "oftensor1bytensor2"
    },
    {
        "X": "Returns a new tensor with what of the elements of input?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   ",
        "Y": "arcsine"
    },
    {
        "X": "What does each element of inputinput return a new resulting tensor?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "Y": "scalarotherto"
    },
    {
        "X": "Who returns a new tensor with the arcsine of the elements ofinput?",
        "Z": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "Y": "Alias fortorch.asin()"
    },
    {
        "X": "What returns a new resulting tensor?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   ",
        "Y": "Adds the scalarotherto each element of the inputinput"
    },
    {
        "X": "What is the name of the function that returns a new tensor with the arcsine of the elements ofinput?",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   ",
        "Y": "Alias fortorch.asin()"
    },
    {
        "X": "Performs what division of oftensor1bytensor2?",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   ",
        "Y": "element-wise division"
    },
    {
        "X": "What does the element-wise division oftensor1bytensor2 multiply the result by?",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "Y": "scalarvalue"
    },
    {
        "X": "What does Alias fortorch.asin() have?",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "Y": "inverse hyperbolic sine"
    },
    {
        "X": "Who returns a new tensor with the inverse hyperbolic sine of the elements ofinput?",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "Y": "Alias fortorch.asin()"
    },
    {
        "X": "What is the element-wise division?",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "Y": "oftensor1bytensor2"
    },
    {
        "X": "What does oftensor1bytensor2 perform?",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "Y": "element-wise multiplication"
    },
    {
        "X": "Returns a new tensor with what of the elements ofinput?",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   ",
        "Y": "cosine"
    },
    {
        "X": "What returns a new tensor with the inverse hyperbolic sine of the elements ofinput?",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "Y": "Alias fortorch.asinh()"
    },
    {
        "X": "What does oftensor1bytensor2 multiply the result by?",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "Y": "scalarvalue"
    },
    {
        "X": "What is the element-wise angle of the elements ofinput?",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "Y": "arcsine"
    },
    {
        "X": "Who returns a new tensor with the arctangent of the elements ofinput?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "Alias fortorch.asinh()"
    },
    {
        "X": "What does Alias fortorch.atan() have?",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   ",
        "Y": "arctangent"
    },
    {
        "X": "What is the name of the new tensor with the arctangent of the elements ofinput?",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "Y": "Alias fortorch.asinh()"
    },
    {
        "X": "What is the element-wise multiplication?",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "Y": "oftensor1bytensor2"
    },
    {
        "X": "What is the element-wise angle of the given inputtensor?",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "Y": "radians"
    },
    {
        "X": "What does Alias fortorch.asin() return a new tensor with?",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   ",
        "Y": "inverse hyperbolic sine"
    },
    {
        "X": "What returns a new tensor with the arctangent of the elements ofinput?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.    ",
        "Y": "Alias fortorch.asinh()"
    },
    {
        "X": "Alias fortorch.asinh(). Returns a new tensor with what of the elements ofinput",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   ",
        "Y": "arctangent"
    },
    {
        "X": "Who returns a new tensor with the arcsine of elements ofinput?",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "Alias fortorch.asin"
    },
    {
        "X": "What is the inverse hyperbolic sine of elements ofinput?",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "Y": "arctangent"
    },
    {
        "X": "What is the name of the tensor that returns a new tensor with the inverse hyperbolic sine of the elements of",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "Y": "Alias fortorch.atanh()"
    },
    {
        "X": "What is the sine of the elements of input?",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "Y": "inverse hyperbolic"
    },
    {
        "X": "What is the inverse hyperbolic tangent of the elements ofinput?",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "arctangent"
    },
    {
        "X": "What returns a new tensor with the inverse hyperbolic tangent of the elements ofinput?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "Alias fortorch.atan()"
    },
    {
        "X": "What does Alias fortorch.atan() return a new tensor with?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()   Raisesinputto the power ofexponent, elementwise, in double precision.   Returns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.    ",
        "Y": "inverse hyperbolic tangent"
    },
    {
        "X": "What is the name of the element ofinput that returns a new tensor?",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   ",
        "Y": "arcsine"
    },
    {
        "X": "Who returns a new tensor with the inverse hyperbolic tangent of the elements ofinput?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "Alias fortorch.atan"
    },
    {
        "X": "What is the name of the tensor that returns a new tensor with the arctangent of the elements ofinput",
        "Z": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   ",
        "Y": "Alias fortorch.atan"
    },
    {
        "X": "What type of sine does Alias fortorch.asin() return?",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "inverse hyperbolic"
    },
    {
        "X": "What element of input does Alias fortorch.asinh() return?",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   ",
        "Y": "arctangent"
    },
    {
        "X": "What is the name of the new tensor with the inverse hyperbolic tangent of the elements ofinput?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "Alias fortorch.atanh"
    },
    {
        "X": "What does Alias fortorch.atanh() have?",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "Y": "inverse hyperbolic tangent"
    },
    {
        "X": "What is the name of the element-wise arctangent ofinputi/otheritextinput_i ",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   ",
        "Y": "Alias fortorch.atanh"
    },
    {
        "X": "What is the element-wise ofinputi/otheritextinput_i / textother",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "arctangent"
    },
    {
        "X": "What is the name of the function that returns a new tensor with the inverse hyperbolic sine of the elements ofinput?",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   ",
        "Y": "Alias fortorch.atanh()"
    },
    {
        "X": "With consideration of what is the arctangent ofinputi/otheri / textother_iinput",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "the quadrant"
    },
    {
        "X": "Computes what of the given input tensor?",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   ",
        "Y": "bitwise NOT"
    },
    {
        "X": "Returns a new tensor with what sine of the elements of input?",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   ",
        "Y": "inverse hyperbolic"
    },
    {
        "X": "What does Alias fortorch.asinh() return a new tensor with?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "arctangent"
    },
    {
        "X": "What is the tangent of elements ofinput?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "hyperbolic"
    },
    {
        "X": "What does Alias fortorch.atanh() do?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   ",
        "Y": "Computes the bitwise NOT of the given input tensor"
    },
    {
        "X": "What does Alias fortorch.asinh() do?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   ",
        "Y": "Computes the bitwise OR ofinputandother"
    },
    {
        "X": "What returns a new tensor with the arctangent of the elements of input?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   ",
        "Y": "Alias fortorch.asinh()"
    },
    {
        "X": "What returns a new tensor with the inverse hyperbolic tangent of the elements of input?",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   ",
        "Y": "Alias fortorch.atan()"
    },
    {
        "X": "What is the name of the function that returns a new tensor with the inverse hyperbolic tangent of the elements of input",
        "Z": "  Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   ",
        "Y": "Alias fortorch.atanh()"
    },
    {
        "X": "What is the indices of the maximum value of all elements in?",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "Y": "theinputtensor"
    },
    {
        "X": "What is the minimum value of a flattened tensor?",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "Y": "the flattened tensor or along a dimension"
    },
    {
        "X": "What returns the maximum value of each slice of theinputtensor in the given dimension(s)dim?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "Y": "the indices of the minimum value(s) of the flattened tensor or along a dimension"
    },
    {
        "X": "What happens if all elements ininputevaluate toTrue?",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "Y": "Tests"
    },
    {
        "X": "What is the maximum value of all elements in theinputtensor?",
        "Z": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "minimum value"
    },
    {
        "X": "What does the input tensor return?",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   ",
        "Y": "unique elements"
    },
    {
        "X": "What returns the indices of the maximum value of all elements in the inputtensor?",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "Y": "Returns the indices of the maximum value of all elements in theinputtensor"
    },
    {
        "X": "Returns what value of the inputtensor in the given dimension(s)dim?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "Y": "the minimum value of each slice"
    },
    {
        "X": "Returns the maximum value of all elements ininputtensor.",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "Y": "Tests if all elements ininputevaluate toTrue"
    },
    {
        "X": "Which element returns the maximum value of all elements ininputtensor?",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   ",
        "Y": "input tensor"
    },
    {
        "X": "Returns what value of all elements in the inputtensor?",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "Y": "minimum value"
    },
    {
        "X": "What returns the indices of the minimum value of each slice of theinputtensor in the given dimension(s)dim?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "Y": "the minimum value of each slice of theinputtensor in the given dimension(s)dim"
    },
    {
        "X": "What returns the minimum value of all elements in theinputtensor?",
        "Z": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "Returns the minimum value of all elements in theinputtensor"
    },
    {
        "X": "Where is the minimum value of all elements in theinputtensor?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "Y": "theinputtensor"
    },
    {
        "X": "What is the return of (input-other) elements in theinputtensor?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "Y": "p-norm"
    },
    {
        "X": "What returns the maximum value of each slice of the inputtensor in the given dimension(s)dim?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "Y": "the indices of the minimum value(s) of the flattened tensor or along a dimension"
    },
    {
        "X": "What does it do when all elements ininputevaluate toTrue?",
        "Z": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   ",
        "Y": "Tests if all elements ininputevaluate toTrue"
    },
    {
        "X": "Which element returns the maximum value of all elements in theinputtensor?",
        "Z": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ",
        "Y": "input tensor"
    },
    {
        "X": "What returns the minimum value of all elements in the inputtensor?",
        "Z": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "Returns the minimum value of all elements in theinputtensor"
    },
    {
        "X": "Returns the what of (input-other)?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   ",
        "Y": "p-norm"
    },
    {
        "X": "What returns the minimum value of each slice of theinputtensor in the given dimension(s)dim?",
        "Z": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "Y": "the minimum value of each slice of theinputtensor in the given dimension(s)dim"
    },
    {
        "X": "What is the return of the maximum value of each slice of theinputtensor in the given dimension(s)dim?",
        "Z": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ",
        "Y": "the minimum value of each slice of theinputtensor in the given dimension(s)dim"
    },
    {
        "X": "What does the input tensor test if all elements ininputevaluate to?",
        "Z": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "Tests if all elements ininputevaluate toTrue"
    },
    {
        "X": "What returns the maximum value of all elements in theinputtensor?",
        "Z": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "Returns the maximum value of all elements in theinputtensor"
    },
    {
        "X": "What is the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   ",
        "Y": "the log of summed exponentials"
    },
    {
        "X": "Returns what value of each slice of theinputtensor in the given dimension(s)dim?",
        "Z": "  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",
        "Y": "minimum value"
    },
    {
        "X": "What is returned for each row of the inputtensor in the given dimensiondim?",
        "Z": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "the p-norm of (input-other) Returns the log of summed exponentials"
    },
    {
        "X": "What is the minimum value of each slice of theinputtensor in the given dimension(s)dim?",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   ",
        "Y": "the flattened tensor or along a dimension"
    },
    {
        "X": "What is the minimum value of all elements in theinputtensor?",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "Y": "maximum value"
    },
    {
        "X": "What is the minimum value of all elements in?",
        "Z": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   ",
        "Y": "theinputtensor"
    },
    {
        "X": "What is the value of all elements in theinputtensor?",
        "Z": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "mean value"
    },
    {
        "X": "Returns what of each row of the inputtensor in the given dimensiondim?",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "the p-norm of (input-other) Returns the log of summed exponentials"
    },
    {
        "X": "How do all elements ininputevaluate toTrue?",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "Tests"
    },
    {
        "X": "What does the p-norm of (input-other) return?",
        "Z": "Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "the log of summed exponentials of each row of theinputtensor in the given dimensiondim"
    },
    {
        "X": "What is the mean value of all elements in theinputtensor?",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   ",
        "Y": "median"
    },
    {
        "X": "What does ignoringNaNvalues mean?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "the median of the values ininput"
    },
    {
        "X": "What does it test if all elements ininputevaluate toTrue?",
        "Z": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "Y": "Tests if all elements ininputevaluate toTrue"
    },
    {
        "X": "What is used to test if all elements ininputevaluate toTrue?",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "Y": "input tensor"
    },
    {
        "X": "Returns what value of the values ininput?",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   ",
        "Y": "median"
    },
    {
        "X": "Returns what value ignoringNaNvalues?",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "the median of the values ininput"
    },
    {
        "X": "What does theinputtensor return?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "the product of all elements in theinputtensor"
    },
    {
        "X": "What does the input tensor ignore?",
        "Z": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "the median of the values ininput"
    },
    {
        "X": "What returns the maximum value of all elements in the inputtensor?",
        "Z": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "input tensor"
    },
    {
        "X": "What does the inputtensor return?",
        "Z": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "the p-norm of (input-other)"
    },
    {
        "X": "What does the input tensor return ignoringNaNvalues?",
        "Z": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "the median of the values ininput"
    },
    {
        "X": "Returns what value of all elements in theinputtensor?",
        "Z": "  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",
        "Y": "mean value"
    },
    {
        "X": "Returns the median of the values ininput, ignoring what?",
        "Z": "Returns the maximum value of all elements in theinputtensor. Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   ",
        "Y": "NaNvalues"
    },
    {
        "X": "What returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returdtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   ",
        "Y": "the p-norm of (input-other) Returns the log of summed exponentials"
    },
    {
        "X": "What is the return of the median of the values ininput?",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   ",
        "Y": "the mean value of all elements in theinputtensor"
    },
    {
        "X": "What is the mode value of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   ",
        "Y": "a namedtuple"
    },
    {
        "X": "Returns what of the values ininput, ignoringNaNvalues?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "Y": "the median"
    },
    {
        "X": "What returns the mode value of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "Y": "namedtuple"
    },
    {
        "X": "What is the mode value of a given tensor?",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "Y": "matrix norm or vector norm"
    },
    {
        "X": "What does return the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "Y": "the p-norm"
    },
    {
        "X": "Returns what of the values ininput?",
        "Z": "  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",
        "Y": "median"
    },
    {
        "X": "Returns what of a given tensor?",
        "Z": "  Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim. ",
        "Y": "matrix norm or vector norm"
    },
    {
        "X": "What is the return of the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   ",
        "Y": "the mean value of all elements in theinputtensor"
    },
    {
        "X": "Returns what of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   ",
        "Y": "the log of summed exponentials"
    },
    {
        "X": "Returns what value in the inputtensor?",
        "Z": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   ",
        "Y": "mean value of all elements"
    },
    {
        "X": "What is the mean value of all elements in?",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ",
        "Y": "theinputtensor"
    },
    {
        "X": "What does ignoringNaNvalues mean value in theinputtensor?",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   ",
        "Y": "the median of the values ininput"
    },
    {
        "X": "What is the name of a value in a given dimensiondim?",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   ",
        "Y": "a namedtuple"
    },
    {
        "X": "What does a given tensor return?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "matrix norm or vector norm"
    },
    {
        "X": "Not a Numbers (NaNs) is treated as what?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "zero"
    },
    {
        "X": "What does not a Numbers return?",
        "Z": "Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "the product of all elements in theinputtensor"
    },
    {
        "X": "What returns the mode value of each row of the inputtensor in the given dimensiondim?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "namedtuple"
    },
    {
        "X": "What is the return of the values ininput?",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   ",
        "Y": "median"
    },
    {
        "X": "What is the name of a given tensor?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   ",
        "Y": "matrix norm or vector norm"
    },
    {
        "X": "What is the quantile of each row of theinputtensor?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "q-th"
    },
    {
        "X": "Returns the median of the values ininput, what?",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   ",
        "Y": "ignoringNaNvalues"
    },
    {
        "X": "Returns the what of a given tensor?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "matrix norm or vector norm"
    },
    {
        "X": "What are the quantiles of each row of theinputtensor along the dimensiondim?",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   ",
        "Y": "q-th"
    },
    {
        "X": "In what order does the indices sort a tensor along a given dimension?",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "ascending order by value"
    },
    {
        "X": "What does Computes element-wise equality?",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   ",
        "Y": "Trueif two tensors have the same size and elements"
    },
    {
        "X": "What is the name of a tensor?",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   ",
        "Y": "Computesinput"
    },
    {
        "X": "What is the name of the name of the tensor?",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   ",
        "Y": "Alias fortorch.gt()"
    },
    {
        "X": "How do two tensors have the same size and elements?",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   ",
        "Y": "Trueif two tensors have the same size and elements"
    },
    {
        "X": "What is the name of the tensor that has the same size and elements?",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "Alias fortorch.ge()"
    },
    {
        "X": "What does Alias fortorch.ge() use to return a new tensor with boolean elements representing if",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   ",
        "Y": "Computesinput"
    },
    {
        "X": "Who returns a new tensor with boolean elements representing if each element isfiniteor not?",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   ",
        "Y": "Alias fortorch.gt()"
    },
    {
        "X": "What represents if each element ofinputis close to the corresponding element ofother?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "boolean elements"
    },
    {
        "X": "What is the name of a new tensor with boolean elements representing if each element isfiniteor not?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "Alias fortorch.le()"
    },
    {
        "X": "What is the name of Alias fortorch.ge()?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "Y": "Computesinput"
    },
    {
        "X": "What is a new tensor with boolean elements representing if each element ofinputis close to the corresponding element of",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "Y": "Alias fortorch.gt()"
    },
    {
        "X": "What is a test if each element ofinputis positive or negative infinity?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "infinite"
    },
    {
        "X": "What is the test if each element ofinputis infinite?",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "Y": "positive infinity"
    },
    {
        "X": "What type of input does Alias fortorch.gt() use?",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "Y": "Computesinput"
    },
    {
        "X": "What is a new tensor with boolean elements representing if each element isfiniteor not?",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   ",
        "Y": "Alias fortorch.gt()"
    },
    {
        "X": "What is a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding",
        "Z": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "Y": "Alias fortorch.gt()"
    },
    {
        "X": "What represents if each element ofinputis \u201cclose\u201d to the corresponding element ofother?",
        "Z": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "Y": "boolean elements"
    },
    {
        "X": "What is the test if each element ofinputis infinite or not?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "positive infinity"
    },
    {
        "X": "What test does each element ofinputis negative infinity or not?",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "Y": "Tests"
    },
    {
        "X": "What is the name of the function that returns a new tensor with boolean elements?",
        "Z": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "Y": "Alias fortorch.gt()"
    },
    {
        "X": "What does a new tensor with boolean elements represent?",
        "Z": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "Y": "if each element ofinputis \u201cclose\u201d to the corresponding element ofother"
    },
    {
        "X": "Is each element ofinput positive or negative infinity or not?",
        "Z": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   ",
        "Y": "infinite"
    },
    {
        "X": "Tests if each element ofinputis what?",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "infinite"
    },
    {
        "X": "Tests if each element ofinputis what infinity or not?",
        "Z": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "negative"
    },
    {
        "X": "What is a positive or negative infinity test?",
        "Z": "Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "infinite"
    },
    {
        "X": "What does a new tensor with boolean elements represent if each element ofinputis negative infinity or not?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "Tests"
    },
    {
        "X": "What represents if each element ofinputis NaN or not?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "boolean elements"
    },
    {
        "X": "What represents if each element ofinputis real-valued or not?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "boolean elements"
    },
    {
        "X": "Returns a new tensor with what elements?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "boolean"
    },
    {
        "X": "What is the value of each element ofinput?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "infinite"
    },
    {
        "X": "Tests if each element ofinputis what or not?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "positive infinity"
    },
    {
        "X": "What is the infinity of each element ofinput?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "negative"
    },
    {
        "X": "Returns a new tensor with boolean elements representing if each element ofinputis what?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "NaN"
    },
    {
        "X": "Returns a new tensor with boolean elements representing if each element ofinput is what?",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "real-valued"
    },
    {
        "X": "What does a new tensor with boolean elements represent if each element ofinputis real-valued or not?",
        "Z": "Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "Tests"
    },
    {
        "X": "What is the value of each element of input?",
        "Z": "Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   ",
        "Y": "infinite"
    },
    {
        "X": "Returns a new tensor with what elements representing if each element ofinputis NaN or not?",
        "Z": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "boolean"
    },
    {
        "X": "Tests if each element ofinputis is what?",
        "Z": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "positive infinity"
    },
    {
        "X": "What is the test if each element ofinputis positive infinity or not?",
        "Z": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "negative infinity"
    },
    {
        "X": "What is thekth smallest element of each row of theinputtensor in the given dimensiondim?",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "a namedtuple(values,indices)"
    },
    {
        "X": "What is the name of the tensor wherevaluesis thekth smallest element of each row of the inputtensor in the",
        "Z": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "namedtuple"
    },
    {
        "X": "What happens if each element ofinputis negative infinity or not?",
        "Z": "Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "Tests"
    },
    {
        "X": "What is a namedtuple?",
        "Z": "Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().   ",
        "Y": "thekth smallest element of each row of theinputtensor in the given dimensiondim"
    },
    {
        "X": "What is the name of the tensor that returns a new tensor with boolean elements representing if each element of",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   ",
        "Y": "Alias fortorch.gt()"
    },
    {
        "X": "What is STFT?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "Short-time Fourier transform"
    },
    {
        "X": "What is the name of the short time Fourier Transform?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "Inverse"
    },
    {
        "X": "What is the name of the window function?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "Bartlett"
    },
    {
        "X": "What type of window function is Hamming window function?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "Blackman"
    },
    {
        "X": "What is the name of the blackman window function?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "Hamming"
    },
    {
        "X": "What is the name of the window function that computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "Hann"
    },
    {
        "X": "What does the Kaiser window have?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "window lengthwindow_lengthand shape parameterbeta"
    },
    {
        "X": "What is another name for Short-time Fourier transform?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "Inverse short time Fourier Transform"
    },
    {
        "X": "Which window function computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta?",
        "Z": "  Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. ",
        "Y": "Hann"
    },
    {
        "X": "What is a view of each input tensor with zero dimensions?",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   ",
        "Y": "3-dimensional"
    },
    {
        "X": "What is the view of each input tensor with zero dimensions?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "Y": "3-dimensional"
    },
    {
        "X": "What is the count of each value in an array of non-negative ints?",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   ",
        "Y": "frequency"
    },
    {
        "X": "What is a block diagonal matrix from provided tensors?",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   ",
        "Y": "Create a block diagonal matrix"
    },
    {
        "X": "Broadcasts the given tensors according to what?",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   ",
        "Y": "Broadcasting semantics"
    },
    {
        "X": "Broadcastsinputto what?",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   ",
        "Y": "shapeshape"
    },
    {
        "X": "Broadcastsinputto the shapeshape is similar tobroadcast_tensors() but for what?",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   ",
        "Y": "shapes"
    },
    {
        "X": "How many dimensions does a 3-dimensional view of each input tensor have?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "Y": "zero"
    },
    {
        "X": "What does Count each value in an array of non-negative ints?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "Y": "frequency"
    },
    {
        "X": "What is the name of a block diagonal matrix from provided tensors?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "Y": "Create a block diagonal matrix"
    },
    {
        "X": "What is the same asbroadcast_tensors()?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   ",
        "Y": "shapes"
    },
    {
        "X": "What are the indices of the buckets to which each value in theinputbelongs?",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "Y": "the boundaries of the buckets are set byboundaries"
    },
    {
        "X": "What doesbroadcast_tensors() look for?",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "Y": "shapes"
    },
    {
        "X": "What is the name of the given sequence of tensors?",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "Y": "Do cartesian product"
    },
    {
        "X": "What distance does a cartesian product batched between each pair of two collections of row vectors?",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "Y": "p-norm distance"
    },
    {
        "X": "What does a cartesian product of the given sequence of tensors return?",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   ",
        "Y": "copy ofinput"
    },
    {
        "X": "What is the name of the input to the shapeshape?",
        "Z": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "Y": "Broadcastsinput"
    },
    {
        "X": "Broadcastsinputto what shape?",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   ",
        "Y": "shapes"
    },
    {
        "X": "What does Broadcastsinput return?",
        "Z": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "Y": "the indices of the buckets"
    },
    {
        "X": "What is returned by Broadcastsinput to the shapeshape?",
        "Z": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "Y": "Do cartesian product of the given sequence of tensors"
    },
    {
        "X": "Computes batched what?",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   ",
        "Y": "p-norm distance between each pair of the two collections of row vectors"
    },
    {
        "X": "What is returned when the p-norm distance between each pair of row vectors is computed?",
        "Z": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "Y": "a copy ofinput"
    },
    {
        "X": "What does it do to return a copy ofinput?",
        "Z": "Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   ",
        "Y": "Compute combinations of lengthrrrof the given tensor"
    },
    {
        "X": "What is the product of the given sequence of tensors?",
        "Z": "Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) ofinputandother.   Computes the histogram of a tensor.   ",
        "Y": "Do cartesian"
    },
    {
        "X": "What is the function that returns the cross product of vectors in dimensiondimofinputandother?",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   ",
        "Y": "Compute combinations of lengthrrrof the given tensor"
    },
    {
        "X": "What does dimensiondimofinputandother return?",
        "Z": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "Y": "the cross product of vectors"
    },
    {
        "X": "What is the cumulative maximum of elements ofinputin the dimensiondim?",
        "Z": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "Y": "namedtuple"
    },
    {
        "X": "What returns the cumulative maximum of elements ofinputin the dimensiondim?",
        "Z": "Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) ofinputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) ofinputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements ofinputin the dimensiondim.   ",
        "Y": "a namedtuple(values,indices)"
    },
    {
        "X": "What returns the cumulative minimum of elements ofinputin the dimensiondim?",
        "Z": "Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.   Flattensinputby reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) ofinputandother.   Computes the histogram of a tensor.   TakeNNNtensors, each of which can be either scalar or 1-dimensional vector, and createNNNN-dimensional grids, where theiiithgrid is defined by expanding theiiithinput over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) ofinputandother.   Returns the logarithm of the cumulative summation of the exponentiation of elements ofinputin the dimensiondim.   ",
        "Y": "a namedtuple(values,indices)"
    },
    {
        "X": "What is returned when a namedtuple(values,indices) returns the cumulative minimum of elements ofinputin the dimensiondim",
        "Z": "Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   ",
        "Y": "cumulative product of elements ofinputin the dimensiondim"
    },
    {
        "X": "What does Returns the cumulative product of elements ofinputin the dimensiondim?",
        "Z": "Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "Y": "the cumulative sum of elements ofinputin the dimensiondim"
    },
    {
        "X": "What is the cumulative minimum of elements ofinputin the dimensiondim?",
        "Z": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "Y": "a namedtuple"
    },
    {
        "X": "What is the cumulative sum of elements ofinputin the dimensiondim?",
        "Z": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    ",
        "Y": "cumulative product"
    },
    {
        "X": "What does a namedtuple return?",
        "Z": "  Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   ",
        "Y": "cumulative sum of elements ofinputin the dimensiondim"
    },
    {
        "X": "What is a reduced matrix-matrix product of matrices stored inbatch1andbatch2?",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "Y": "add step"
    },
    {
        "X": "What does the batch matrix-matrix product of matrices stored inbatch1andbatch2 perform?",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "Y": "matrix multiplication"
    },
    {
        "X": "What does the vectorvec perform?",
        "Z": "Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   ",
        "Y": "matrix-vector product"
    },
    {
        "X": "What does the matrix-vector product perform?",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   ",
        "Y": "batch matrix-matrix product of matrices inbatch1andbatch2"
    },
    {
        "X": "What is the name of the matrices inbatch1andbatch2?",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "Y": "batch matrix-matrix product"
    },
    {
        "X": "What happens to all matrix multiplications along the first dimension?",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "Y": "all matrix multiplications get accumulated along the first dimension"
    },
    {
        "X": "What does the matrix multiplication of matrices perform?",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "Y": "matricesmat1andmat2"
    },
    {
        "X": "What product of the matrixmatand the vectorvec?",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   ",
        "Y": "matrix-vector product"
    },
    {
        "X": "What happens to the outer-product of vectorsvec1andvec2?",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   ",
        "Y": "adds it to the matrixinput"
    },
    {
        "X": "What is the name of the matrices that performs a batch matrix-matrix product?",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   ",
        "Y": "inbatch1andbatch2"
    },
    {
        "X": "What does the matricesmat1andmat2 perform?",
        "Z": "Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   ",
        "Y": "matrix multiplication"
    },
    {
        "X": "What is the matrix-vector product of vectorsvec1andvec2?",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   ",
        "Y": "outer-product"
    },
    {
        "X": "What is the name of the batch matrix-matrix product of matrices stored ininputandmat2?",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "Y": "Alias"
    },
    {
        "X": "What is a batch matrix-matrix product of matrices stored?",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "ininputandmat2"
    },
    {
        "X": "What is the matrix product of?",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "Y": "NNN2-D"
    },
    {
        "X": "What decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   ",
        "Y": "Cholesky"
    },
    {
        "X": "What is the outer-product of?",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "Y": "vectorsvec1andvec2"
    },
    {
        "X": "What does a batch matrix-matrix product of matrices inbatch1andbatch2 perform?",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   ",
        "Y": "batch matrix-matrix product of matrices inbatch1andbatch2"
    },
    {
        "X": "What is the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "Y": "Cholesky"
    },
    {
        "X": "What is the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?",
        "Z": " Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   ",
        "Y": "Cholesky factor matrixuuu"
    },
    {
        "X": "What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuuu",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   ",
        "Y": "Solves"
    },
    {
        "X": "What is used to perform a batch matrix-matrix product?",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "matrices stored ininputandmat2"
    },
    {
        "X": "What returns the matrix product of theNNN2-D tensors?",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   ",
        "Y": "Returns the matrix product of theNNN2-D tensors"
    },
    {
        "X": "Computes the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "Y": "Cholesky"
    },
    {
        "X": "Computes the inverse of a symmetric positive-definite matrixAAAusing what?",
        "Z": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu(). ",
        "Y": "Cholesky factoruuu"
    },
    {
        "X": "Solves what with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ",
        "Y": "a linear system of equations"
    },
    {
        "X": "What matrix product does the Cholesky decomposition of a symmetric positive-definite matrixAAAor return?",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ",
        "Y": "NNN2-D tensors"
    },
    {
        "X": "What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ",
        "Y": "Solves"
    },
    {
        "X": "What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ",
        "Y": "dot product of two 1D tensors"
    },
    {
        "X": "Returns what of theNNN2-D tensors?",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "Y": "matrix product"
    },
    {
        "X": "What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu return?",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ",
        "Y": "matrixinv"
    },
    {
        "X": "What does the Cholesky decomposition of for batches of symmetric positive-definite matrices?",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ",
        "Y": "symmetric positive-definite matrixAAAor"
    },
    {
        "X": "What is the inverse of a symmetric positive-definite matrixAAAor?",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   ",
        "Y": "returns matrixinv"
    },
    {
        "X": "What is the Cholesky factor matrixuuu?",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   ",
        "Y": "dot product of two 1D tensors"
    },
    {
        "X": "What is a real square matrix?",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "Y": "eigenvalues and eigenvectors"
    },
    {
        "X": "What is returned by Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   ",
        "Y": "matrixinv"
    },
    {
        "X": "Computes what of a real square matrix?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "Y": "eigenvalues and eigenvectors"
    },
    {
        "X": "What does AAAusing's Cholesky factor matrixuuu compute?",
        "Z": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   ",
        "Y": "dot product of two 1D tensors"
    },
    {
        "X": "What does a real square matrix have?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   ",
        "Y": "eigenvalues and eigenvectors"
    },
    {
        "X": "What is a low-level function for calling directly?",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "Y": "LAPACK\u2019s geqrf"
    },
    {
        "X": "What is a low-level function for calling LAPACK's geqrf directly?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "Y": "Alias oftorch.outer()"
    },
    {
        "X": "What does Alias oftorch.outer() compute the dot product for?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "Y": "1D tensors"
    },
    {
        "X": "What is the matrixuuu of a positive semidefinite matrix?",
        "Z": " Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   ",
        "Y": "Cholesky factor"
    },
    {
        "X": "What is the dot product of two 1D tensors?",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   ",
        "Y": "dot product of two 1D tensors"
    },
    {
        "X": "What is this function for calling LAPACK's geqrf directly?",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   ",
        "Y": "low-level function"
    },
    {
        "X": "What is the low-level function for calling LAPACK's geqrf directly?",
        "Z": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   ",
        "Y": "Alias oftorch.outer()"
    },
    {
        "X": "Computes the dot product for what?",
        "Z": "Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   ",
        "Y": "1D tensors"
    },
    {
        "X": "What is a positive semidefinite matrix to be inverted?",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "Y": "Cholesky factor matrixuuu"
    },
    {
        "X": "Computes what dot product of two 1D tensors?",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "Y": "dot product of two 1D tensors"
    },
    {
        "X": "Computes the eigenvalues and eigenvectors of what?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "Y": "real square matrix"
    },
    {
        "X": "What is a low-level function for calling LAPACK's geqrf?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   ",
        "Y": "Alias oftorch.outer()"
    },
    {
        "X": "What is the name of Alias fortorch.linalg.det()?",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "Y": "Alias fortorch.linalg.inv"
    },
    {
        "X": "A linear system of equations with a positive semidefinite matrix to be inverted given what matrix?",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "Y": "Cholesky factor matrix"
    },
    {
        "X": "What is the low-level function for calling LAPACK\u2019s geqrf directly?",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "Alias oftorch.outer()"
    },
    {
        "X": "What is a low-level function for calling LAPACK\u2019s geqrf directly?",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   ",
        "Y": "Computes the dot product of two 1D tensors"
    },
    {
        "X": "What is this function for calling LAPACK\u2019s geqrf directly?",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "low-level function"
    },
    {
        "X": "What does Alias fortorch.linalg.det() call?",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   ",
        "Y": "Alias fortorch.linalg.inv()"
    },
    {
        "X": "Computes the eigenvalues and eigenvectors of a real square matrix?",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "dot product"
    },
    {
        "X": "What are the numbers of a real square matrix?",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "eigenvalues and eigenvectors"
    },
    {
        "X": "What does this function do for calling LAPACK's geqrf directly?",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "low-level function"
    },
    {
        "X": "What does Alias fortorch.linalg.det() calculate?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a namedtuplesolution, LU.   Computes the singular value decomposition of either a matrix or batch of matricesinput.   ",
        "Y": "log determinant of a square matrix or batches of square matrices"
    },
    {
        "X": "Who calculates log determinant of a square matrix or batches of square matrices?",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "Alias fortorch.linalg.slogdet"
    },
    {
        "X": "What calculates the log determinant of a square matrix or batches of square matrices?",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   ",
        "Y": "Alias fortorch.linalg.slogdet()"
    },
    {
        "X": "What is the eigenvalues and eigenvectors of a real square matrix?",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   ",
        "Y": "low-level function"
    },
    {
        "X": "Computes the dot product for 1D tensors?",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   ",
        "Y": "Alias oftorch.outer()"
    },
    {
        "X": "What does Alias oftorch.outer() do?",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.   ",
        "Y": "Computes the dot product for 1D tensors"
    },
    {
        "X": "What problems does Alias fortorch.linalg.slogdet() solve?",
        "Z": "  Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   ",
        "Y": "least squares and least norm problems"
    },
    {
        "X": "What computes the dot product for 1D tensors?",
        "Z": "  Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   ",
        "Y": "Alias oftorch.outer()"
    },
    {
        "X": "Computes what of a matrix or batches of matricesA?",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   ",
        "Y": "LU factorization"
    },
    {
        "X": "What is the tensor filled with?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. ",
        "Y": "scalar value0"
    },
    {
        "X": "As of 0.4, this function does not support what?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "anoutkeyword"
    },
    {
        "X": "What is equivalent totorch.zeros?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "oldtorch.zeros_like"
    },
    {
        "X": "What is the size ofinput that determines the size of the output tensor?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. ",
        "Y": "input(Tensor)"
    },
    {
        "X": "What returns a tensor filled with the scalar value0?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "a tensor filled with the scalar value0"
    },
    {
        "X": "What is equivalent to totorch.zeros(input.size(),out=output)?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. ",
        "Y": "oldtorch.zeros_like"
    },
    {
        "X": "What determines the size of the output tensor?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "input(Tensor)"
    },
    {
        "X": "Warning As of 0.4, this function does not support what?",
        "Z": "Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. ",
        "Y": "anoutkeyword"
    },
    {
        "X": "What is the oldtorch.zeros_like equivalent?",
        "Z": "As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. ",
        "Y": "totorch.zeros"
    },
    {
        "X": "What is the size ofinput?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "input(Tensor)"
    },
    {
        "X": "What is the default to the dtype ofinput?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "ifNone"
    },
    {
        "X": "What is the desired layout of tensor?",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "layout"
    },
    {
        "X": "What is the default to the layout ofinput?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "ifNone"
    },
    {
        "X": "As of what date does this function not support anoutkeyword?",
        "Z": "As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. ",
        "Y": "0.4"
    },
    {
        "X": "What is equivalent totorch.zeros(input.size(),out=output)?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "oldtorch.zeros_like"
    },
    {
        "X": "What defaults to the dtype of input?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "ifNone"
    },
    {
        "X": "What is the desired layout of returned tensor?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "layout"
    },
    {
        "X": "What defaults to the layout of input?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "ifNone"
    },
    {
        "X": "What is the desired device of returned tensor?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "device(torch.device, optional)"
    },
    {
        "X": "What is the default to the device ofinput?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "ifNone"
    },
    {
        "X": "What determines size of output tensor?",
        "Z": "input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. ",
        "Y": "input(Tensor)"
    },
    {
        "X": "What does ifNone default to?",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "device ofinput"
    },
    {
        "X": "What should record operations on the returned tensor?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "autograd"
    },
    {
        "X": "What is the default for a tensor?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "False"
    },
    {
        "X": "What defaults to the device of input?",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "ifNone"
    },
    {
        "X": "What is the default setting for autograd to record operations on the returned tensor?",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "Y": "False"
    },
    {
        "X": "What is the name of the object that gets the current device of a Generator?",
        "Z": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "Y": "torch.Generator object"
    },
    {
        "X": "Generator.device -> device Gets what of the generator?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "current device"
    },
    {
        "X": "What does atorch.Generator object do?",
        "Z": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "Y": "Sets the seed for generating random numbers"
    },
    {
        "X": "What are remapped to positive values with the formula0xfff_fff_fff_fff + seed?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "Negative inputs"
    },
    {
        "X": "What is an example of a generator that gets a non-deterministic random number from std::random_device?",
        "Z": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "Y": "torch.Generator object"
    },
    {
        "X": "What does a generator get a non-deterministic random number from?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "std::random_device or the current time"
    },
    {
        "X": "What does the generator do?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "Sets the Generator state"
    },
    {
        "X": "What object is used to seed a Generator?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "torch.Generator"
    },
    {
        "X": "What does it do when a generator is seeded?",
        "Z": "Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "Y": "Sets the Generator state"
    },
    {
        "X": "What does generator.device -> device get?",
        "Z": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "Y": "current device"
    },
    {
        "X": "What object is used to get a non-deterministic random number from std::random_device or the current time?",
        "Z": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "Y": "torch.Generator"
    },
    {
        "X": "What does a Generator get a non-deterministic random number from?",
        "Z": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. ",
        "Y": "std::random_device or the current time"
    },
    {
        "X": "What is the desired state?",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state. ",
        "Y": "new_state"
    },
    {
        "X": "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "Y": "non_blocking(bool)"
    },
    {
        "X": "What type of storage does this storage cast to?",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "float type"
    },
    {
        "X": "Everytorch.Tensorcasts this storage to what type of type?",
        "Z": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "bfloat16"
    },
    {
        "X": "What type of storage does everytorch.Tensorcast?",
        "Z": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "float type"
    },
    {
        "X": "Casts this storage to what type Casts this storage to bool type Casts this storage to bool type Casts this storage to ",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "Y": "bfloat16"
    },
    {
        "X": "What type of storage does Casts this storage to bool type Casts this storage to char type Returns?",
        "Z": "Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "a copy"
    },
    {
        "X": "IfsharedisTrue, what happens?",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "Y": "memory is shared between all processes"
    },
    {
        "X": "IfsharedisTrue, then memory is shared between all processes.",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "Y": "All changes are written to the file"
    },
    {
        "X": "What type of storage does not affect the file?",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "What does Casts this storage to char type Return?",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "a copy"
    },
    {
        "X": "What does Casts this storage to char type return?",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "a copy"
    },
    {
        "X": "Where is a copy of this storage stored?",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "CUDA memory"
    },
    {
        "X": "IfTrue and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "non_blocking(bool)"
    },
    {
        "X": "What type of memory is shared between all processes?",
        "Z": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "Y": "IfsharedisTrue"
    },
    {
        "X": "What type of storage does the changes on the storage not affect the file?",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "What is the name of the function that returns a copy of the object in CUDA memory?",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "Y": "non_blocking(bool)"
    },
    {
        "X": "If the object is already in what memory and on the correct device, then no copy is performed and the original object is returned?",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "Y": "CUDA memory"
    },
    {
        "X": "What type of argument has no effect?",
        "Z": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "non_blocking"
    },
    {
        "X": "What is the name of the argument that asynchronous with respect to the host?",
        "Z": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type ",
        "Y": "non_blocking(bool)"
    },
    {
        "X": "What happens if sharedisTrue?",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "Y": "memory is shared between all processes"
    },
    {
        "X": "If the changes on the storage do not affect the file, what is the name of the file that must contain at leastsize * sizeof(Type)",
        "Z": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "If the file must contain at leastsize * sizeof(Type)bytes (Typeis the type of storage)?",
        "Z": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "What does filename(str) mean?",
        "Z": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "filename(str) \u2013 file name to map shared"
    },
    {
        "X": "What does it do to the storage if it's not already pinned?",
        "Z": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. ",
        "Y": "Moves the storage to shared memory"
    },
    {
        "X": "What type of storage does Casts this storage to?",
        "Z": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "Y": "float"
    },
    {
        "X": "What type of storage does the changes on the storage do not affect the file?",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "Returns a list containing the elements of what storage?",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "Y": "self Casts this storage to short type"
    },
    {
        "X": "What is the name of the file that must contain at leastsize * sizeof(Type)bytes?",
        "Z": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "Y": "IfsharedisFalse"
    },
    {
        "X": "What Casts this storage to short type Returns a list containing the elements of this storage?",
        "Z": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. Ion GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "self"
    },
    {
        "X": "When do gradients with respect toUandV depend onS1?",
        "Z": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "Y": "when the matrix has small singular values"
    },
    {
        "X": "What may be multiplied by for complex-valuedinput?",
        "Z": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "Y": "arbitrary phase factor"
    },
    {
        "X": "What happens when inputhas multiple singular values?",
        "Z": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "Y": "repeated singular values"
    },
    {
        "X": "What may be multiplied byUandV?",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "Y": "arbitrary phase factor"
    },
    {
        "X": "When is the singular value decomposition not unique?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "wheninputhas repeated singular values"
    },
    {
        "X": "What setting controls whether to compute the reduced or full decomposition, and consequently, the shape of returnedUandV?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "Default:True"
    },
    {
        "X": "What controls whether to computeUandV?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "compute_uv"
    },
    {
        "X": "What is the default value for compute_uv?",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example: ",
        "Y": "Default:True"
    },
    {
        "X": "What is a float tensor converted to?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "quantized tensor"
    },
    {
        "X": "What has to be the desired data type of returned tensor?",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "Y": "one of the quantized dtypes"
    },
    {
        "X": "When inputis on CUDA,torch.eig()causes what?",
        "Z": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing ",
        "Y": "host-device synchronization"
    },
    {
        "X": "Where is each row an eigenvalue of input?",
        "Z": "input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "Y": "the first element is the real part and the second element is the imaginary part"
    },
    {
        "X": "Ifeigenvectors=False, what is it?",
        "Z": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example: ",
        "Y": "empty tensor"
    },
    {
        "X": "What can shape(nn)(n times n)(nn) be used to compute?",
        "Z": "Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example: ",
        "Y": "normalized (unit length) eigenvectors"
    },
    {
        "X": "If the correspondingeigenvalues[j]is a what?",
        "Z": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example: ",
        "Y": "real number"
    },
    {
        "X": "If the correspondingeigenvalues[j]andeigenvalues[j + 1]form what?",
        "Z": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example: ",
        "Y": "a complex conjugate pair"
    },
    {
        "X": "What could a map-style dataset read from a folder on the disk?",
        "Z": "At the heart of PyTorch data loading utility is thetorch.utils.data.DataLoaderclass.  It represents a Python iterable over a dataset, with support for map-style and iterable-style datasets, customizing data loading order, automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. ",
        "Y": "theidx-th image and its corresponding label"
    },
    {
        "X": "Who configures the options for map-style and iterable-style datasets?",
        "Z": "map-style and iterable-style datasets, customizing data loading order, automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time). ",
        "Y": "the constructor arguments of aDataLoader"
    },
    {
        "X": "What is the name of a dataset that could read theidx-th image and its corresponding label from a folder on the disk?",
        "Z": "For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "Y": "SeeDataset"
    },
    {
        "X": "Automatic batching, single- and multi-process data loading, and what other option are configured by the constructor arguments of aDataLoa",
        "Z": "automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. ",
        "Y": "automatic memory pinning"
    },
    {
        "X": "How are the options configured?",
        "Z": "single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. ",
        "Y": "the constructor arguments of aDataLoader"
    },
    {
        "X": "What is an instance of a subclass ofIterableDataset that implements the__iter__()protocol?",
        "Z": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "Y": "iterable-style dataset"
    },
    {
        "X": "What are iterable-style datasets particularly suitable for?",
        "Z": "An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "Y": "cases where random reads are expensive or even improbable"
    },
    {
        "X": "What is the name of a map-style dataset that could read theidx-th image and its corresponding label from a folder on",
        "Z": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. ",
        "Y": "SeeDataset"
    },
    {
        "X": "When using anIterableDatasetwith what?",
        "Z": "For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "Y": "multi-process data loading"
    },
    {
        "X": "What is the name of the dataset that implements the__iter__()protocol?",
        "Z": "SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "Y": "SeeDataset"
    },
    {
        "X": "What is anIterableDatasetwith?",
        "Z": "The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time). ",
        "Y": "multi-process data loading"
    },
    {
        "X": "What document describes how to avoid duplicated data when using multi-process data loading?",
        "Z": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. ",
        "Y": "SeeIterableDatasetdocumentations"
    },
    {
        "X": "How is the data loading order controlled by the user-defined iterable?",
        "Z": "The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time). ",
        "Y": "yielding a batched sample at each time"
    },
    {
        "X": "What are sampleclasses used to do?",
        "Z": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note ",
        "Y": "represent iterable objects over the indices to datasets"
    },
    {
        "X": "What can aSampler do in the common case with stochastic gradient decent?",
        "Z": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note ",
        "Y": "randomly permute a list of indices and yield each one at a time"
    },
    {
        "X": "How will a sequential or shuffled sampler be constructed?",
        "Z": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note ",
        "Y": "based on theshuffleargument to aDataLoader"
    },
    {
        "X": "Who supports automatically collating individual fetched data samples into batches?",
        "Z": "A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). ",
        "Y": "DataLoader"
    },
    {
        "X": "What are batched samples?",
        "Z": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note ",
        "Y": "containing Tensors"
    },
    {
        "X": "What yields batched samples instead of individual samples whenbatch_size(default1) is notNone?",
        "Z": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note ",
        "Y": "data loader"
    },
    {
        "X": "What type of datasets can users alternatively specifybatch_sampler?",
        "Z": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note ",
        "Y": "map-style datasets"
    },
    {
        "X": "In some cases, users may want to handle batching what?",
        "Z": "and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "Y": "manually in dataset code"
    },
    {
        "X": "What type of data would it be cheaper to load manually?",
        "Z": "and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "Y": "batched data"
    },
    {
        "X": "Who returns each member of thedatasetobject?",
        "Z": "After fetching a list of samples using the indices from sampler, the function\npassed as thecollate_fnargument is used to collate lists of samples\ninto batches. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. ",
        "Y": "the data loader"
    },
    {
        "X": "In certain cases, users may want to handle batching what?",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "Y": "manually"
    },
    {
        "X": "What is used to collate samples?",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. ",
        "Y": "automatic batching"
    },
    {
        "X": "What is disabled when bothbatch_sizeandbatch_samplerareNone(default value forbatch_samplerareN",
        "Z": "In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. ",
        "Y": "automatic batching"
    },
    {
        "X": "Each sample obtained from thedatasetis processed with the function passed as what?",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "Y": "thecollate_fnargument"
    },
    {
        "X": "What could be cheaper to handle batching manually in dataset code?",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "Y": "directly load batched data"
    },
    {
        "X": "What is the function passed to each sample obtained from thedataset?",
        "Z": "When fetching fromiterable-style datasetswithmulti-processing, thedrop_lastargument drops the last non-full batch of each worker\u2019s dataset replica. After fetching a list of samples using the indices from sampler, the function\npassed as thecollate_fnargument is used to collate lists of samples\ninto batches. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. ",
        "Y": "thecollate_fnargument"
    },
    {
        "X": "What does the defaultcollate_fnsimply do when automatic batching is disabled?",
        "Z": "When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "Y": "defaultcollate_fnsimply converts NumPy arrays into PyTorch Tensors"
    },
    {
        "X": "What is roughly equivalent to loading from when automatic batching is disabled?",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "Y": "map-style dataset"
    },
    {
        "X": "When automatic batching is disabled,collate_fnis called with what?",
        "Z": "When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. ",
        "Y": "each individual data sample"
    },
    {
        "X": "What is expected when automatic batching is enabled?",
        "Z": "In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. For instance, if each data sample consists of a 3-channel image and an integral\nclass label, i.e., each element of the dataset returns a tuple(image,class_index), the defaultcollate_fncollates a list of\nsuch tuples into a single tuple of a batched image tensor and a batched class\nlabel Tensor. In particular, the defaultcollate_fnhas the following\nproperties: It always prepends a new dimension as the batch dimension.",
        "Y": "to collate the input samples into a batch"
    },
    {
        "X": "How many subprocesses to use for data loading?",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "how many subprocesses to use for data loading"
    },
    {
        "X": "What is the name of the main process that will be loaded in the main process?",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "Y": "default:0"
    },
    {
        "X": "What can be used to define the strategy to draw samples from the dataset?",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "Y": "anyIterablewith__len__implemented"
    },
    {
        "X": "What must not be specified if specified?",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "Y": "shuffle"
    },
    {
        "X": "What is the name of how many subprocesses to use for data loading?",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "Y": "num_workers"
    },
    {
        "X": "What is the default value for data loading?",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) ",
        "Y": "default:0"
    },
    {
        "X": "What is the name of the sampler that likesampler, but returns a batch of indices at a time?",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. ",
        "Y": "batch_sampler"
    },
    {
        "X": "What is the mutually exclusive feature of batch_sampler?",
        "Z": "batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "Y": "withbatch_size,shuffle,sampler, anddrop_last"
    },
    {
        "X": "What does collate_fn merge a list of samples to form?",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. ",
        "Y": "a mini-batch of Tensor"
    },
    {
        "X": "What is used when using a map-style dataset?",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "Y": "batched loading"
    },
    {
        "X": "What is num_workers?",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. When called in a worker, this returns an object guaranteed to have the\nfollowing attributes: id: the current worker id. num_workers: the total number of workers. seed: the random seed set for the current worker. This value is\ndetermined by main process RNG and the worker id. SeeDataLoader\u2019s documentation for more details. dataset: the copy of the dataset object inthisprocess. Note\nthat this will be a different object in a different process than the one\nin the main process. When called in the main process, this returnsNone. Note When used in aworker_init_fnpassed over toDataLoader, this method can be useful to\nset up each worker process differently, for instance, usingworker_idto configure thedatasetobject to only read a specific fraction of a\nsharded dataset, or useseedto seed other libraries used in dataset\ncode. Randomly split a dataset into non-overlapping new datasets of given lengths.\nOptionally fix the generator for reproducible results, e.g.: dataset(Dataset) \u2013 Dataset to be split ",
        "Y": "total number of workers"
    },
    {
        "X": "What merges a list of samples to form a mini-batch of Tensor(s)?",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "collate_fn"
    },
    {
        "X": "What type of dataset is collate_fn used when using batched loading?",
        "Z": "batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. ",
        "Y": "map-style dataset"
    },
    {
        "X": "When using batched loading from what type of dataset?",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) ",
        "Y": "map-style dataset"
    },
    {
        "X": "What merges a list of samples to form a mini-batch of Tensors?",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. ",
        "Y": "collate_fn"
    },
    {
        "X": "What is collate_fn used when using?",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. ",
        "Y": "batched loading"
    },
    {
        "X": "What is used when using batched loading from a map-style dataset?",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "Y": "pin_memory"
    },
    {
        "X": "If your data elements are what?",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. ",
        "Y": "a custom type"
    },
    {
        "X": "What pinned memory does the data loader copy Tensors into?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) ",
        "Y": "CUDA"
    },
    {
        "X": "What type of data elements does yourcollate_fn return a batch that is a custom type?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) ",
        "Y": "a custom type"
    },
    {
        "X": "What is the name of the data loader that will copy Tensors into CUDA pinned memory before returning them?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "Y": "pin_memory"
    },
    {
        "X": "If your data elements are a what type?",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "Y": "custom type"
    },
    {
        "X": "What is bool,optional?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "pin_memory"
    },
    {
        "X": "What is the size of the dataset not divisible by the batch size?",
        "Z": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) ",
        "Y": "IfFalseand the size of dataset is not divisible by the batch size"
    },
    {
        "X": "What is the name of the name of a batch that is not divisible by the batch size?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "Y": "default:False"
    },
    {
        "X": "What type of data elements does yourcollate_fnreturns a batch that is a custom type?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "Y": "a custom type"
    },
    {
        "X": "What is set toTrue to drop the last incomplete batch if the dataset size is not divisible by the batch size?",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "drop_last"
    },
    {
        "X": "If the size of dataset is not divisible by the batch size, what happens?",
        "Z": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. ",
        "Y": "the last batch will be smaller"
    },
    {
        "X": "What is the default setting for the size of a dataset?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) ",
        "Y": "default:False"
    },
    {
        "X": "What is set toTrueto drop the last incomplete batch?",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "drop_last"
    },
    {
        "X": "What is the timeout value for collecting a batch from workers?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "timeout"
    },
    {
        "X": "What should the timeout value for collecting a batch from workers always be?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "non-negative"
    },
    {
        "X": "What is the default timeout value for collecting a batch from workers?",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "Y": "0"
    },
    {
        "X": "What will be smaller if the dataset size is not divisible by the batch size?",
        "Z": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) ",
        "Y": "the last batch"
    },
    {
        "X": "What is the default value for worker_init_fn?",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) ",
        "Y": "default:None"
    },
    {
        "X": "What will be called on each worker subprocess with the worker id (an int in[0,num_workers-1]) as",
        "Z": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "worker_init_fn"
    },
    {
        "X": "If notNone, worker_init_fn(callable,optional) will be called on each worker subprocess with what input?",
        "Z": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) ",
        "Y": "worker id"
    },
    {
        "X": "Who will use this RNG to generate random indexes?",
        "Z": "timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "Y": "RandomSampler"
    },
    {
        "X": "What is the default value of generator(torch.Generator,optional)?",
        "Z": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) ",
        "Y": "default:None"
    },
    {
        "X": "Who will use generator to generate random indexes and multiprocessing to generatebase_seedfor workers?",
        "Z": "generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "RandomSampler"
    },
    {
        "X": "What is int,optional,keyword-only arg?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "prefetch_factor"
    },
    {
        "X": "How many num_workers samples will be prefetched across all workers?",
        "Z": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) ",
        "Y": "2"
    },
    {
        "X": "How many prefetch_factors are there?",
        "Z": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning ",
        "Y": "2"
    },
    {
        "X": "What does the data loader not shutdown the worker processes after a dataset has been consumed once?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "IfTrue"
    },
    {
        "X": "What does the data loader allow to keep alive after a dataset has been consumed once?",
        "Z": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "Datasetinstances"
    },
    {
        "X": "What is the name of the word \"False\"?",
        "Z": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning ",
        "Y": "Warning"
    },
    {
        "X": "What is the term for persistent_workers?",
        "Z": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "IfTrue"
    },
    {
        "X": "What is a worker_init_fncannot be an unpicklable object?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "lambda function"
    },
    {
        "X": "Where is Multiprocessing best practiceson more details related to multiprocessing?",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "PyTorch"
    },
    {
        "X": "What does PyTorch have to do with multiprocessing best practices?",
        "Z": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What does persistent_workers allow to keep alive?",
        "Z": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "workersDatasetinstances"
    },
    {
        "X": "Worker_init_fncannot be an unpicklable object, e.g., a lambda function.",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "Y": "thespawnstart method"
    },
    {
        "X": "Where can you find more details about multiprocessing?",
        "Z": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "PyTorch"
    },
    {
        "X": "What does PyTorch do when it comes to multiprocessing?",
        "Z": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What is an example of a function that is not unpicklable?",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "lambda function"
    },
    {
        "X": "Where is the Multiprocessing best practiceson more details related to multiprocessing?",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "PyTorch"
    },
    {
        "X": "What is an example of a spawnstart method?",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "Warning"
    },
    {
        "X": "Worker_init_fncannot be an unpicklable object, e.g., a lambda function?",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "thespawnstart method"
    },
    {
        "X": "Where can you find more information about multiprocessing?",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "PyTorch"
    },
    {
        "X": "What does worker_init_fncannot be an unpicklable object?",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "lambda function"
    },
    {
        "X": "What is the name of the multiprocessing best practices?",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning ",
        "Y": "PyTorch"
    },
    {
        "X": "Worker_init_fncannot be an unpicklable object, e.g., what?",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "Y": "a lambda function"
    },
    {
        "X": "What is the warning len(dataloader)heuristic based on?",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "Y": "the length of the sampler used"
    },
    {
        "X": "Whendatasetis what, it instead returns an estimate based on onlen(dataset)/batch_size?",
        "Z": "len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. ",
        "Y": "anIterableDataset"
    },
    {
        "X": "What does PyTorch trust in handling multi-process loading?",
        "Z": "Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "userdatasetcode"
    },
    {
        "X": "What is len(dataloader)heuristic based on?",
        "Z": "len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "Y": "the length of the sampler used"
    },
    {
        "X": "When can more than one batch worth of samples be dropped?",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "Y": "whendrop_lastis set"
    },
    {
        "X": "Which type of dataset can not detect such cases in general?",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "Y": "PyTorch"
    },
    {
        "X": "What interacts with Multi-process data loading?",
        "Z": "Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "howIterableDataset"
    },
    {
        "X": "What can happen to an otherwise complete batch?",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "Y": "broken into multiple ones"
    },
    {
        "X": "What program can not detect sharding cases in general?",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "Y": "PyTorch"
    },
    {
        "X": "What does howIterableDataset interact with?",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning ",
        "Y": "Multi-process data loading"
    },
    {
        "X": "What interacts withMulti-process data loading?",
        "Z": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. ",
        "Y": "howIterableDataset"
    },
    {
        "X": "What type of numbers do My data loader workers return?",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "Y": "random"
    },
    {
        "X": "What class represents aDataset?",
        "Z": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "abstract class"
    },
    {
        "X": "What does My data loader workers return identical random numbers?",
        "Z": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "Y": "Warning SeeReproducibility"
    },
    {
        "X": "What do multi-process data loadingnotes return for random seed related questions?",
        "Z": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "Y": "Randomness"
    },
    {
        "X": "What does an abstract class represent?",
        "Z": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "Y": "aDataset"
    },
    {
        "X": "What does SeeReproducibility andMy data loader workers return?",
        "Z": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. ",
        "Y": "random numbers"
    },
    {
        "X": "Randomness in multi-process data loadingnotes for what?",
        "Z": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "Y": "random seed related questions"
    },
    {
        "X": "All datasets that represent what should subclass it?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "a map from keys to data samples"
    },
    {
        "X": "What should all subclasses overwrite__getitem__() support?",
        "Z": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "fetching a data sample for a given key"
    },
    {
        "X": "What are subclasses expected to overwrite__len__()?",
        "Z": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. ",
        "Y": "manySamplerimplementations and the default options ofDataLoader"
    },
    {
        "X": "What type of class represents aDataset?",
        "Z": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note ",
        "Y": "abstract class"
    },
    {
        "X": "All datasets that represent a map from keys to data samples should what?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "subclass it"
    },
    {
        "X": "What should all subclasses do to support fetching a data sample for a given key?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "overwrite__getitem__()"
    },
    {
        "X": "What could subclasses optionally do?",
        "Z": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. ",
        "Y": "overwrite__len__()"
    },
    {
        "X": "What should all subclasses overwrite to overwrite__len__()?",
        "Z": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note ",
        "Y": "Note"
    },
    {
        "X": "What do all datasets that represent from keys to data samples should subclass it?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note ",
        "Y": "a map"
    },
    {
        "X": "What is the name of a subclass that should overwrite__getitem__()?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note ",
        "Y": "Note"
    },
    {
        "X": "What should all datasets that represent a map from keys to data samples do?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "subclass it"
    },
    {
        "X": "What does DataLoaderby default construct?",
        "Z": "Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "Y": "index sampler"
    },
    {
        "X": "What must be provided to make it work with a map-style dataset with non-integral indices/keys?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "a custom sampler must be provided"
    },
    {
        "X": "What does a custom sampler need to be provided to make it work with a map-style dataset with non-integral indices/",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "Y": "iterable Dataset"
    },
    {
        "X": "What should all datasets that represent an iterable of data samples should do?",
        "Z": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "subclass it"
    },
    {
        "X": "Where does data come from?",
        "Z": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "a stream"
    },
    {
        "X": "What should all subclasses do?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "overwrite__iter__()"
    },
    {
        "X": "What does DataLoaderby use to create a custom sampler?",
        "Z": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "An iterable Dataset"
    },
    {
        "X": "What default constructs a index sampler that yields integral indices?",
        "Z": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "Y": "DataLoaderby"
    },
    {
        "X": "What is a custom sampler required to work with a map-style dataset with non-integral indices/keys?",
        "Z": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. ",
        "Y": "iterable Dataset"
    },
    {
        "X": "What should all datasets that represent an iterable of data samples do?",
        "Z": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. ",
        "Y": "subclass it"
    },
    {
        "X": "When are iterable datasets particularly useful?",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. ",
        "Y": "when data come from a stream"
    },
    {
        "X": "What is the name of the dataset that represents an iterable of data samples?",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "An iterable Dataset"
    },
    {
        "X": "What is an iterable dataset particularly useful when data come from?",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "a stream"
    },
    {
        "X": "What would return an iterator of samples in an iterable dataset?",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "overwrite__iter__()"
    },
    {
        "X": "What is a dataset that represents an iterable of data samples called?",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. ",
        "Y": "iterable Dataset"
    },
    {
        "X": "What type of stream does a subclass of data come from?",
        "Z": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "a stream"
    },
    {
        "X": "When are subclasses particularly useful?",
        "Z": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. ",
        "Y": "when data come from a stream"
    },
    {
        "X": "What would overwrite__iter__() do?",
        "Z": "All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. ",
        "Y": "return an iterator of samples"
    },
    {
        "X": "What is the example of splitting workload across all workers using worker_init_fn?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "splitting workload across all workers"
    },
    {
        "X": "What will each sample be retrieved by?",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "indexing tensors along the first dimension"
    },
    {
        "X": "What does *tensors (Tensor) mean?",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "Y": "tensors that have the same size of the first dimension"
    },
    {
        "X": "What is a concatenation of multiple datasets?",
        "Z": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "Y": "Dataset"
    },
    {
        "X": "What is this class useful to?",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "Y": "assemble different existing datasets"
    },
    {
        "X": "What is the definition of datasets to be concatenated?",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "Y": "List of datasets to be concatenated"
    },
    {
        "X": "What is the example 1 of?",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "Y": "splitting workload across all workers"
    },
    {
        "X": "Each sample will be retrieved by what along the first dimension?",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "Y": "indexing tensors"
    },
    {
        "X": "What are tensors that have the same size of the first dimension?",
        "Z": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "Y": "*tensors(Tensor)"
    },
    {
        "X": "What class is useful to assemble different existing datasets?",
        "Z": "When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated ",
        "Y": "Dataset"
    },
    {
        "X": "What is this class useful for?",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "Y": "assemble different existing dataset streams"
    },
    {
        "X": "What is the name of the list of datasets to be concatenated?",
        "Z": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "datasets(sequence) \u2013 List of datasets to be concatenated"
    },
    {
        "X": "What is used to split workload across all workers?",
        "Z": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "worker_init_fn"
    },
    {
        "X": "How is each sample retrieved?",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. ",
        "Y": "indexing tensors along the first dimension"
    },
    {
        "X": "What is a concatenation of multiple datasets useful for?",
        "Z": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. ",
        "Y": "to assemble different existing datasets"
    },
    {
        "X": "What is the name of the list of datasets to be concatenated Dataset for chainning multipleIterableDatasets?",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. When called in a worker, this returns an object guaranteed to have the\nfollowing attributes: id: the current worker id. num_workers: the total number of workers. ",
        "Y": "List of datasets to be concatenated Dataset for chainning multipleIterableDatasets"
    },
    {
        "X": "Returns what in the inputtensor?",
        "Z": "Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   ",
        "Y": "total number of elements"
    },
    {
        "X": "dtype(torch.dtype, optional) \u2013 what type of returned tensor?",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. ",
        "Y": "desired data type"
    },
    {
        "X": "What is casted todtype before the operation is performed?",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. ",
        "Y": "preventing data type overflows"
    },
    {
        "X": "Why is the input tensor casted todtypebefore the operation is performed?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "preventing data type overflows"
    },
    {
        "X": "What is the default value of the input tensor?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. ",
        "Y": "None"
    },
    {
        "X": "What does this return?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "the sum of each row of theinputtensor in the given dimensiondim"
    },
    {
        "X": "Ifdimis a list of dimensions, what does it do?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "reduce over all of them"
    },
    {
        "X": "If specified, the input tensor is casted what before the operation is performed?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "todtype"
    },
    {
        "X": "What is this useful for preventing?",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. ",
        "Y": "data type overflows"
    },
    {
        "X": "What is the default for the input tensor?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "None"
    },
    {
        "X": "What is a return of the input tensor in the given dimensiondim?",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. ",
        "Y": "the sum of each row of theinputtensor in the given dimensiondim"
    },
    {
        "X": "IfkeepdimisTrue, the output tensor is of what size?",
        "Z": "IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "size 1"
    },
    {
        "X": "How many dimensions does the output tensor have?",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "1"
    },
    {
        "X": "What is intortuple of python:ints?",
        "Z": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "Y": "dim"
    },
    {
        "X": "What is returned when a row of the inputtensor is set to a given dimensiondim?",
        "Z": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "Y": "Returns the sum of each row of theinputtensor in the given dimensiondim"
    },
    {
        "X": "Ifdimis a list of dimensions, reduce over all of them?",
        "Z": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "Y": "IfkeepdimisTrue"
    },
    {
        "X": "What is the name of the output tensor that has fewer dimension(s)?",
        "Z": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "Y": "orlen(dim)"
    },
    {
        "X": "What is the sum of each row of theinputtensor in the given dimensiondim?",
        "Z": "Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "Y": "Ifdimis a list of dimensions"
    },
    {
        "X": "Ifdimis a list of dimensions, what do you do over all of them?",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. ",
        "Y": "reduce"
    },
    {
        "X": "What returns the output tensor of the same size as input except in the dimension(s)dim where it is of size 1?",
        "Z": "Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "Y": "IfkeepdimisTrue"
    },
    {
        "X": "What is the name of the output tensor that has fewer dimension(s) than input?",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. ",
        "Y": "orlen(dim)"
    },
    {
        "X": "If the output tensor is of the same size as input, what is it?",
        "Z": "IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "Y": "IfkeepdimisTrue"
    },
    {
        "X": "What does the input tensor compute?",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "Y": "bitwise NOT"
    },
    {
        "X": "What type of type does the input tensor have?",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "Y": "Boolean types"
    },
    {
        "X": "For what type of tensor, it computes the logical NOT of the input tensor?",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "Y": "bool tensors"
    },
    {
        "X": "What is out of the input tensor?",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "Y": "output tensor"
    },
    {
        "X": "What does compute the bitwise NOT of the given input tensor do?",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "Y": "Computes the bitwise NOT"
    },
    {
        "X": "The input tensor must be of what types?",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "Y": "integral or Boolean types"
    },
    {
        "X": "For what type of tensor does it compute the logical NOT?",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example ",
        "Y": "bool tensors"
    },
    {
        "X": "What is a tensor with two or more dimensions?",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "Splitsinput"
    },
    {
        "X": "What is splitsinput?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "view ofinput"
    },
    {
        "X": "What is the equivalent of calling Splitsinput?",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "torch"
    },
    {
        "X": "What is the function based on?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "NumPy"
    },
    {
        "X": "What is tensor to split?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "input(Tensor)"
    },
    {
        "X": "What is used to split a tensor with two or more dimensions into multiple tensors vertically?",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. ",
        "Y": "indices_or_sections"
    },
    {
        "X": "What is each split?",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "a view ofinput"
    },
    {
        "X": "What is the difference between splitsinput and torch.tensor_split(input, indices_or_sections",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. ",
        "Y": "ifindices_or_sectionsis an integer it must evenly divide the split dimension"
    },
    {
        "X": "What is this function based on?",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "NumPy\u2019snumpy.vsplit()"
    },
    {
        "X": "What is the equivalent of calling input, indices_or_sections, dim=0?",
        "Z": "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "torch.tensor_split"
    },
    {
        "X": "What is the split dimension of torch.tensor_split?",
        "Z": "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "dim=0"
    },
    {
        "X": "What is input to split?",
        "Z": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. ",
        "Y": "tensor"
    },
    {
        "X": "What is the name of the tensor to split?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "indices_or_sections"
    },
    {
        "X": "What is the difference between torch.tensor_split and torch.tensor_split?",
        "Z": "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "ifindices_or_sectionsis an integer it must evenly divide the split dimension"
    },
    {
        "X": "What does input(Tensor) - tensor to split?",
        "Z": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. ",
        "Y": "input(Tensor) \u2013 tensor to split"
    },
    {
        "X": "What is the argument for indices_or_sections(Tensor,intorlistortuple of py",
        "Z": "This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "intorch.tensor_split()"
    },
    {
        "X": "What is a tensor with one or more dimensions?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "Splitsinput"
    },
    {
        "X": "What is a split in a tensor?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. ",
        "Y": "view ofinput"
    },
    {
        "X": "What are splitsinput according to?",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. ",
        "Y": "indices"
    },
    {
        "X": "What is the split dimension of tensor_split?",
        "Z": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. ",
        "Y": "zero"
    },
    {
        "X": "What is equivalent to calling torch?",
        "Z": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. ",
        "Y": "ifinputhas two or more dimensions"
    },
    {
        "X": "What is input derived from NumPy'snumpy.hsplit()?",
        "Z": "This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "tensor"
    },
    {
        "X": "What is the tensor to split?",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split(). ",
        "Y": "input(Tensor)"
    },
    {
        "X": "In what direction do the entries in each row of tensor appear in a different order than before?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "left/right"
    },
    {
        "X": "What are preserved, but appear in a different order than before?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "Columns"
    },
    {
        "X": "What is the name of'snp.fliplr'?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "NumPy"
    },
    {
        "X": "What is torch.fliplris expected to be?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "slower thannp.fliplr"
    },
    {
        "X": "What does flip tensor in the left/right direction return?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "a new tensor"
    },
    {
        "X": "What does flip tensor do?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "Flip the entries in each row in the left/right direction"
    },
    {
        "X": "What must the tensor be at least?",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr. ",
        "Y": "2-D"
    },
    {
        "X": "What is the name of the elements ofinput that returns a new tensor?",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "Y": "hyperbolic cosine"
    },
    {
        "X": "What library does torch.cosh use?",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "Y": "Sleef library"
    },
    {
        "X": "What does the Sleef library use?",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "Y": "Seeherefor details"
    },
    {
        "X": "Returns what with the hyperbolic cosine of the elements of input?",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "Y": "a new tensor"
    },
    {
        "X": "Wheninputis on the CPU, the implementation of torch.cosh may use what library?",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "Y": "Sleef library"
    },
    {
        "X": "What does the Sleef library provide?",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details. ",
        "Y": "details"
    },
    {
        "X": "Fillsselftensor with numbers samples from what distribution parameterized by the given meanmuand standard deviationsigma?",
        "Z": "Fillsselftensor with numbers samples from the log-normal distribution\nparameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3. Note thatmeanandstdare the mean and\nstandard deviation of the underlying normal distribution, and not of the\nreturned distribution: ",
        "Y": "log-normal distribution"
    },
    {
        "X": "What is not represented by the mean and standard deviation of the underlying normal distribution?",
        "Z": "Fillsselftensor with numbers samples from the log-normal distribution\nparameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3. Note thatmeanandstdare the mean and\nstandard deviation of the underlying normal distribution, and not of the\nreturned distribution: ",
        "Y": "returned distribution"
    },
    {
        "X": "What is similar tobroadcast_tensors()but for?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "shapes"
    },
    {
        "X": "What is the equivalent tobroadcast_tensors()but for shapes?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "totorch.broadcast_tensors"
    },
    {
        "X": "What is this useful for?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "broadcasting tensors of common batch shape"
    },
    {
        "X": "What is used for broadcasting tensors of common batch shape but different rightmost shape?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "mean vectors with covariance matrices"
    },
    {
        "X": "What is an example of a shape compatible with all input shapes?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "*shapes(torch.Size) \u2013 Shapes of tensors"
    },
    {
        "X": "What is a shape compatible with all input shapes?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "RuntimeError"
    },
    {
        "X": "What is similar to tobroadcast_tensors() but for?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "shapes"
    },
    {
        "X": "What does shapebut avoid?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "create to intermediate tensors"
    },
    {
        "X": "What shape is used to broadcast tensors of common batch shape but different?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "rightmost shape"
    },
    {
        "X": "What is useful for broadcasting tensors of common batch shape but different rightmost shape?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "mean vectors with covariance matrices"
    },
    {
        "X": "A shape compatible with what?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "all input shapes"
    },
    {
        "X": "What happens if shapes are incompatible?",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible. ",
        "Y": "RuntimeError"
    },
    {
        "X": "What is the behavior similar to python\u2019sitertools.product?",
        "Z": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example: ",
        "Y": "Do cartesian product"
    },
    {
        "X": "What is a *tensor?",
        "Z": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example: ",
        "Y": "any number of 1 dimensional tensors"
    },
    {
        "X": "What is a tensor equivalent to?",
        "Z": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example: ",
        "Y": "converting all the input tensors into lists"
    },
    {
        "X": "What is a tensor equivalent to converting all the input tensors into lists?",
        "Z": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example: ",
        "Y": "Tensor Example"
    },
    {
        "X": "What package contains data structures for multi-dimensional tensors?",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   ",
        "Y": "torch"
    },
    {
        "X": "What does the torch package provide for efficient serializing of Tensors and arbitrary types?",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "Y": "utilities"
    },
    {
        "X": "What does the torch package have?",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   ",
        "Y": "CUDA counterpart"
    },
    {
        "X": "What does the torch package return?",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "Y": "total number of elements in theinputtensor"
    },
    {
        "X": "What utility does the torch package provide for Tensors and arbitrary types?",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   ",
        "Y": "efficient serializing"
    },
    {
        "X": "What is the counterpart of the torch package?",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   ",
        "Y": "CUDA"
    },
    {
        "X": "What does True ifobjis return?",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   ",
        "Y": "PyTorch storage object"
    },
    {
        "X": "What does the PyTorch tensor have?",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "Y": "CUDA counterpart"
    },
    {
        "X": "What is True ifobjis?",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "Y": "PyTorch storage object"
    },
    {
        "X": "What is the data type ofinputis a complex data type?",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "Y": "one oftorch.complex64, andtorch.complex128"
    },
    {
        "X": "What is the data type ofinputis?",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "Y": "floating point data type"
    },
    {
        "X": "What is the name of the GPU that allows you to run tensor computations on an NVIDIA GPU with compute capability >= 3.0",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "Y": "CUDA"
    },
    {
        "X": "What does True return ifobjis a PyTorch tensor?",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "Y": "PyTorch storage object"
    },
    {
        "X": "Returns True if the data type ofinput is a what?",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   ",
        "Y": "complex data type"
    },
    {
        "X": "Returns True if the data type ofinputis a what type of data type?",
        "Z": "Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "Y": "complex data type"
    },
    {
        "X": "Returns True ifobjis a what?",
        "Z": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "Y": "PyTorch storage object"
    },
    {
        "X": "Returns True if the data type ofinputis a what?",
        "Z": "Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "Y": "floating point data type"
    },
    {
        "X": "What is not equal to zero after type conversions?",
        "Z": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "a single element tensor"
    },
    {
        "X": "What does Returns True if the data type ofinputis a single element tensor which is not equal to zero after type conversion",
        "Z": "Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   ",
        "Y": "Sets the default floating point dtype tod"
    },
    {
        "X": "What is returned ifobjis a PyTorch storage object?",
        "Z": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   ",
        "Y": "PyTorch storage object"
    },
    {
        "X": "Returns True what if theinputis a single element tensor?",
        "Z": "Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   ",
        "Y": "if theinputis a single element tensor"
    },
    {
        "X": "What does it do if the input is a single element tensor which is not equal to zero after type conversions?",
        "Z": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   ",
        "Y": "Sets the default floating point dtype tod"
    },
    {
        "X": "What does Sets the default floating point dtype tod return?",
        "Z": "Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   ",
        "Y": "current default floating pointtorch.dtype"
    },
    {
        "X": "What does it do if the data type ofinputis a single element tensor which is not equal to zero after type conversions",
        "Z": "Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   ",
        "Y": "Sets the default floating point dtype tod"
    },
    {
        "X": "Returns what if the data type ofinputis a floating point data type?",
        "Z": "Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   ",
        "Y": "True"
    },
    {
        "X": "What does it do if the input is a single element tensor?",
        "Z": "Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   ",
        "Y": "Sets the default floating point dtype tod"
    },
    {
        "X": "Sets what to floating point tensor typet?",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ",
        "Y": "defaulttorch.Tensortype"
    },
    {
        "X": "What is returned when the defaulttorch.Tensortype is set to floating point tensor typet?",
        "Z": "Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "the total number of elements in theinputtensor"
    },
    {
        "X": "Set options for what?",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data. ",
        "Y": "printing"
    },
    {
        "X": "Returns what if theinputis a single element tensor which is not equal to zero after type conversions?",
        "Z": "Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "True"
    },
    {
        "X": "Sets what?",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution ",
        "Y": "random number generator state"
    },
    {
        "X": "What is the default floating pointtorch.dtype?",
        "Z": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   ",
        "Y": "current"
    },
    {
        "X": "What is returned by the defaulttorch.Tensortype?",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   ",
        "Y": "the total number of elements in theinputtensor"
    },
    {
        "X": "Disables what on CPU?",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "Y": "denormal floating numbers"
    },
    {
        "X": "Disables denormal floating numbers on CPU?",
        "Z": "Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.   Returns True if theinputis a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "Note"
    },
    {
        "X": "What is the default floating point dtype tod?",
        "Z": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "current default floating pointtorch.dtype"
    },
    {
        "X": "What is returned by setting the defaulttorch.Tensortype to floating point tensor typet?",
        "Z": "Sets the default floating point dtype tod.   Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "the total number of elements in theinputtensor"
    },
    {
        "X": "What does the defaulttorch.Tensortype set to floating point tensor typet?",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "current default floating pointtorch.dtype"
    },
    {
        "X": "What is the defaulttorch.Tensortype?",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   ",
        "Y": "current default floating pointtorch.dtype"
    },
    {
        "X": "What is the default option for?",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "printing"
    },
    {
        "X": "What does the default floating pointtorch.dtype mean?",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "Note"
    },
    {
        "X": "What do you get?",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "current default floating pointtorch.dtype"
    },
    {
        "X": "What is a floating point tensor typet?",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "defaulttorch.Tensortype"
    },
    {
        "X": "What is the defaulttorch.Tensortype to floating point tensor typet?",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "Note"
    },
    {
        "X": "What does the defaulttorch.Tensortype do?",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "Disables denormal floating numbers on CPU"
    },
    {
        "X": "What is the name of the variable that can be used to set denormal floating numbers on the CPU?",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note ",
        "Y": "Note"
    },
    {
        "X": "What is the total number of elements in?",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "Y": "theinputtensor"
    },
    {
        "X": "What is a Disables denormal floating numbers on CPU?",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "Y": "Set options for printing"
    },
    {
        "X": "What does a denormal floating number on a CPU do?",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "Y": "Disables denormal floating numbers on CPU"
    },
    {
        "X": "What are listed underRandom sampling?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "Random sampling creation ops"
    },
    {
        "X": "What does a tensor have?",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      ",
        "Y": "Splits a tensor into a specific number of chunks"
    },
    {
        "X": "What does this function do?",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   ",
        "Y": "Unpacks the data and pivots"
    },
    {
        "X": "What type of creation ops are listed under Random sampling?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   ",
        "Y": "Random sampling"
    },
    {
        "X": "What is constructed withdata?",
        "Z": "Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "Y": "a tensor"
    },
    {
        "X": "What are Disables denormal floating numbers on CPU?",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "Y": "Set options for printing"
    },
    {
        "X": "What does the CPU do?",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "Y": "Disables denormal floating numbers"
    },
    {
        "X": "What type of withdata does torch.empty() construct?",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   ",
        "Y": "tensor"
    },
    {
        "X": "What does it do to disable denormal floating numbers on CPU?",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "Y": "Set options for printing"
    },
    {
        "X": "What happens on CPU?",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   ",
        "Y": "Disables denormal floating numbers"
    },
    {
        "X": "What are listed underRandom samplingand include:torch.rand()torch.rand_like()torch.",
        "Z": "Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   ",
        "Y": "Random sampling creation ops"
    },
    {
        "X": "What does torch.empty() construct?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   ",
        "Y": "tensor withdata"
    },
    {
        "X": "What format does asparse tensor in?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "COO(rdinate) format"
    },
    {
        "X": "What creation ops are listed under Random sampling?",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "Y": "Random sampling"
    },
    {
        "X": "What does asparse tensor in COO(rdinate) format contain?",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "Y": "specified values at the givenindices"
    },
    {
        "X": "What do you convert the data into?",
        "Z": "Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   ",
        "Y": "atorch.Tensor"
    },
    {
        "X": "What does the asparse tensor contain?",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",
        "Y": "specified values at the givenindices"
    },
    {
        "X": "What is the data converted into?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "atorch.Tensor"
    },
    {
        "X": "What does a view of an existingtorch.Tensorinput have?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "specifiedsize,strideandstorage_offset"
    },
    {
        "X": "What is the name of aTensor created?",
        "Z": "Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "anumpy.ndarray"
    },
    {
        "X": "What does a tensor fill with the scalar value0 have?",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "the same size asinput"
    },
    {
        "X": "What is a tensor filled with the scalar value1?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "the shape defined by the variable argumentsize"
    },
    {
        "X": "What does an existingtorch.Tensorinput have?",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "specifiedsize,strideandstorage_offset"
    },
    {
        "X": "What is the same size asinput?",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "a tensor filled with the scalar value0"
    },
    {
        "X": "What does a tensor fill with the scalar value1 have?",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "the same size asinput"
    },
    {
        "X": "What is the shape of a tensor filled with the scalar value0?",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "the shape defined by the variable argumentsize"
    },
    {
        "X": "What does the tensor fill with the scalar value0 have?",
        "Z": "Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "the same size asinput"
    },
    {
        "X": "What is the shape of a tensor filled with the scalar value1?",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "the shape defined by the variable argumentsize"
    },
    {
        "X": "What is the name of the aTensor created from?",
        "Z": "Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "anumpy.ndarray"
    },
    {
        "X": "Returns a tensor filled with the shape defined by the variable argumentsize. Returns a tensor filled with what value",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "scalar value1"
    },
    {
        "X": "What is returned when the scalar value0 is returned?",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "a tensor"
    },
    {
        "X": "Returns a tensor filled with the scalar value0, with what as input?",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   ",
        "Y": "same size"
    },
    {
        "X": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize. Returns a",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "same size"
    },
    {
        "X": "What is the tensor of sizeendstartstepleftlceil fractextend?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "1-D"
    },
    {
        "X": "Returns a tensor filled with the scalar value1 with what?",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   ",
        "Y": "the shape defined by the variable argumentsize"
    },
    {
        "X": "Returns a tensor filled with the scalar value1, with what as input?",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   ",
        "Y": "same size"
    },
    {
        "X": "Returns what tensor of sizeendstartstepleftlceil fractextend - ",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",
        "Y": "1-D"
    },
    {
        "X": "What defines the shape of a tensor filled with the scalar value?",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "Y": "variable argumentsize"
    },
    {
        "X": "What does a tensor fill with the scalar value have?",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "Y": "the same size asinput"
    },
    {
        "X": "What is the tensor of sizeendstartstepleftlceil?",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "Y": "1-D"
    },
    {
        "X": "Returns what type of tensor?",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   ",
        "Y": "a 1-D tensor"
    },
    {
        "X": "What is a tensor of sizeendstartstepleftlceil fractextend?",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "Y": "1-D"
    },
    {
        "X": "What is a tensor of sizeendstartstep+1leftlfloor fractextend?",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "Y": "1-D"
    },
    {
        "X": "Returns what type of scalar value?",
        "Z": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   ",
        "Y": "a tensor"
    },
    {
        "X": "Where are the values from the interval[start,end]taken?",
        "Z": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   ",
        "Y": "fromstart"
    },
    {
        "X": "Returns a tensor filled with the scalar value of how many tensors?",
        "Z": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   ",
        "Y": "1"
    },
    {
        "X": "How many -D tensor of sizeendstartstep+1leftlceil fractextend",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "1"
    },
    {
        "X": "What is the tensor of sizesteps created?",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "one-dimensional"
    },
    {
        "X": "Where are the values from the interval[start,end] taken?",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "fromstart"
    },
    {
        "X": "How many tensors does this function return?",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "1"
    },
    {
        "X": "What type of tensor of sizesteps is created?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "one-dimensional"
    },
    {
        "X": "What is the tensor of sizeendstartstep+1leftlfloor?",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "1-D"
    },
    {
        "X": "Returns a tensor of sizesteps of how many dimensions?",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   ",
        "Y": "1"
    },
    {
        "X": "What is a tensor of sizesteps that values are evenly spaced fromstarttoend, inclusive?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "one-dimensional"
    },
    {
        "X": "What is the logarithmic scale of a one-dimensional tensor of sizesteps?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "basebase"
    },
    {
        "X": "What is a tensor on the diagonal and zeros elsewhere?",
        "Z": " Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "Y": "2-D"
    },
    {
        "X": "On a logarithmic scale, what is used to create a one-dimensional tensor of sizesteps?",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   ",
        "Y": "basebase"
    },
    {
        "X": "Returns a 2-D tensor with what?",
        "Z": " Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "Y": "ones on the diagonal and zeros elsewhere"
    },
    {
        "X": "Returns a tensor filled with what?",
        "Z": "  Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences. ",
        "Y": "random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)"
    },
    {
        "X": "What type of tensor is created on a logarithmic scale with basebase?",
        "Z": " Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   ",
        "Y": "one-dimensional"
    },
    {
        "X": "What is a tensor filled with uninitialized data?",
        "Z": "  Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput. ",
        "Y": "uninitialized"
    },
    {
        "X": "What type of tensor is created?",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   ",
        "Y": "one-dimensional tensor of sizesteps"
    },
    {
        "X": "What does a complex tensor return?",
        "Z": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning ",
        "Y": "a view ofinputas"
    },
    {
        "X": "What does this function return?",
        "Z": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning ",
        "Y": "a view ofinput as a complex tensor"
    },
    {
        "X": "The last dimension of the input tensor is expected to represent what?",
        "Z": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning ",
        "Y": "real and imaginary components of complex numbers"
    },
    {
        "X": "What does the function return for an input complex tensor ofsizem1,m2,...,mim1, m2, do",
        "Z": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning ",
        "Y": "torch.view_as_complex"
    },
    {
        "X": "What does compute the element-wise logical XOR of the given input tensors do?",
        "Z": "Computes the element-wise logical XOR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute XOR with out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "Computes the element-wise logical XOR"
    },
    {
        "X": "Nonzeros are treated as what?",
        "Z": "Computes the element-wise logical XOR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute XOR with out(Tensor,optional) \u2013 the output tensor. Example: ",
        "Y": "True"
    },
    {
        "X": "What is the name of the function that generates a question?",
        "Z": "Alias fortorch.abs() ",
        "Y": "Alias fortorch.abs()"
    },
    {
        "X": "What does Alias fortorch.clamp() do?",
        "Z": "Alias fortorch.clamp(). ",
        "Y": "Alias fortorch.clamp()"
    },
    {
        "X": "What is an object that represents the data type of atorch.Tensor?",
        "Z": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "Y": "Atorch.dtype"
    },
    {
        "X": "How many different data types does PyTorch have?",
        "Z": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "Y": "twelve"
    },
    {
        "X": "How many -bit floating point1 torch does PyTorch have?",
        "Z": "Eachtorch.Tensorhas atorch.dtype,torch.device, andtorch.layout. Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half ",
        "Y": "16"
    },
    {
        "X": "What is the 16-bit floating point1 torch?",
        "Z": "Eachtorch.Tensorhas atorch.dtype,torch.device, andtorch.layout. Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half ",
        "Y": "double"
    },
    {
        "X": "What type of torch does atorch.Tensor have?",
        "Z": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 ",
        "Y": "double torch"
    },
    {
        "X": "What is the name of the 16-bit floating point2 torch?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "Y": "HalfTensor"
    },
    {
        "X": "What is the name of the floating point torch?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. ",
        "Y": "32-bit floating point torch"
    },
    {
        "X": "What is the name of the torch that float64ortorch.double torch?",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) ",
        "Y": "64-bit floating point torch"
    },
    {
        "X": "What is the name of the unsigned torch.uint8 torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is the number of integers in the torch.int8 torch?",
        "Z": "torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. ",
        "Y": "8-bit"
    },
    {
        "X": "What is the name of the name of the torch.int16ortorch.short torch?",
        "Z": "torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int ",
        "Y": "16-bit integer"
    },
    {
        "X": "What is the name of the bfloat16 torch?",
        "Z": "torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int ",
        "Y": "HalfTensor 16-bit floating point2 torch"
    },
    {
        "X": "What is the number of unsigned torch.uint8 torch in the BFloat16Tensor?",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) ",
        "Y": "8-bit"
    },
    {
        "X": "What is the name of the integer (signed) torch.int8 torch?",
        "Z": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) ",
        "Y": "8-bit"
    },
    {
        "X": "What is the name of the complex torch.complex64ortorch.cfloat 128-bit complex torch?",
        "Z": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) ",
        "Y": "64-bit"
    },
    {
        "X": "What is the unsigned number of torch.uint8 torch?",
        "Z": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) ",
        "Y": "8-bit"
    },
    {
        "X": "What is the name of the torch that is int16ortorch.short torch?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "Y": "16-bit integer"
    },
    {
        "X": "What is the number of integers in the torch.int32ortorch.int torch?",
        "Z": "torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. ",
        "Y": "32-bit"
    },
    {
        "X": "What type of integer is intTensor?",
        "Z": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) ",
        "Y": "64-bit"
    },
    {
        "X": "What is the number of integers in torch.int32ortorch.int torch?",
        "Z": "torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: ",
        "Y": "32-bit"
    },
    {
        "X": "IntTensor is what type of integer (signed) torch.int64ortorch.long?",
        "Z": "torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long ",
        "Y": "64-bit"
    },
    {
        "X": "How many -bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "Y": "128"
    },
    {
        "X": "What is the number of unsigned torch in the BFloat16Tensor?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "Y": "8-bit"
    },
    {
        "X": "What is the name of the torch that is intTensor?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool ",
        "Y": "64-bit"
    },
    {
        "X": "How many -bit integer (signed) torch.int16ortorch.short torch?",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. ",
        "Y": "16"
    },
    {
        "X": "What is a 8-bit integer?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. ",
        "Y": "unsigned"
    },
    {
        "X": "What is the name of the 8-bit integer (signed) torch.int8 torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. ",
        "Y": "ByteTensor"
    },
    {
        "X": "When is binary16 useful?",
        "Z": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "Y": "when precision is important"
    },
    {
        "X": "What is important when a binary16 uses 1 sign, 5 exponent, and 10 significand bits?",
        "Z": "Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. ",
        "Y": "precision"
    },
    {
        "X": "What is the sign of a ByteTensor?",
        "Z": "torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is important when it has the same number of exponent bits asfloat32?",
        "Z": "torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "Y": "range"
    },
    {
        "X": "What is the sign for torch.int32ortorch.int torch?",
        "Z": "32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "Y": "32-bit integer"
    },
    {
        "X": "What is the name of the 64-bit integer (signed) torch.int64ortorch.long torch?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "Y": "Boolean torch"
    },
    {
        "X": "What is the BoolTensor sometimes referred to as?",
        "Z": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "Y": "binary16"
    },
    {
        "X": "What is binary16 sometimes referred to as?",
        "Z": "Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. ",
        "Y": "Brain Floating Point"
    },
    {
        "X": "When is Brain Floating Point useful?",
        "Z": "torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. ",
        "Y": "when range is important"
    },
    {
        "X": "What is the name of a torch.int32ortorch.long torch?",
        "Z": "torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "Y": "64-bit integer"
    },
    {
        "X": "What is another name for a 64-bit integer?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "Y": "long torch"
    },
    {
        "X": "What is important when a brain floating point is used?",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. ",
        "Y": "precision"
    },
    {
        "X": "What is used to use 1 sign, 8 exponent and 7 significand bits?",
        "Z": "torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "Brain Floating Point"
    },
    {
        "X": "What can be used to find out if atorch.dtypeis a floating point data type?",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: ",
        "Y": "propertyis_floating_pointcan be used"
    },
    {
        "X": "What is BooleanTensor sometimes referred to as?",
        "Z": "Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. ",
        "Y": "binary16"
    },
    {
        "X": "To find out if atorch.dtypeis a complex data type, what property can be used?",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "propertyis_complex"
    },
    {
        "X": "What is an object representing the device on which atorch.Tensoris or will be allocated?",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "Y": "Atorch.device"
    },
    {
        "X": "What is a device type called?",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). ",
        "Y": "cpu'or'cuda"
    },
    {
        "X": "What is called if the device ordinal is not present?",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). ",
        "Y": "aftertorch.cuda.set_device()"
    },
    {
        "X": "If the device ordinal is not present, this object will always represent what for the device type?",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "Y": "current device"
    },
    {
        "X": "What does the torch.device contains?",
        "Z": "Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. ",
        "Y": "device type"
    },
    {
        "X": "How can Atorch.Tensor's device be accessed?",
        "Z": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note ",
        "Y": "theTensor.deviceproperty"
    },
    {
        "X": "What does Thetorch.device contain?",
        "Z": "Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. ",
        "Y": "device type"
    },
    {
        "X": "What can be done by using a string or a device ordinal?",
        "Z": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note ",
        "Y": "fast prototyping of code"
    },
    {
        "X": "What returns an ordinal for cuda tensors?",
        "Z": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "Y": "matchesTensor.get_device()"
    },
    {
        "X": "What is the name of the device that is not supported for cpu tensors?",
        "Z": "Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note ",
        "Y": "Note"
    },
    {
        "X": "What does the device ordinal allow for?",
        "Z": "Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note ",
        "Y": "fast prototyping of code"
    },
    {
        "X": "What is an object that represents the memory layout of atorch.Tensor?",
        "Z": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse. ",
        "Y": "A torch.layoutis"
    },
    {
        "X": "What do strided tensors provide?",
        "Z": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse. ",
        "Y": "multi-dimensional,stridedview of a storage"
    },
    {
        "X": "What represents the jump in the memory necessary to go from one element to the next in the k-th dimension of the Tensor?",
        "Z": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse. ",
        "Y": "k-th stride"
    },
    {
        "X": "What does the k-th stride make it possible to do?",
        "Z": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse. ",
        "Y": "perform many tensor operations efficiently"
    },
    {
        "X": "What is an object representing the memory format on which atorch.Tensoris or will be allocated?",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "Y": "A torch.memory_format is"
    },
    {
        "X": "What are the possible values atorch.memory_format?",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "Y": "torch.contiguous_format"
    },
    {
        "X": "What type of order are values represented by?",
        "Z": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "Y": "decreasing order"
    },
    {
        "X": "What is the name of the tensor that is or will be allocated in dense non-overlapping memory?",
        "Z": "torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "Y": "torch.channels_last"
    },
    {
        "X": "Where is Tensor allocated?",
        "Z": "torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "Y": "dense non-overlapping memory"
    },
    {
        "X": "What order are atorch.Tensoris represented by?",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "Y": "decreasing"
    },
    {
        "X": "What are the possible values of torch.contiguous_format?",
        "Z": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "Y": "dense non-overlapping memory"
    },
    {
        "X": "What is the name of the order in which tensor is or will be allocated in dense non-overlapping memory?",
        "Z": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. ",
        "Y": "NHWC"
    },
    {
        "X": "What does CharTensor stand for?",
        "Z": "torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. ",
        "Y": "16-bit integer"
    },
    {
        "X": "How many bits is a short torch?",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "Y": "16"
    },
    {
        "X": "How do we promote when the dtypes of inputs to an arithmetic operation differ?",
        "Z": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "Y": "by finding the minimum dtype that satisfies the following rules"
    },
    {
        "X": "What property returnsTrue if the data type is a floating point data type?",
        "Z": "torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "Y": "propertyis_floating_pointcan be used"
    },
    {
        "X": "What property returnsTrue if the data type is a complex data type?",
        "Z": "torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: ",
        "Y": "property is_complex can be used"
    },
    {
        "X": "What is important when Brain Floating Point has the same number of exponent bits asfloat32?",
        "Z": "torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. ",
        "Y": "range"
    },
    {
        "X": "When the dtypes of inputs to an operation (add,sub,div,mul) differ, we promote by finding the minimum d",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "arithmetic"
    },
    {
        "X": "What is the term used to describe the use of 1 sign, 5 exponent, and 10 significand bits?",
        "Z": "torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: ",
        "Y": "binary16"
    },
    {
        "X": "What does get_device() do?",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "Y": "returns an ordinal for cuda tensors"
    },
    {
        "X": "Where is atorch.Tensor allocated?",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "Y": "dense non-overlapping memory"
    },
    {
        "X": "What order are atorch.tensoris strides represented by?",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "Y": "decreasing"
    },
    {
        "X": "What is used in functions likecloneto preserve the memory format of the input tensor?",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format ",
        "Y": "torch.preserve_format"
    },
    {
        "X": "When is 1 sign, 5 exponent, and 10 significand bits used?",
        "Z": "Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "Y": "when precision is important"
    },
    {
        "X": "What are some inputs to an arithmetic operation?",
        "Z": "Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. ",
        "Y": "add,sub,div,mul"
    },
    {
        "X": "How many bits does a CharTensor have?",
        "Z": "torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. ",
        "Y": "16-bit"
    },
    {
        "X": "What is the size of a ShortTensor?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. ",
        "Y": "32-bit"
    },
    {
        "X": "IntTensor is what type of integer?",
        "Z": "torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: ",
        "Y": "64-bit"
    },
    {
        "X": "64-bit integer torch.int64ortorch is what?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "long torch"
    },
    {
        "X": "If there are no higher-category zero-dim operands, we promote to a type with sufficient what to hold all dimensione",
        "Z": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "size and category"
    },
    {
        "X": "What is the name of the program that does not inspect values when determining the minimumdtypesof an operand?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "numpy"
    },
    {
        "X": "What types are not yet supported?",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "Quantized and complex types"
    },
    {
        "X": "We do not inspect values when determining the minimumdtypesof an operand. Quantized and complex types are not yet supported. Unlike what",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "numpy"
    },
    {
        "X": "What operand has a higher category than dimensioned operands?",
        "Z": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "zero-dimension tensor"
    },
    {
        "X": "Promotion Examples:",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. ",
        "Y": "An integral output tensor cannot accept a floating point tensor"
    },
    {
        "X": "What is optional for the device type?",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: ",
        "Y": "device ordinal"
    }
]